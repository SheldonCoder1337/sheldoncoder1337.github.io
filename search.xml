<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>READ-Telemetry</title>
      <link href="/2024/07/28/READ-Telemetry/"/>
      <url>/2024/07/28/READ-Telemetry/</url>
      
        <content type="html"><![CDATA[<h2 id="What-is-Telemetry"><a href="#What-is-Telemetry" class="headerlink" title="What is Telemetry?"></a>What is Telemetry?</h2><p>A nurse in a hospital is far too busy to watch every patient every minute. She relies on telemetry to monitor their vital signs, such as their blood pressure, and alert her if their condition worsens.</p><p>Telemetry systems automatically <strong>collect data from sensors</strong>, whether they are attached to a patient, a jet engine or an application server. It then <strong>sends that information to a central site for performance monitoring and to identify problems</strong>.</p><ul><li>遥测系统是一种自动收集传感器数据，并将该信息发送到中央站点以进行性能监控和识别问题。</li></ul><p>Telemetry was developed to automatically measure industrial, scientific and military data <strong>from remote locations</strong>. These included tracking how a missile performed in flight or the temperatures in a blast furnace. In the world of IT and security, telemetry data monitors metrics such as application downtime, database errors, or network connections. This data is the raw material for <strong>observability</strong> – understanding how well applications and services are working, and how users interact with them.</p><ul><li>遥测数据是可观察性的原材料，用于了解应用程序和服务的运行状况以及用户与它们交互的方式。</li></ul><h2 id="How-does-Telemetry-work"><a href="#How-does-Telemetry-work" class="headerlink" title="How does Telemetry work?"></a>How does Telemetry work?</h2><ul><li>When telemetry monitors physical objects, it relies on <strong>sensors</strong> that measure characteristics such as temperature, pressure or vibration.</li><li>When telemetry is used to monitor IT systems, <strong>software agents</strong> gather digital data about performance, uptime and security. They send that data to collectors that process the data and transmit it for storage or analysis.</li></ul><p>Telemetry data can be produced in multiple forms by different types of agents. It must thus be <strong>“normalized”</strong> or made to fit a standard structure for use by any analytic tool.</p><ul><li>因为遥测数据可能有不同的数据来源，所以其必须规范化或者使其适合任何分析工具使用的标准结构。</li></ul><p>Historically, normalization was done through a <strong>schema-on-write</strong> process, which required knowing the required format in advance and enforcing that schema before the data was logged. That process is no longer viable given the volume, variety and velocity of data produced by IT infrastructures. A more popular current approach is <strong>schema-on-read</strong>. This converts data into the required format before it is stored and analyzed.</p><ul><li>过去，规范化操作是通过**写入时模范( schema-on-write)**过程完成的，即事先知道所需的格式，并在记录数据之前强制执行该架构，即传统数据库的模式</li><li>当下，由于数据规模3V问题，当前更流行的方法是**读取时架构(schema-on-read)**。这会在存储和分析数据之前将数据转换为所需的格式。</li></ul><h2 id="Types-of-Telemetry"><a href="#Types-of-Telemetry" class="headerlink" title="Types of Telemetry"></a>Types of Telemetry</h2><p>The information produced by IT telemetry data depends on the system being tracked and how the data is used.</p><ul><li>For servers, the data might include how close processors and memory are to being overloaded.</li><li>For networks, it might be latency and bandwidth.For applications and databases, it might be uptime and response time.</li><li>Telemetry designed to detect attacks may include tracking the number of incoming requests to a server, changes to the configuration of an application or a server, or the number or type of files being created or accessed.</li></ul><p>Telemetry data comes in three forms:</p><table><thead><tr><th>Type</th><th>Format</th><th>Description</th><th>Example</th></tr></thead><tbody><tr><td>Metrics</td><td>Numeric values</td><td>计量指标，例如处理请求所需的时间、对服务器的传入请求数或失败请求数</td><td><code>cpu_usage=0.8</code></td></tr><tr><td>Logs</td><td>Text</td><td>日志文件, 时间序列信息</td><td><code>2023-04-05T12:34:56Z INFO: User logged in</code></td></tr><tr><td>Traces</td><td></td><td>显示事务跨基础结构组件（如应用进程、数据库和网络）和服务（如搜索引擎或身份验证机制）所采用的路径</td><td></td></tr></tbody></table><h2 id="How-Telemetry-is-Used"><a href="#How-Telemetry-is-Used" class="headerlink" title="How Telemetry is Used"></a>How Telemetry is Used</h2><p>The data gathered by telemetry can provide a real-time view of application performance, so teams can perform root cause analysis on problems, prevent bottlenecks, and identify security threats.</p><ul><li>遥测收集的数据可以提供应用进程性能的**实时视图(real-time view)**，因此团队可以对问题执行根本原因分析，防止瓶颈并识别安全威胁。</li></ul><p>For security monitoring, unusual network traffic patterns might indicate a denial of service attack. Unusual requests for data from an unknown application or repeated unsuccessful attempts to log into a user account may also signal an attempted hack.</p><p>Telemetry data can also be used to track how users are interacting with applications and systems. Such user behavior testing can help improve user interfaces and compare whether tweaks to applications and websites can increase user engagement or sales.</p><ul><li>遥测数据还可用于跟踪用户与应用进程和系统的交互方式。这种用户行为测试可以帮助改进用户界面，并比较对应用进程和网站的调整是否可以提高用户参与度或销售额。</li></ul><p>Telemetry data can also help cut costs. By identifying and eliminating underused assets, such as cloud servers that are no longer needed, or helping plan and budget for infrastructure needs by identifying usage trends.</p><p>Telemetry from devices on the Internet of Things can do everything from tracking shipments to preventive equipment maintenance. This data can also enable new business models in which a company sells performance, maintenance or production data from equipment in the field.</p><ul><li>跟踪实时应用进程性能 Track real-time application performance</li><li>防止安全攻击 Prevent security attacks</li><li>改善用户体验 Improve user experience</li><li>跟踪物联网设备的性能和状态 Track the performance and status of IoT devices</li></ul><h2 id="Drawbacks-and-Challenges-of-Telemetry"><a href="#Drawbacks-and-Challenges-of-Telemetry" class="headerlink" title="Drawbacks and Challenges of Telemetry"></a>Drawbacks and Challenges of Telemetry</h2><p>Modern IT infrastructures generate very large data streams in a variety of formats. Not all of this data is critical or even important. It’s easy for system administrators and other IT staff to be overwhelmed by this data, and for storage costs to rise to unacceptable levels.</p><ul><li>现代 IT 基础设施会生成各种格式的海量数据流。并非所有这些数据都是至关重要的甚至是重要的。系统管理员和其他 IT 员工很容易被这些数据淹没，并且存储成本也会上升到不可接受的水平。</li></ul><p>System administrators and software developers must thus decide what data is most important and how to transmit, format and analyze it. Each data transmission method has its pluses and minuses. One option is sending telemetry data directly from the application being monitored. This eliminates the need to run additional software and to manage ports or processes. But if the sending application is complex and generates lots of data, sending that data could bog down the application or network being monitored.</p><ul><li>因此，系统管理员和软件开发人员必须决定哪些数据最重要，以及如何传输、格式化和分析这些数据。每种数据传输方法都有其优点和缺点。一种选择是直接从被监控的应用进程发送遥测数据。这样就无需运行额外的软件以及管理端口或进程。但是，如果发送应用进程很复杂并且生成大量数据，则发送该数据可能会使正在监视的应用进程或网络陷入困境。</li></ul><p>System administrators and software developers must also find ways to minimize the cost of storing telemetry data. One option is to store all the data in a data lake, retrieving only what is needed for analysis when it is needed. Another challenge is how to gather and analyze information from older devices and applications that may not support telemetry. One example is networks that provide performance and health data using the Simple Network Management Protocol.</p><ul><li>系统管理员和软件开发人员还必须找到最大限度降低遥测数据存储成本的方法。一种选择是将所有数据存储在数据湖中，仅在需要时检索分析所需的数据。另一个挑战是如何收集和分析来自可能不支持遥测的旧设备和应用进程的信息。一个例子是使用简单网络管理协议提供性能和运行状况数据的网络。</li></ul><p>Another challenge is finding, acquiring, and deploying analytical tools, including those using artificial intelligence and machine learning, that can sift through Tbytes of data to uncover the incidents and trends that require further attention.</p><ul><li>另一个挑战是寻找、获取和部署分析工具，包括使用人工智能和机器学习的分析工具，这些工具可以筛选数兆字节的数据，以发现需要进一步关注的事件和趋势。</li></ul><h2 id="Telemetry-Tools"><a href="#Telemetry-Tools" class="headerlink" title="Telemetry Tools"></a>Telemetry Tools</h2><p>Telemetry often relies on software agents running on the source systems to gather the data. In other cases, the source would be an application programming interface (API) to an application or monitoring tool. Connectors then manage the flow of data to multiple destinations and convert it to the protocols and data formats used by various analytical tools. Telemetry data also requires a storage site. This might be a data lake, a time-series database, or a security information and event management (SIEM) system.</p><ul><li>遥测通常依赖于源系统上运行的软件代理来收集数据。在其他情况下，源可能是应用进程或监视工具的应用进程编程接口 (API)。然后，连接器管理流向多个目的地的数据流，并将其转换为各种分析工具使用的协议和数据格式。遥测数据还需要一个存储站点。这可能是数据湖、时间串行数据库或安全信息和事件管理 (SIEM) 系统。</li></ul><p>Given the wide variety of sources of telemetry data, it can be useful to look for tools that comply with <strong>the OpenTelemetry Protocol</strong>, which describes the encoding, transport, and delivery mechanism of telemetry data between telemetry sources and destinations.</p>]]></content>
      
      
      <categories>
          
          <category> READ </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>RV-practice</title>
      <link href="/2024/07/12/RV-Arithmetic-practice/"/>
      <url>/2024/07/12/RV-Arithmetic-practice/</url>
      
        <content type="html"><![CDATA[<h2 id="汇编指令练习"><a href="#汇编指令练习" class="headerlink" title="汇编指令练习"></a>汇编指令练习</h2><ul><li>对sub执行反汇编，查看<code>sub x5, x6, x7</code>这条汇编指令对应的机器指令的编码，并对照RISC-V的specification解析该条指令的编码</li><li>现知道某条RISC-V的机器指令在内存中的值为<code>b3 05 95 00</code>, 从左往右从低地址到高地址，单位为字节，请将其翻译为对应的汇编指令。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://riscv.org/wp-content/uploads/2019/12/riscv-spec-20191213.pdf">The RISC-V Instruction Set Manual https://riscv.org/wp-content/uploads/2019/12/riscv-spec-20191213.pdf</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> RISCV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RV-Arithmetic-Instructions</title>
      <link href="/2024/07/12/RV-Arithmetic-Ins/"/>
      <url>/2024/07/12/RV-Arithmetic-Ins/</url>
      
        <content type="html"><![CDATA[<h2 id="算术运算指令-Arithmetic-Instructions"><a href="#算术运算指令-Arithmetic-Instructions" class="headerlink" title="算术运算指令 (Arithmetic Instructions)"></a>算术运算指令 (Arithmetic Instructions)</h2><table><thead><tr><th>指令</th><th>格式</th><th>语法</th><th>描述</th><th>例子</th></tr></thead><tbody><tr><td>AND</td><td>R-type</td><td>ADD RD,RS1,RS2</td><td>RS1和RS2的值相加，结果保存到RD</td><td>add x5,x6,x7</td></tr><tr><td>SUB</td><td>R-type</td><td>SUB RD,RS1,RS2</td><td>RS1的值减去RS2的值，结果保存到RD</td><td>sub x5,x6,x7</td></tr><tr><td>ADDI</td><td>I-type</td><td>ADDI RD,RS1,IMM</td><td>RS1的值和IMM相加，结果保存到RD</td><td>addi x5,x6,100</td></tr><tr><td>LUI</td><td>U-type</td><td>LUI RD,IMM</td><td>构造一个32bit的数，高20bit存放IMM，低12位清零。结果保存到RD</td><td>lui x5, 0x12345</td></tr><tr><td>AUIPC</td><td>U-type</td><td>AUIPC RD,IMM</td><td>构造一个32bit的数，高20bit存放IMM，低12位清零。结果和PC相加后保存到RD</td><td>auipc x5, 0x12345</td></tr></tbody></table><p>还有由基本的算术运算指令衍生的伪指令</p><table><thead><tr><th>指令</th><th>等价指令</th><th>语法</th><th>描述</th><th>例子</th></tr></thead><tbody><tr><td>LI</td><td>LUI + ADDI</td><td>LI RD,IMM</td><td>将立即数IMM加载到RD中</td><td>li x5, 0x12345678</td></tr><tr><td>LA</td><td>AUIPC + ADDI</td><td>LA RD,LABEL</td><td>为RD加载一个地址值</td><td>la x5, label</td></tr><tr><td>NEG</td><td>SUB RD,x0,RS</td><td>NEG RD,RS</td><td>对RS中的值取反并将结果存放在RD中</td><td>neg x5, x6</td></tr><tr><td>MV</td><td>ADDI RD,RS,0</td><td>MV RD,RS</td><td>将RS中的值拷贝到RD中</td><td>mv x5, x6</td></tr><tr><td>NOP</td><td>ADDI x0,x0,0</td><td>NOP</td><td>什么也不做</td><td>nop</td></tr></tbody></table><h3 id="ADD"><a href="#ADD" class="headerlink" title="ADD"></a>ADD</h3><table><thead><tr><th>指令</th><th>格式</th><th>语法</th><th>描述</th><th>例子</th></tr></thead><tbody><tr><td>AND</td><td>R-type</td><td>ADD RD,RS1,RS2</td><td>RS1和RS2的值相加，结果保存到RD</td><td>add x5,x6,x7</td></tr></tbody></table><p>具体编码规则如下：</p><table><thead><tr><th>funct7</th><th>rs2</th><th>rs1</th><th>funct3</th><th>rd</th><th>opcode</th></tr></thead><tbody><tr><td>0000000</td><td>x7</td><td>x6</td><td>000</td><td>x5</td><td>0110011</td></tr><tr><td>0000000</td><td>00111</td><td>00110</td><td>000</td><td>00101</td><td>0110011</td></tr></tbody></table><ul><li>二进制 <code>0000000-00111-00110-000-00101-0110011</code>转为16进制为<code>0x007302B3</code>，即为可执行文件中的二进制编码</li><li>注意编译生成的可执行文件的字节序问题：<code>00 73 02 B3</code> -&gt; <code>B3 02 73 00</code></li><li>示例汇编代码：</li></ul><pre><code class="s"># Add# Format:#   ADD RD, RS1, RS2# Description:#   The contents of RS1 is added to the contents of RS2 and the result is #   placed in RD.    .text           # Define beginning of text section    .global _start  # Define entry _start_start:    li x6, 1        # x6 = 1    li x7, 2        # x7 = 2    add x5, x6, x7  # x5 = x6 + x7stop:    j stop          # Infinite loop to stop execution    .end            # End of file</code></pre><h3 id="二进制补码？"><a href="#二进制补码？" class="headerlink" title="二进制补码？"></a>二进制补码？</h3><ul><li>无符号数 v.s. 有符号数</li><li>有符号数在计算中的表示：二进制补码(two’s complement)</li><li>符号扩展(Sign extension) v.s. 零扩展(Zero extension)</li><li>口诀：取绝对值、转二进制、取反+1</li><li>例如：<code>-4</code> 取绝对值、转二进制 -&gt; <code>100</code> 取反+1 -&gt; <code>011+1</code> <code>100</code></li><li>符号扩展 <code>11111-100</code></li><li>零扩展 <code>00000-100</code></li></ul><pre><code class="s"># Add# Format:#   ADD RD, RS1, RS2# Description:#   The contents of RS1 is added to the contents of RS2 and the result is #   placed in RD.    .text           # Define beginning of text section    .global _start  # Define entry _start_start:    li x6, 1        # x6 = 1    li x7, -2       # x7 = -2    add x5, x6, x7  # x5 = x6 + x7stop:    j stop          # Infinite loop to stop execution    .end            # End of file</code></pre><h3 id="SUB-Substract"><a href="#SUB-Substract" class="headerlink" title="SUB(Substract)"></a>SUB(Substract)</h3><table><thead><tr><th>指令</th><th>格式</th><th>语法</th><th>描述</th><th>例子</th></tr></thead><tbody><tr><td>SUB</td><td>R-type</td><td>SUB RD,RS1,RS2</td><td>RS1的值减去RS2的值，结果保存到RD</td><td>sub x5,x6,x7</td></tr></tbody></table><pre><code class="s"># Substract# Format:#   SUB RD, RS1, RS2# Description:#   The contents of RS2 is subtracted from the contents of RS1 and the result#   is placed in RD.    .text           # Define beginning of text section    .global_start  # Define entry _start_start:    li x6, -1       # x6 = -1    li x7, -2       # x7 = -2    sub x5, x6, x7      # x5 = x6 - x7stop:    j stop          # Infinite loop to stop execution    .end            # End of file</code></pre><h3 id="ADDI-ADD-Immediate"><a href="#ADDI-ADD-Immediate" class="headerlink" title="ADDI(ADD Immediate)"></a>ADDI(ADD Immediate)</h3><p>|imm[11:0]     |rs1  |funct3|rd   |opcode |<br>|—    —    |—  |—   |—  |—    |<br>|0000 0000 0000|x6   |000   |x5   |0110011|<br>|0000 0000 0000|00110|000   |00101|0110011|</p><h3 id="LUI-Load-Upper-Immediate"><a href="#LUI-Load-Upper-Immediate" class="headerlink" title="LUI(Load Upper Immediate)"></a>LUI(Load Upper Immediate)</h3><p>|imm[31:12]              |rd   |opcode |<br>|—    —              |—  |—    |<br>|0000 0000 0000 0000 0000|x5   |0110111|<br>|0000 0000 0000 0000 0000|00101|0110111|</p><h3 id="AUIPC-ADD-Upper-Immediate-Program-Counter"><a href="#AUIPC-ADD-Upper-Immediate-Program-Counter" class="headerlink" title="AUIPC(ADD Upper Immediate Program Counter)"></a>AUIPC(ADD Upper Immediate Program Counter)</h3><p>|imm[31:12]              |rd   |opcode |<br>|—    —              |—  |—    |<br>|0000 0000 0000 0000 0000|x5   |0110111|<br>|0000 0000 0000 0000 0000|00101|0110111|</p><h3 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h3><ul><li>对sub执行反汇编，查看<code>sub x5, x6, x7</code>这条汇编指令对应的机器指令的编码，并对照RISC-V的specification解析该条指令的编码</li><li>现知道某条RISC-V的机器指令在内存中的值为<code>b3 05 95 00</code>, 从左往右从低地址到高地址，单位为字节，请将其翻译为对应的汇编指令。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://riscv.org/wp-content/uploads/2019/12/riscv-spec-20191213.pdf">The RISC-V Instruction Set Manual https://riscv.org/wp-content/uploads/2019/12/riscv-spec-20191213.pdf</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> RISCV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DB-Cache</title>
      <link href="/2024/07/08/DB-Cache/"/>
      <url>/2024/07/08/DB-Cache/</url>
      
        <content type="html"><![CDATA[<ul><li>缓存失效</li><li>缓存穿透</li><li>缓存雪崩</li><li>缓存热点</li><li>数据不一致</li><li>数据并发竞争</li><li>Hot Key</li><li>Big Key</li></ul>]]></content>
      
      
      <categories>
          
          <category> DB </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Distribution </tag>
            
            <tag> Big Data </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SE-OOD</title>
      <link href="/2024/07/08/SE-OOD/"/>
      <url>/2024/07/08/SE-OOD/</url>
      
        <content type="html"><![CDATA[<h2 id="面向对象的23种设计模式"><a href="#面向对象的23种设计模式" class="headerlink" title="面向对象的23种设计模式"></a>面向对象的23种设计模式</h2><p>设计原则：高内聚低耦合，开闭原则。</p><h3 id="两大基础设计原则"><a href="#两大基础设计原则" class="headerlink" title="两大基础设计原则"></a>两大基础设计原则</h3><p>在说面向对象设计的六大原则之前，我们先来说下程序设计的原则：模块内高内聚，模块间低耦合。我们在面向对象时只需把类看成模块，那么就容易理解封装等了。</p><p>说是七大原则，这里我先提出来一个：对扩展开放，对修改关闭。 为啥这么说，因为我们都知道软件是要改的。对扩展开放保证了可以增加功能，像泛型啦这些。对修改关闭保证了像前的兼容性，jdk7兼容jdk6这样。所以开闭原则围绕软件的整个生命周期。</p><h3 id="从基础原则出发，产生六个具体的原则"><a href="#从基础原则出发，产生六个具体的原则" class="headerlink" title="从基础原则出发，产生六个具体的原则"></a>从基础原则出发，产生六个具体的原则</h3><ul><li>SOLID 设计原则是由 Robert C. Martin 在21世纪早期提出的便于记忆的首字母缩写，指代面向对象编程和面向对象设计的五个基本原则。</li><li>这些原则提供了一套方法论，帮助开发者避免设计上的常见陷阱，如过度耦合、缺乏灵活性、难以测试等问题，从而是代码更加易于理解、修改和扩展</li><li>SOLID 设计原则主要包括以下五项设计原则：<ul><li>单一职责原则（Single Responsibility Principle, SRP</li><li>开闭原则（Open-Closed Principle, OCP</li><li>里氏替换原则（Liskov Substitution Principle, LSP</li><li>接口隔离原则（Interface Segregation Principle, ISP）</li><li>依赖倒置原则（Dependency Inversion Principle, DIP）</li></ul></li></ul><ol><li>单一职责（一个方法或一个类只做一件事，为了模块内高内聚）</li><li>迪米特法则（也叫最少知道原则，为了模块间低耦合）</li><li>里氏替换（就是继承原则，子类可以无缝替代父类。很好的符合了开闭原则）</li><li>依赖倒置（类之间的依赖通过接口实现，低耦合的同时对扩展开放）</li><li>接口隔离（即把单个复杂接口拆分为多个独立接口，与上条共同实现面向接口编程）</li><li>合成复用原则（即尽量使用合成&#x2F;聚合的方式，而不是使用继承。主要为了防止继承滥用而导致的类之间耦合严重。记住只有符合继承原则时才用继承）</li></ol><h2 id="设计模式"><a href="#设计模式" class="headerlink" title="设计模式"></a>设计模式</h2><p>我觉得程序员最好的沟通方式是代码，所以每个设计模式都是一个例子。所有例子都很方便，可以复制直接运行。因为对java熟悉，所以下面设计模式例子都是用java语言来实现的。</p><h3 id="创建型模式（IOC：控制反转，就是创建分离的集大成）"><a href="#创建型模式（IOC：控制反转，就是创建分离的集大成）" class="headerlink" title="创建型模式（IOC：控制反转，就是创建分离的集大成）"></a>创建型模式（IOC：控制反转，就是创建分离的集大成）</h3><ol><li>Singleton:单例模式(全局只要一个实例)</li><li>Prototype:原型模式（通过拷贝原对象创建新对象）</li><li>Factory Method:工厂方法模式（对象创建可控，隐藏具体类名等实现解耦）</li><li>Abstract Factory:抽象工厂模式（解决对象与其属性匹配的工厂模式）</li><li>Builder:建造者模式（封装降低耦合，生成的对象与构造顺序无关）</li></ol><p>创建型模式的五种有各自的使用环境，单例和原型比较简单就不说了，工厂方法模式和建造者模式，都是封装和降低耦合有啥不同呢，其实工厂方法关注的是一个类有多个子类的对象创建（汽车类的各种品牌），而建造者模式关注的是属性较多的对象创建（能达到过程无关）。而抽象工厂模式关注的是对象和属性及属性与属性的匹配关系（如奥迪汽车与其发动机及空调的匹配）。</p><h3 id="结构型模式（对象的组成以及对象之间的依赖关系）"><a href="#结构型模式（对象的组成以及对象之间的依赖关系）" class="headerlink" title="结构型模式（对象的组成以及对象之间的依赖关系）"></a>结构型模式（对象的组成以及对象之间的依赖关系）</h3><ol><li>Adapter:适配器模式（适配不同接口和类，一般解决历史遗留问题）</li><li>Decorator:装饰器模式（比继承更灵活，可用排列组合形成多种扩展类）</li><li>Proxy:代理模式（可以给类的每个方法增加逻辑，如身份验证）</li><li>Facade:外观模式（对模块或产品的封装，降低耦合）</li><li>Bridge:桥接模式（就是接口模式，抽象与实现分离）</li><li>Plyweight:享元模式（相同对象的重用）</li><li>Composite:组合模式（整体和部分相同时，如文件夹包含文件夹）</li></ol><p>我们可以看到适配器模式、装饰器模式、代理模式都可以用包装对象来实现（把对象作为一个属性放在用的对象里），所以模式关注的并不是实现，而是解决的问题。模式更多体现的是类与类之间的逻辑关系，比如代理模式和装饰器模式很像。但从字面就知道，代理是访问不了实际工作对象的，这是他们的区别。</p><h3 id="行为型模式（即方法及其调用关系）"><a href="#行为型模式（即方法及其调用关系）" class="headerlink" title="行为型模式（即方法及其调用关系）"></a>行为型模式（即方法及其调用关系）</h3><p>行为型模式(即方法及其调用关系)</p><p>1.Strategy:策略模式 (提供功能不同实现方式，且实现可选) 2.Template Method:模板方法模式(相同流程用一个模板方法) 3.Observer:观察者模式(用订阅-发布实现的被观察者变化时回调)<br>4.Iterator:迭代器模式(一种内部实现无关的集合遍历模式)<br>5.Chain of Responsibility:责任链模式 (事件处理的分层结构产生的责任链条)<br>6.Command:命令模式 (将命令者与被命令者分离)<br>7.Memento:备忘录模式(需要撤销与恢复操作时使用)<br>8.Slate:状态模式(当对象两种状态差别很大时使用)<br>9.Visitor.访问者模式(当对同一对象有多种不同操作时使用)<br>10.Mediator:中介者模式(以中介为中心，将网状关系变成星型关系)<br>11.Interpreter:解释器模式(常用于纯文本的表达式执行)</p><p>写完设计模式之后感觉设计模式更多的一种逻辑关系，如果代码中有这种逻辑关系就可以用了。记得需要时候再用，不能为了设计模式而设计模式。没有什么就是好的，最主要用起来舒服吧。</p>]]></content>
      
      
      <categories>
          
          <category> SE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Software Engineering </tag>
            
            <tag> Object Oriented </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DB-NormalForms</title>
      <link href="/2024/07/08/DB-NormalForms/"/>
      <url>/2024/07/08/DB-NormalForms/</url>
      
        <content type="html"><![CDATA[<h2 id="数据库设计三范式"><a href="#数据库设计三范式" class="headerlink" title="数据库设计三范式"></a>数据库设计三范式</h2><ul><li>第一范式（1NF）：原子性（存储的数据应该具有“不可再分性”）</li><li>第二范式（2NF）：唯一性 (消除非主键部分依赖联合主键中的部分字段)（一定要在第一范式已经满足的情况下）</li><li>第三范式（3NF）：独立性，消除传递依赖(非主键值不依赖于另一个非主键值)</li></ul>]]></content>
      
      
      <categories>
          
          <category> DB </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Database System </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ML-Clustering-Introduction</title>
      <link href="/2024/07/06/ML-Clustering-Introduction/"/>
      <url>/2024/07/06/ML-Clustering-Introduction/</url>
      
        <content type="html"><![CDATA[<h2 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h2><p>For Clustering, the following exercises are included</p><ul><li>ML-clustering-Kmeans<ul><li>Exercise 1: kmeans clustering</li><li>Exercise 2: kmeans clustering result analysis using silhouette analysis¶</li><li>Exercise 3: How kmeans performs across datasets with different structures</li><li>Exercise 4: Silhouette analysis on these three datasets of different structures</li></ul></li><li>ML-clustering-Hierarchical<ul><li>Exercise 5: Hierarchical (Agglomerative) clustering dendrogram</li><li>Exercise 6: Hierarchical (Agglomerative) clustering results analysis</li><li>Exercise 7: Hierarchical (Agglomerative) clustering with different linkage methods</li><li>Exercise 8: Hierarchical (Agglomerative) clustering with silhouette analysis</li></ul></li><li>ML-clustering-DBSCAN<ul><li>Exercise 9: DBSCAN</li></ul></li><li>ML-clustering-Mixture<ul><li>Exercise 10: Gaussian Mixture Model</li><li>Exercise 11: Choose the covariance type of GMM using Bayesian Information Criterion (BIC)</li><li>Exercise 12: Choose the number of clusters using Bayesian Information Criterion (BIC)</li><li>Exercise 13: GMM clustering result analysis using silhouette analysis</li><li>Exercise 14: When GMM with BIC is powerful?</li><li>Exercise 15: GMM on three datasets of different structures</li></ul></li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul><li>Clustering analysis：Unsupervised learning</li><li>Kmeans: A partitioning-based method<ul><li>Initialization matters -&gt; K-means++</li><li>Choose number of clusters -&gt; silhouette analysis</li></ul></li><li>Hierarchical Clustering<ul><li>Dendrogram</li><li>Linkage methods matter</li></ul></li><li>DBSCAN<ul><li>Outlier&#x2F;noise detection</li><li>Choose ε can be a tricky task</li></ul></li><li>Mixture Models<ul><li>Limitations of k-means: Hard assignment to clusters -&gt; Soft (probabilistic) assignment</li><li>Mixture Models: Mixture of Gaussian<ul><li>Soft assignment</li><li>Model parameters {πk , μk , Σk}</li></ul></li><li>Expectation Maximization<ul><li>E-step: estimate cluster responsibilities given current parameter estimates</li><li>M-step: maximize likelihood over parameters given current responsibilities</li><li>Connection to k-means (Infinitely Small Variance Gaussian Mixture EM &#x3D; k-means)</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GaussDB</title>
      <link href="/2024/07/04/JAVA-GaussDB/"/>
      <url>/2024/07/04/JAVA-GaussDB/</url>
      
        <content type="html"><![CDATA[<h2 id="GaussDB"><a href="#GaussDB" class="headerlink" title="GaussDB"></a>GaussDB</h2><p>2024-07-04 工作小结:</p><ul><li>不同会话不能同时查询、修改等，会阻塞；</li><li>执行事务有一条失败，就会整个回滚；可使用savepoint设置达到Oracle类似效果</li><li>默认空间为目录，与Oracle不一样</li><li>空串“”和null是否一致（不确定），Oracle是一样的，MySQL是不一样的</li><li>建议严格astore，使用vaccum对无用数据进行清除</li><li>DDL自身不会提交，未提交的DDL操作会阻塞表或分区的操作；执行DDL时不会提交当前连接中的事务</li></ul>]]></content>
      
      
      <categories>
          
          <category> JAVA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 朱朱 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JavaNoteBook</title>
      <link href="/2024/07/03/JAVA-Note/"/>
      <url>/2024/07/03/JAVA-Note/</url>
      
        <content type="html"><![CDATA[<h2 id="重载与重写"><a href="#重载与重写" class="headerlink" title="重载与重写"></a>重载与重写</h2><h3 id="重载-Overload"><a href="#重载-Overload" class="headerlink" title="重载 Overload"></a>重载 Overload</h3><ul><li>在一个类中，同名的方法如果有不同的参数列表（参数类型不同、参数个数不同甚至是参数顺序不同）则视为重载。</li><li>同时，重载对返回类型没有要求，可以相同也可以不同，但不能通过返回类型是否相同来判断重载。</li></ul><pre><code class="java">public class Father &#123;    public static void main(String[] args) &#123;        // TODO Auto-generated method stub        Father s = new Father();        s.sayHello();        s.sayHello(&quot;Vila&quot;);    &#125;    public void sayHello() &#123;        System.out.println(&quot;Hello&quot;);    &#125;    public void sayHello(String name) &#123;        System.out.println(&quot;Hello&quot; + &quot; &quot; + name);    &#125;&#125;</code></pre><h3 id="重写-Override"><a href="#重写-Override" class="headerlink" title="重写 Override"></a>重写 Override</h3><ul><li>从字面上看，重写就是 重新写一遍的意思。其实就是在子类中把父类本身有的方法重新写一遍。子类继承了父类原有的方法，但有时子类并不想原封不动的继承父类中的某个方法，所以在方法名，参数列表，返回类型(除过子类中方法的返回值是父类中方法返回值的子类时)都相同的情况下，对方法体进行修改或重写，这就是重写。</li><li>但要注意子类函数的访问修饰权限不能少于父类的。</li></ul><pre><code class="java">public class Father &#123;    public static void main(String[] args) &#123;        // TODO Auto-generated method stub        Son s = new Son();        s.sayHello();    &#125;    public void sayHello() &#123;        System.out.println(&quot;Hello&quot;);    &#125;&#125;class Son extends Father&#123;    @Override    public void sayHello() &#123;        // TODO Auto-generated method stub        System.out.println(&quot;hello by &quot;);    &#125;&#125;</code></pre><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><table><thead><tr><th>Overide</th><th>Overload</th></tr></thead><tbody><tr><td>发生在父类与子类之间</td><td>一个类中多态性的一种表现</td></tr><tr><td>方法名，参数列表，返回类型（除过子类中方法的返回类型是父类中返回类型的子类）必须相同</td><td>重载要求同名方法的参数列表不同(参数类型，参数个数甚至是参数顺序)</td></tr><tr><td>访问修饰符的限制一定要大于被重写方法的访问修饰符（public&gt;protected&gt;default&gt;private)</td><td>重载的时候，返回值类型可以相同也可以不相同。无法以返回型别作为重载函数的区分标准</td></tr><tr><td>重写方法一定不能抛出新的检查异常或者比被重写方法申明更加宽泛的检查型异常</td><td></td></tr></tbody></table><h3 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h3><p>问：重载（Overload）和重写（Override）的区别？</p><p>答：方法的重载和重写都是实现多态的方式，区别在于前者实现的是编译时的多态性，而后者实现的是运行时的多态性。重载发生在一个类中，同名的方法如果有不同的参数列表（参数类型不同、参数个数不同或者二者都不同）则视为重载；重写发生在子类与父类之间，重写要求子类被重写方法与父类被重写方法有相同的参数列表，有兼容的返回类型，比父类被重写方法更好访问，不能比父类被重写方法声明更多的异常（里氏代换原则）。重载对返回类型没有特殊的要求，不能根据返回类型进行区分。</p>]]></content>
      
      
      <categories>
          
          <category> JAVA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 朱朱 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RV-Makefile</title>
      <link href="/2024/06/30/RV-Makefile/"/>
      <url>/2024/06/30/RV-Makefile/</url>
      
        <content type="html"><![CDATA[<h2 id="Makefile"><a href="#Makefile" class="headerlink" title="Makefile"></a>Makefile</h2><p>为了方便演示，后续的每一个案例代码将按照如下Makefile结构构建:</p><pre><code class="txt">asm/    demo1/        Makefile        test.s    demo2/        Makefile        test.s    demo3/        ...rule.mkcommon.mk</code></pre><pre><code class="makefile"># asm/demo/MakefileEXEC = testSRC = $&#123;EXEC&#125;.sGDBINIT = ../gdbinitinclude ../rule.mk</code></pre><pre><code class="makefile"># asm/rule.mkinclude ../common.mk.DEFAULT_GOAL := allall:    @$&#123;CC&#125; $&#123;CFLAGS&#125; $&#123;SRC&#125; -Ttext=0x80000000 -o $&#123;EXEC&#125;.elf    @$&#123;OBJCOPY&#125; -O binary $&#123;EXEC&#125;.elf $&#123;EXEC&#125;.bin.PHONY : runrun: all    @echo &quot;Press Ctrl-A and then X to exit QEMU&quot;    @echo &quot;------------------------------------&quot;    @echo &quot;No output, please run &#39;make debug&#39; to see details&quot;    @$&#123;QEMU&#125; $&#123;QFLAGS&#125; -kernel ./$&#123;EXEC&#125;.elf.PHONY : debugdebug: all    @echo &quot;Press Ctrl-C and then input &#39;quit&#39; to exit GDB and QEMU&quot;    @echo &quot;-------------------------------------------------------&quot;    @$&#123;QEMU&#125; $&#123;QFLAGS&#125; -kernel $&#123;EXEC&#125;.elf -s -S &amp;     @$&#123;GDB&#125; $&#123;EXEC&#125;.elf -q -x $&#123;GDBINIT&#125;...</code></pre><pre><code class="makefile">@$&#123;QEMU&#125; $&#123;QFLAGS&#125; -kernel $&#123;EXEC&#125;.elf -s -S &amp;</code></pre><p>QEMU模拟器启动参数</p><ul><li><code>-s</code>: 启动gdbserver调试服务</li><li><code>-S-</code>: suspend 断点等待</li><li><code>&amp;</code>: 后台运行</li></ul><pre><code class="makefile">@$&#123;GDB&#125; $&#123;EXEC&#125;.elf -q -x $&#123;GDBINIT&#125;</code></pre><p>QEMU模拟器启动参数</p><ul><li><code>-q</code>: quiet</li><li><code>-S-</code>: suspend 断点等待</li><li><code>x</code>: 执行 GDBINIT</li></ul><pre><code class="makefile"># common.mkCROSS_COMPILE = riscv64-unknown-elf-CFLAGS = -nostdlib -fno-builtin -march=rv32g -mabi=ilp32 -g -Wall# QEMU系统模式模拟QEMU = qemu-system-riscv32# 非图形界面 单核 virt设备类型QFLAGS = -nographic -smp 1 -machine virt -bios none  # 调试器GDB = gdb-multiarchCC = $&#123;CROSS_COMPILE&#125;gccOBJCOPY = $&#123;CROSS_COMPILE&#125;objcopyOBJDUMP = $&#123;CROSS_COMPILE&#125;objdump</code></pre><h2 id="gbdinit-文件"><a href="#gbdinit-文件" class="headerlink" title="gbdinit 文件"></a>gbdinit 文件</h2><pre><code class="gbdinit">display/z $x5display/z $x6display/z $x7set disassemble-next-line onb _starttarget remote : 1234c</code></pre><h2 id="第一个案例"><a href="#第一个案例" class="headerlink" title="第一个案例"></a>第一个案例</h2><p><strong>构建和使用说明</strong></p><ul><li><code>make</code>：编译构建</li><li><code>make run</code>：启动 qemu 并运行</li><li><code>make debug</code>：启动调试</li><li><code>make code</code>：反汇编查看二进制代码</li><li><code>make clean</code>：清理</li></ul><pre><code class="s"># Add# Format:# ADD RD, RS1, RS2# Description:# The contents of RS1 is added to the contents of RS2 and the result is # placed in RD.  .text     # Define beginning of text section  .global _start    # Define entry _start_start:  li x6, 1    # x6 = 1  li x7, 2    # x7 = 2  add x5, x6, x7    # x5 = x6 + x7stop:  j stop      # Infinite loop to stop execution  .end      # End of file</code></pre><h3 id="make-debug"><a href="#make-debug" class="headerlink" title="make debug"></a>make debug</h3><pre><code class="bash">&gt; make debug</code></pre><pre><code class="txt">Press Ctrl-C and then input &#39;quit&#39; to exit GDB and QEMU-------------------------------------------------------Reading symbols from test.elf...Breakpoint 1 at 0x80000000: file test.s, line 12.0x00001000 in ?? ()=&gt; 0x00001000:  97 02 00 00     auipc   t0,0x01: /z $x5 = 0x000000002: /z $x6 = 0x000000003: /z $x7 = 0x00000000Breakpoint 1, _start () at test.s:1212              li x6, 1                # x6 = 1--Type &lt;RET&gt; for more, q to quit, c to continue without paging--si=&gt; 0x80000000 &lt;_start+0&gt;:       13 03 10 00     li      t1,11: /z $x5 = 0x800000002: /z $x6 = 0x000000003: /z $x7 = 0x00000000(gdb) si13              li x7, 2                # x7 = 2=&gt; 0x80000004 &lt;_start+4&gt;:       93 03 20 00     li      t2,21: /z $x5 = 0x800000002: /z $x6 = 0x000000013: /z $x7 = 0x00000000(gdb) si14              add x5, x6, x7          # x5 = x6 + x7=&gt; 0x80000008 &lt;_start+8&gt;:       b3 02 73 00     add     t0,t1,t21: /z $x5 = 0x800000002: /z $x6 = 0x000000013: /z $x7 = 0x00000002(gdb) stop () at test.s:1717              j stop                  # Infinite loop to stop execution=&gt; 0x8000000c &lt;stop+0&gt;: 6f 00 00 00     j       0x8000000c &lt;stop&gt;1: /z $x5 = 0x000000032: /z $x6 = 0x000000013: /z $x7 = 0x00000002(gdb) 17              j stop                  # Infinite loop to stop execution=&gt; 0x8000000c &lt;stop+0&gt;: 6f 00 00 00     j       0x8000000c &lt;stop&gt;1: /z $x5 = 0x000000032: /z $x6 = 0x000000013: /z $x7 = 0x00000002(gdb) </code></pre><ul><li>在gbdinit文件中，定义了<code>display/z $x5</code>, 会显示当前寄存器的数值</li><li>在gbdinit文件中，定义了<code>set disassemble-next-line on</code>逐句编译, 可以输入<code>si</code>(single instruction), 执行下一条指令。</li></ul><h3 id="make-hex"><a href="#make-hex" class="headerlink" title="make hex"></a>make hex</h3><pre><code class="bash">&gt; make hex</code></pre><pre><code class="txt">00000000  13 03 10 00 93 03 20 00  b3 02 73 00 6f 00 00 00  |...... ...s.o...|00000010</code></pre><h3 id="make-code"><a href="#make-code" class="headerlink" title="make code"></a>make code</h3><pre><code class="bash">&gt; make code</code></pre><pre><code class="txt">test.elf:     file format elf32-littleriscvDisassembly of section .text:80000000 &lt;_start&gt;:        .text                   # Define beginning of text section        .global _start          # Define entry _start_start:        li x6, 1                # x6 = 180000000:       00100313                li      t1,1        li x7, 2                # x7 = 280000004:       00200393                li      t2,2        add x5, x6, x7          # x5 = x6 + x780000008:       007302b3                add     t0,t1,t28000000c &lt;stop&gt;:stop:        j stop                  # Infinite loop to stop execution8000000c:       0000006f                j       8000000c &lt;stop&gt;(END)</code></pre><p>上述案例显示了add操作经过编译后的结果<code>007302b3</code>。具体来说add属于R-type指令，通过查询手册得知，一条R-type指令的构成：</p><table><thead><tr><th>funct7</th><th>rs2</th><th>rs1</th><th>funct3</th><th>rd</th><th>opcode</th></tr></thead><tbody><tr><td>0000000</td><td>x7</td><td>x6</td><td>000</td><td>x5</td><td>0110011</td></tr><tr><td>0000000</td><td>00111</td><td>00110</td><td>000</td><td>00101</td><td>0110011</td></tr></tbody></table><ul><li>funct7、funct3、opcode确定了add指令</li><li>32bit 重新编排，注意字节序 <code>b3 02 73 00</code> -&gt; <code>00 73 02 b3</code></li></ul><table><thead><tr><th>0000</th><th>0000</th><th>0111</th><th>0011</th><th>0000</th><th>0010</th><th>1011</th><th>0011</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>7</td><td>3</td><td>0</td><td>2</td><td>b</td><td>3</td></tr></tbody></table><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://riscv.org/wp-content/uploads/2019/12/riscv-spec-20191213.pdf">The RISC-V Instruction Set Manual https://riscv.org/wp-content/uploads/2019/12/riscv-spec-20191213.pdf</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> RISCV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RV-Assemble Language</title>
      <link href="/2024/06/30/RV-Assemble-language/"/>
      <url>/2024/06/30/RV-Assemble-language/</url>
      
        <content type="html"><![CDATA[<h2 id="汇编语言入门-GNU"><a href="#汇编语言入门-GNU" class="headerlink" title="汇编语言入门(GNU)"></a>汇编语言入门(GNU)</h2><ul><li>一个完整的RISC-V汇编程序有多条<code>语句(statement)</code>组成</li><li>一条典型的RISC-V汇编语句由3部分组成 <code>statement = [label:][operation][comment]</code></li><li><code>label:</code>GNU汇编中，任何以冒号<code>:</code>结尾的标识符都被认为是一个标号。<ul><li>本质上是一个符号地址，地址别名</li></ul></li><li><code>operation</code>可以有以下多种类型：<ul><li><code>instruction</code>指令：直接对应二进制机器指令的字符串</li><li><code>pseudo-instruction</code>伪指令，类似与封装一个功能函数或者包含多条指令的脚本</li><li><code>directive</code>指示&#x2F;伪操作，以<code>.</code>开头，通知汇编器如何控制代码产生等，不对应具体的指令</li><li><code>macro</code>: 采用 .macro&#x2F;.endm 自定义的宏</li></ul></li><li><code>comment</code>：常用方式: <code>#</code>开始到当前行结束。</li></ul><pre><code class="s"># First RISC-V Assemble Sample.macro do_nothing   # directive    nop             # pseudo-instruction    nop             # pseudo-instruction.endm               # directive    .text           # directive    .global _start  # directive_start:             # Label    li x6, 5        # pseudo-instruction    li x7, 4        # pseudo-instruction    add x5, x6, x7  # instruction    do_nothing      # Calling macrostop:   j stop      # statement in one line    .end            # End of file</code></pre><h2 id="RISC-V汇编指令总览"><a href="#RISC-V汇编指令总览" class="headerlink" title="RISC-V汇编指令总览"></a>RISC-V汇编指令总览</h2><ul><li>RISC-V汇编指令操作对象<ul><li>寄存器<ul><li>32个通用寄存器，x0~x31 ( RV32I 通用寄存器组)</li><li>Hart 在执行算术逻辑运算时所操作的数据必须直接来自寄存器</li></ul></li><li>内存<ul><li>Hart 可以执行在寄存器和内存之间的数据读写操作；</li><li>读写操作使用<code>字节（Byte）</code>为基本单位进行寻址；</li><li>RV32 可以访问最多 2^32 个字节的内存空间。</li></ul></li></ul></li><li>RISC-V汇编指令编码格式<ul><li>指令长度：ILEN1&#x3D; 32 bits (RV32I)</li><li>指令对齐：IALIGN &#x3D; 32 bits (RV32I)</li><li>32 个 bit 划分成不同的 <code>“域（field）”</code></li><li><code>funct3/funct7</code>和<code>opcode</code>一起决定最终的指令类型</li><li>指令在内存中按照<code>小端序</code>排列</li></ul></li></ul><ul><li><p>RISC-V指令格式 6 种指令格式（format）</p><ul><li><code>R-type:（Register）</code>，每条指令中有三个 fields，用于指定 3 个 寄存器参数</li><li><code>I-type: Immediate）</code>，每条指令除了带有两个寄存器参数外，还带有一个立即数参数（宽度为 12 bits）。</li><li><code>S-type: （Store）</code>，每条指令除了带有两个寄存器参数外，还带有一个立即数参数（宽度为 12 bits，但 fields 的组织方式不同于 I-type）</li><li><code>B-type: (Branch)</code>，每条指令除了带有两个寄存器参数外，还带有一个立即数参数（宽度为 12 bits，但取值为 2 的倍数）。</li><li><code>U-type: （Upper）</code>，每条指令含有一个寄存器参数再加上一个立即数参数（宽度为 20  bits，用于表示一个立即数的高 20 位）</li><li><code>J-type: （Jump）</code>，每条指令含有一个寄存器参数再加上一个立即数参数（宽度为 20  bits）</li></ul></li><li><p>RISC-V汇编指令分类</p><ul><li>算术运算指令</li><li>逻辑运算指令</li><li>移位运算指令</li><li>内存读写指令</li><li>分支与跳转指令</li><li>…</li></ul></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li>The RISC-V Instruction Set Manual，Volume I: Unprivileged ISA，Document Version 20191213</li><li>Using as：<a href="https://sourceware.org/binutils/docs/as/">https://sourceware.org/binutils/docs/as/</a></li><li>How to Use Inline Assembly Language in C Code：<a href="https://gcc.gnu.org/onlinedocs/gcc/Using-Assembly-Language-with-C.html">https://gcc.gnu.org/onlinedocs/gcc/Using-Assembly-Language-with-C.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> RISCV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> assemble language </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RV-编译与链接</title>
      <link href="/2024/06/29/RV-%E7%BC%96%E8%AF%91%E4%B8%8E%E9%93%BE%E6%8E%A5/"/>
      <url>/2024/06/29/RV-%E7%BC%96%E8%AF%91%E4%B8%8E%E9%93%BE%E6%8E%A5/</url>
      
        <content type="html"><![CDATA[<h2 id="GCC简介"><a href="#GCC简介" class="headerlink" title="GCC简介"></a>GCC简介</h2><ul><li><a href="https://gcc.gnu.org/">GCC (GNU Compiler Collection)</a></li><li>由 GNU开发的，遵循 GPL 许可证发行的编译器套件。</li><li>支持 C、C++、Objective-C、Fortran、Ada 和 Go 语言等多种语言前端，已被移植到多种计算机体系架构上，如 x86、ARM、RISC-V 等。</li><li>GCC 的初衷是为 GNU 操作系统专门编写一款编译器，现已被大多数 “Unix-like”操作系统（如 Linux、BSD、MacOS 等）采纳为标准的编译器。</li></ul><h3 id="GCC命令格式"><a href="#GCC命令格式" class="headerlink" title="GCC命令格式"></a>GCC命令格式</h3><ul><li><strong>gcc [option] [filenames]</strong></li></ul><table><thead><tr><th>Option</th><th>含义</th></tr></thead><tbody><tr><td>-E</td><td>只作预处理</td></tr><tr><td>-c</td><td>只编译不链接，生成目标文件”.o”</td></tr><tr><td>-S</td><td>生成汇编代码</td></tr><tr><td>-o file</td><td>把输出生成到由file指定文件名的文件中</td></tr><tr><td>-g</td><td>把输出的文件中加入支持调试的信息</td></tr><tr><td>-v</td><td>显示输出详细的命令执行过程信息</td></tr></tbody></table><h3 id="GCC涉及的文件类型"><a href="#GCC涉及的文件类型" class="headerlink" title="GCC涉及的文件类型"></a>GCC涉及的文件类型</h3><table><thead><tr><th>文件类型</th><th>描述</th></tr></thead><tbody><tr><td>.c</td><td>C 源文件</td></tr><tr><td>.i</td><td>预处理后的 C 源文件</td></tr><tr><td>.s&#x2F;.S</td><td>汇编代码</td></tr><tr><td>.h</td><td>头文件</td></tr><tr><td>.o</td><td>目标文件</td></tr><tr><td>.a</td><td>静态库文件</td></tr><tr><td>.so</td><td>动态库文件</td></tr><tr><td>.out</td><td>可执行文件</td></tr></tbody></table><h3 id="GDB调试步骤"><a href="#GDB调试步骤" class="headerlink" title="GDB调试步骤"></a>GDB调试步骤</h3><ul><li>Editor[foo.c] –&gt; Preprocessor[gcc -E foo.c -o foo.i]</li><li>Preprocessor –&gt; Compiler[gcc -S foo.i -o foo.s]</li><li>Compiler –&gt; Assembler[gcc -c foo.s -o foo.o]</li><li>Assembler –&gt; Linker[gcc foo.o -o a.out]</li><li>.C –&gt; .i –&gt; .s –&gt; .o</li></ul><h2 id="ELF文件格式"><a href="#ELF文件格式" class="headerlink" title="ELF文件格式"></a>ELF文件格式</h2><ul><li>ELF（Executable and Linkable Format，可执行和链接格式）是一种 Unix-like系统上的二进制文件格式，由 GNU 开源软件基金会开发，用于描述可执行文件、共享库、动态链接库、object 文件等。</li><li>ELF标准中定义的采用ELF格式的文件分为4类：</li></ul><table><thead><tr><th>ELF文件类型</th><th>说明</th><th>实例</th></tr></thead><tbody><tr><td>Executable File</td><td>可执行文件，可以运行在操作系统上</td><td>Linux上的.out文件</td></tr><tr><td>Relocatable File</td><td>可重定位文件，内容包含了代码和数据，可以被链接成可执行文件或者共享目标文件</td><td>.o 文件</td></tr><tr><td>Shared Object File</td><td>共享目标文件，内容包含了代码和数据，可以链接到动态链接库或者可执行文件</td><td>.so文件</td></tr><tr><td>Core Dump File</td><td>核心转储文件，进程意外终止时，用于存储程序崩溃时的状态信息，以供调试分析</td><td>core文件</td></tr></tbody></table><h3 id="Binutils-ELF文件处理工具"><a href="#Binutils-ELF文件处理工具" class="headerlink" title="Binutils - ELF文件处理工具"></a>Binutils - ELF文件处理工具</h3><p><a href="https://www.gnu.org/software/binutils/">Binutils</a></p><ul><li>ar：归档文件，将多个文件打包成一个大文件。</li><li>as：被 gcc 调用，输入汇编文件，输出目标文件供链接器 ld 连接。</li><li>ld：GNU 链接器。被 gcc 调用，它把目标文件和各种库文件结合在一起，重定位数据，并链接符号引用。</li><li>objcopy：执行文件格式转换。</li><li>objdump：显示 ELF 文件的信息。</li><li>readelf：显示更多 ELF 格式文件的信息（包括 DWARF 调试信息）。</li><li>……</li></ul><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><h3 id="练习-1"><a href="#练习-1" class="headerlink" title="练习 1"></a>练习 1</h3><p>使用 gcc 编译代码并使用 binutils 工具对生成的目标文件和可执行文件（ELF 格式）进行分析。具体要求如下：</p><ul><li>编写一个简单的打印 “hello world！” 的程序源文件：hello.c</li><li>对源文件进行本地编译，生成针对支持 x86_64 指令集架构处理器的目标文件 hello.o。</li><li>查看 hello.o 的文件的文件头信息。</li><li>查看 hello.o 的 Section header table。</li><li>对 hello.o 反汇编，并查看 hello.c 的 C 程序源码和机器指令的对应关系。</li></ul><p><strong>解答：</strong></p><p>1.编写一个简单的打印 “hello world！” 的程序源文件：hello.c :</p><pre><code class="c">#include &lt;stdio.h&gt;void main()&#123;    printf(&quot;Hello, world!\n&quot;);&#125;</code></pre><p>2.对源文件进行本地编译，生成针对支持 x86_64 指令集架构处理器的目标文件 hello.o :</p><pre><code class="bash">&gt; gcc -c hello.c -o hello.o</code></pre><p>3.查看 hello.o 的文件的文件头信息。</p><pre><code class="bash">&gt; readelf -h hello.oELF Header:  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00   Class:                             ELF64  Data:                              2&#39;s complement, little endian  Version:                           1 (current)  OS/ABI:                            UNIX - System V  ABI Version:                       0  Type:                              REL (Relocatable file)  Machine:                           Advanced Micro Devices X86-64  Version:                           0x1  Entry point address:               0x0  Start of program headers:          0 (bytes into file)  Start of section headers:          784 (bytes into file)  Flags:                             0x0  Size of this header:               64 (bytes)  Size of program headers:           0 (bytes)  Number of program headers:         0  Size of section headers:           64 (bytes)  Number of section headers:         14  Section header string table index: 13</code></pre><p>4.查看 hello.o 的 Section header table。</p><pre><code class="bash">&gt; readelf -SW hello.o  // 这里的W参数是让命令行输出样式 &quot;widely&quot;There are 14 section headers, starting at offset 0x310:Section Headers:  [Nr] Name              Type            Address          Off    Size   ES Flg Lk Inf Al  [ 0]                   NULL            0000000000000000 000000 000000 00      0   0  0  [ 1] .text             PROGBITS        0000000000000000 000040 000017 00  AX  0   0  1  [ 2] .rela.text        RELA            0000000000000000 000250 000030 18   I 11   1  8  [ 3] .data             PROGBITS        0000000000000000 000057 000000 00  WA  0   0  1  [ 4] .bss              NOBITS          0000000000000000 000057 000000 00  WA  0   0  1  [ 5] .rodata           PROGBITS        0000000000000000 000057 00000d 00   A  0   0  1  [ 6] .comment          PROGBITS        0000000000000000 000064 00002c 01  MS  0   0  1  [ 7] .note.GNU-stack   PROGBITS        0000000000000000 000090 000000 00      0   0  1  [ 8] .note.gnu.property NOTE            0000000000000000 000090 000020 00   A  0   0  8  [ 9] .eh_frame         PROGBITS        0000000000000000 0000b0 000038 00   A  0   0  8  [10] .rela.eh_frame    RELA            0000000000000000 000280 000018 18   I 11   9  8  [11] .symtab           SYMTAB          0000000000000000 0000e8 000138 18     12  10  8  [12] .strtab           STRTAB          0000000000000000 000220 000029 00      0   0  1  [13] .shstrtab         STRTAB          0000000000000000 000298 000074 00      0   0  1Key to Flags:  W (write), A (alloc), X (execute), M (merge), S (strings), I (info),  L (link order), O (extra OS processing required), G (group), T (TLS),  C (compressed), x (unknown), o (OS specific), E (exclude),  l (large), p (processor specific)</code></pre><p>5.对 hello.o 反汇编，并查看 hello.c 的 C 程序源码和机器指令的对应关系。重新编译并添加调试信息命令：gcc -g -c</p><pre><code class="bash">&gt; rm hello.o&gt; gcc -g -c hello.c&gt; objdump -S hello.ohello.o:     file format elf64-x86-64Disassembly of section .text:0000000000000000 &lt;main&gt;:#include &lt;stdio.h&gt;void main()&#123;   0:   f3 0f 1e fa             endbr64    4:   55                      push   %rbp   5:   48 89 e5                mov    %rsp,%rbp        printf(&quot;hellp world!\n&quot;);   8:   48 8d 3d 00 00 00 00    lea    0x0(%rip),%rdi        # f &lt;main+0xf&gt;   f:   e8 00 00 00 00          callq  14 &lt;main+0x14&gt;&#125;  14:   90                      nop  15:   5d                      pop    %rbp  16:   c3                      retq   </code></pre><h2 id="练习-2"><a href="#练习-2" class="headerlink" title="练习 2"></a>练习 2</h2><p>如下例子 C 语言代码 example.c：</p><pre><code class="c">#include &lt;stdio.h&gt; /* unintialize */int global_uninit;const int global_uninit_const;static int global_uninit_static;const static int global_uninit_static_const;/* initialize */int global_init = 1;const int global_init_const = 2;static int global_init_static = 3;const static int global_init_static_const = 4; void main() &#123;        /* unintialize */        int local_uninit;        const int local_uninit_const;        static int local_uninit_static;        const static int local_uninit_static_const;        /* initialize */        int local_init = 5;        const int local_init_const = 6;        static int local_init_static = 7;        const static int local_init_static_const = 8;        // string        printf(&quot;hello world!\n&quot;);         return; &#125;</code></pre><p>请问编译为 .o 文件后，global_init, global_init_const, global_init_static, local_uninit, local_init 等这些变量分别存放在那些 section 里，”hello world!\n” 这个字符串又在哪里？并尝试用工具查看并验证你的猜测。</p><ul><li>可以使用 gcc -c example.c 命令来编译生成 .o 文件，然后使用 readelf -a example.o 命令查看文件的详细信息。其中，可以查看到各个 section 的起始地址、大小等信息，也可以查看到字符串常量的位置和内容信息。</li></ul><pre><code class="bash">&gt; gcc -c example.c -o example.o&gt; readelf -a examlple.o&gt; readelf -aW example.oELF Header:  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00   Class:                             ELF64  Data:                              2&#39;s complement, little endian  Version:                           1 (current)  OS/ABI:                            UNIX - System V  ABI Version:                       0  Type:                              REL (Relocatable file)  Machine:                           Advanced Micro Devices X86-64  Version:                           0x1  Entry point address:               0x0  Start of program headers:          0 (bytes into file)  Start of section headers:          1392 (bytes into file)  Flags:                             0x0  Size of this header:               64 (bytes)  Size of program headers:           0 (bytes)  Number of program headers:         0  Size of section headers:           64 (bytes)  Number of section headers:         14  Section header string table index: 13Section Headers:  [Nr] Name              Type            Address          Off    Size   ES Flg Lk Inf Al  [ 0]                   NULL            0000000000000000 000000 000000 00      0   0  0  [ 1] .text             PROGBITS        0000000000000000 000040 000029 00  AX  0   0  1  [ 2] .rela.text        RELA            0000000000000000 0004b0 000030 18   I 11   1  8  [ 3] .data             PROGBITS        0000000000000000 00006c 00000c 00  WA  0   0  4  [ 4] .bss              NOBITS          0000000000000000 000078 000008 00  WA  0   0  4  [ 5] .rodata           PROGBITS        0000000000000000 000078 000024 00   A  0   0  4  [ 6] .comment          PROGBITS        0000000000000000 00009c 00002c 01  MS  0   0  1  [ 7] .note.GNU-stack   PROGBITS        0000000000000000 0000c8 000000 00      0   0  1  [ 8] .note.gnu.property NOTE            0000000000000000 0000c8 000020 00   A  0   0  8  [ 9] .eh_frame         PROGBITS        0000000000000000 0000e8 000038 00   A  0   0  8  [10] .rela.eh_frame    RELA            0000000000000000 0004e0 000018 18   I 11   9  8  [11] .symtab           SYMTAB          0000000000000000 000120 000258 18     12  18  8  [12] .strtab           STRTAB          0000000000000000 000378 000133 00      0   0  1  [13] .shstrtab         STRTAB          0000000000000000 0004f8 000074 00      0   0  1Key to Flags:  W (write), A (alloc), X (execute), M (merge), S (strings), I (info),  L (link order), O (extra OS processing required), G (group), T (TLS),  C (compressed), x (unknown), o (OS specific), E (exclude),  l (large), p (processor specific)...Symbol table &#39;.symtab&#39; contains 25 entries:   Num:    Value          Size Type    Bind   Vis      Ndx Name     0: 0000000000000000     0 NOTYPE  LOCAL  DEFAULT  UND      1: 0000000000000000     0 FILE    LOCAL  DEFAULT  ABS example.c     2: 0000000000000000     0 SECTION LOCAL  DEFAULT    1      3: 0000000000000000     0 SECTION LOCAL  DEFAULT    3      4: 0000000000000000     0 SECTION LOCAL  DEFAULT    4      5: 0000000000000000     4 OBJECT  LOCAL  DEFAULT    4 global_uninit_static     6: 0000000000000000     0 SECTION LOCAL  DEFAULT    5      7: 0000000000000000     4 OBJECT  LOCAL  DEFAULT    5 global_uninit_static_const     8: 0000000000000004     4 OBJECT  LOCAL  DEFAULT    3 global_init_static     9: 0000000000000008     4 OBJECT  LOCAL  DEFAULT    5 global_init_static_const    10: 000000000000001c     4 OBJECT  LOCAL  DEFAULT    5 local_init_static_const.2330    11: 0000000000000008     4 OBJECT  LOCAL  DEFAULT    3 local_init_static.2329    12: 0000000000000020     4 OBJECT  LOCAL  DEFAULT    5 local_uninit_static_const.2326    13: 0000000000000004     4 OBJECT  LOCAL  DEFAULT    4 local_uninit_static.2325    14: 0000000000000000     0 SECTION LOCAL  DEFAULT    7     15: 0000000000000000     0 SECTION LOCAL  DEFAULT    8     16: 0000000000000000     0 SECTION LOCAL  DEFAULT    9     17: 0000000000000000     0 SECTION LOCAL  DEFAULT    6     18: 0000000000000004     4 OBJECT  GLOBAL DEFAULT  COM global_uninit    19: 0000000000000004     4 OBJECT  GLOBAL DEFAULT  COM global_uninit_const    20: 0000000000000000     4 OBJECT  GLOBAL DEFAULT    3 global_init    21: 0000000000000004     4 OBJECT  GLOBAL DEFAULT    5 global_init_const    22: 0000000000000000    41 FUNC    GLOBAL DEFAULT    1 main    23: 0000000000000000     0 NOTYPE  GLOBAL DEFAULT  UND _GLOBAL_OFFSET_TABLE_    24: 0000000000000000     0 NOTYPE  GLOBAL DEFAULT  UND puts...</code></pre><p><strong>解答：</strong> 编译为 .o 文件后，这些变量和字符串会被放置在不同的 section 中，从返回的ELF信息可以看到，</p><ul><li>[3]号节即.data section：global_init, global_init_static, local_init_static</li><li>栈: Auto 自动变量：local_init</li><li>[4]号节即.bss section：global_uninit_static, local_uninit_static</li><li>[5]号节即.rodata section：包含const声明的对象(除了global_uninit_const)</li><li>COM 公共变量：global_uninit, global_uninit_const</li></ul><p>【C语言复习】</p><ul><li>存储类型 static 的作用：<ul><li>块外：外部链接改为内部链接</li><li>块内：自动存储期限改为静态存储期限</li></ul></li><li>类型限定符 const 的作用：只读</li></ul><h2 id="练习-3"><a href="#练习-3" class="headerlink" title="练习 3"></a>练习 3</h2><p>熟悉交叉编译概念，使用 riscv gcc 编译代码并使用 binutils 工具对生成的目标文件和可执行文件（ELF 格式）进行分析。具体要求如下：</p><ul><li>编写一个简单的打印 “hello world！” 的程序源文件：hello.c</li><li>对源文件进行编译，生成针对支持 rv32ima 指令集架构处理器的目标文件 hello.o。</li><li>查看 hello.o 的文件的文件头信息。</li><li>查看 hello.o 的 Section header table。</li><li>对 hello.o 反汇编，并查看 hello.c 的 C</li></ul><p><strong>解答</strong></p><p>1.编写一个简单的打印 “hello world！” 的程序源文件：hello.c</p><pre><code class="c">#include &lt;stdio.h&gt;void main()&#123;    printf(&quot;hellp world!\n&quot;);&#125;</code></pre><pre><code class="bash">&gt; riscv64-unknown-elf-gcc -march=rv32ima -mabi=ilp32 hello.c hello.c:1:10: fatal error: stdio.h: No such file or directory    1 | #include &lt;stdio.h&gt;      |          ^~~~~~~~~compilation terminated.</code></pre><p>如果报错，可以换成riscv64-linux-gun-gcc</p><pre><code class="bash">$ sudo apt -y install gcc-riscv64-linux-gun$ riscv64-linux-gun-gcc hello.c$ file a.outa.out: ELF 64-bit LSB shared object, UCB RISC-V, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-riscv64-lp64d.so.1, BuildID[sha1]=6b3277a79e18ad6acfa00fb14e8f73a2b33fa3a3, for GNU/Linux 4.15.0, not stripped</code></pre><p>2.对源文件进行编译，生成针对支持 rv32ima 指令集架构处理器的目标文件 hello.o。</p><pre><code class="bash">&gt; riscv64-linux-gnu-gcc -c hello.c -o hello.o</code></pre><p>3.查看 hello.o 的文件的文件头信息。</p><pre><code class="bash">&gt; readelf -h hello.oELF Header:  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00   Class:                             ELF64  Data:                              2&#39;s complement, little endian  Version:                           1 (current)  OS/ABI:                            UNIX - System V  ABI Version:                       0  Type:                              REL (Relocatable file)  Machine:                           RISC-V  Version:                           0x1  Entry point address:               0x0  Start of program headers:          0 (bytes into file)  Start of section headers:          712 (bytes into file)  Flags:                             0x5, RVC, double-float ABI  Size of this header:               64 (bytes)  Size of program headers:           0 (bytes)  Number of program headers:         0  Size of section headers:           64 (bytes)  Number of section headers:         11  Section header string table index: 10</code></pre><p>Machine: RISC-V</p><p>4.查看 hello.o 的 Section header table。</p><pre><code class="bash">&gt; readelf -SW hello.oThere are 11 section headers, starting at offset 0x2c8:Section Headers:  [Nr] Name              Type            Address          Off    Size   ES Flg Lk Inf Al  [ 0]                   NULL            0000000000000000 000000 000000 00      0   0  0  [ 1] .text             PROGBITS        0000000000000000 000040 000022 00  AX  0   0  2  [ 2] .rela.text        RELA            0000000000000000 0001e0 000090 18   I  8   1  8  [ 3] .data             PROGBITS        0000000000000000 000062 000000 00  WA  0   0  1  [ 4] .bss              NOBITS          0000000000000000 000062 000000 00  WA  0   0  1  [ 5] .rodata           PROGBITS        0000000000000000 000068 00000d 00   A  0   0  8  [ 6] .comment          PROGBITS        0000000000000000 000075 00002a 01  MS  0   0  1  [ 7] .note.GNU-stack   PROGBITS        0000000000000000 00009f 000000 00      0   0  1  [ 8] .symtab           SYMTAB          0000000000000000 0000a0 000120 18      9  10  8  [ 9] .strtab           STRTAB          0000000000000000 0001c0 00001d 00      0   0  1  [10] .shstrtab         STRTAB          0000000000000000 000270 000052 00      0   0  1Key to Flags:  W (write), A (alloc), X (execute), M (merge), S (strings), I (info),  L (link order), O (extra OS processing required), G (group), T (TLS),  C (compressed), x (unknown), o (OS specific), E (exclude),  p (processor specific)</code></pre><p>对比linux-gcc编译的hello.o 少了3个section</p><pre><code class="bash">[ 8] .note.gnu.property NOTE[ 9] .eh_frame         PROGBITS [10] .rela.eh_frame    RELA </code></pre><p>5.对 hello.o 反汇编，并查看 hello.c 的 C</p><pre><code class="bash">$ riscv64-linux-gnu-gcc -g -c hello.c$ riscv64-linux-gnu-objdump -d hello.ohello.o:     file format elf64-littleriscvDisassembly of section .text:0000000000000000 &lt;main&gt;:#include &lt;stdio.h&gt;void main()&#123;   0:   1141                    addi    sp,sp,-16   2:   e406                    sd      ra,8(sp)   4:   e022                    sd      s0,0(sp)   6:   0800                    addi    s0,sp,16        printf(&quot;hellp world!\n&quot;);   8:   00000517                auipc   a0,0x0   c:   00050513                mv      a0,a0  10:   00000097                auipc   ra,0x0  14:   000080e7                jalr    ra # 10 &lt;main+0x10&gt;&#125;  18:   0001                    nop  1a:   60a2                    ld      ra,8(sp)  1c:   6402                    ld      s0,0(sp)  1e:   0141                    addi    sp,sp,16  20:   8082                    ret</code></pre><h2 id="练习-4"><a href="#练习-4" class="headerlink" title="练习 4"></a>练习 4</h2><p>基于 练习 3 继续熟悉 qemu&#x2F;gdb 等工具的使用，具体要求如下：</p><ul><li>将 hello.c 编译成可调式版本的可执行程序 a.out</li><li>先执行 qemu-riscv32 运行 a.out。</li><li>使用 qemu-riscv32 和 gdb 调试 a.out。</li></ul><pre><code class="bash">&gt; riscv64-linux-gnu-gcc -march=rv32im -mabi=ilp32 -g hello.c -o a.out&gt; qemu-riscv32 ./a.out</code></pre><h2 id="练习-5"><a href="#练习-5" class="headerlink" title="练习 5"></a>练习 5</h2><p>自学 Makefile 的语法，理解在 riscv 仓库的根目录下执行 make 会发生什么。</p><pre><code class="makefile">include ../../common.mkSRCS_ASM = \    start.S \SRCS_C = \    kernel.c \OBJS = $(SRCS_ASM:.S=.o)OBJS += $(SRCS_C:.c=.o).DEFAULT_GOAL := allall: os.elf# start.o must be the first in dependency!os.elf: $&#123;OBJS&#125;    $&#123;CC&#125; $&#123;CFLAGS&#125; -Ttext=0x80000000 -o os.elf $^    $&#123;OBJCOPY&#125; -O binary os.elf os.bin%.o : %.c    $&#123;CC&#125; $&#123;CFLAGS&#125; -c -o $@ $&lt;%.o : %.S    $&#123;CC&#125; $&#123;CFLAGS&#125; -c -o $@ $&lt;run: all    @$&#123;QEMU&#125; -M ? | grep virt &gt;/dev/null || exit    @echo &quot;Press Ctrl-A and then X to exit QEMU&quot;    @echo &quot;------------------------------------&quot;    @$&#123;QEMU&#125; $&#123;QFLAGS&#125; -kernel os.elf.PHONY : debugdebug: all    @echo &quot;Press Ctrl-C and then input &#39;quit&#39; to exit GDB and QEMU&quot;    @echo &quot;-------------------------------------------------------&quot;    @$&#123;QEMU&#125; $&#123;QFLAGS&#125; -kernel os.elf -s -S &amp;    @$&#123;GDB&#125; os.elf -q -x ../gdbinit.PHONY : codecode: all    @$&#123;OBJDUMP&#125; -S os.elf | less.PHONY : cleanclean:    rm -rf *.o *.bin *.elf</code></pre>]]></content>
      
      
      <categories>
          
          <category> RISCV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RV-RISC-V preliminaries</title>
      <link href="/2024/06/29/RV-pre/"/>
      <url>/2024/06/29/RV-pre/</url>
      
        <content type="html"><![CDATA[<p>从零开始为 RISC-V 平台编写一个简单的操作系统内核。</p><h2 id="1-环境配置"><a href="#1-环境配置" class="headerlink" title="1.环境配置"></a>1.环境配置</h2><p>推荐使用 Ubuntu 20.04，Ubuntu 20.04 是目前最新的 Ubuntu 长期稳定发行版，在这个环境下安装运行环境也最简单。</p><p>所有演示代码在以下环境下验证通过，请仔细核对你的 Ubuntu 版本和内核版本与以下信息是否一致。</p><pre><code class="bash">$ lsb_release -aNo LSB modules are available.Distributor ID:UbuntuDescription:Ubuntu 20.04.3 LTSRelease:20.04Codename:focal$ uname -r5.11.0-27-generic</code></pre><p>目前在 Ubuntu 20.04 环境下我们可以直接使用官方提供的 GNU工具链和 QEMU 模拟器，执行如下命令在线安装即可开始试验：</p><pre><code class="bash">$ sudo apt update$ sudo apt install build-essential gcc make perl dkms git gcc-riscv64-unknown-elf gdb-multiarch qemu-system-misc</code></pre><p>以下是每个依赖项的简要说明：</p><ul><li>build-essential：该软件包提供了编译和构建软件所需的基本工具，包括编译器和C库。</li><li>gcc：GNU编译器集合，用于编译C和C++程序。</li><li>make：一个构建工具，用于自动化软件构建过程。</li><li>perl：一种脚本语言，常用于文本处理和系统管理任务。</li><li>dkms：动态内核模块支持，用于自动编译和安装内核模块。</li><li>git：版本控制系统，用于跟踪和管理项目代码。</li><li>gcc-riscv64-unknown-elf：RISC-V架构的交叉编译器，用于编译RISC-V架构的程序。</li><li>gdb-multiarch：一个多架构调试器，支持多种架构的程序调试。</li><li>qemu-system-misc：QEMU模拟器的一部分，包含了一些杂项工具和二进制文件。</li></ul><p>使用包管理器：在终端中运行dpkg -s &lt;package-name&gt; 命令，将&lt;package-name&gt;替换为要检查的软件包名称。如果软件包已成功安装，将显示软件包的详细信息；否则，将显示软件包未安装的信息。例如，dpkg -s gcc将检查GCC编译器是否已安装。</p><pre><code class="bash">$ dpkg -s gccPackage: gccStatus: install ok installedPriority: optionalSection: develInstalled-Size: 50Maintainer: Ubuntu Developers &lt;ubuntu-devel-discuss@lists.ubuntu.com&gt;Architecture: amd64Source: gcc-defaults (1.185.1ubuntu2)Version: 4:9.3.0-1ubuntu2Provides: c-compiler, gcc-x86-64-linux-gnu (= 4:9.3.0-1ubuntu2)Depends: cpp (= 4:9.3.0-1ubuntu2), gcc-9 (&gt;= 9.3.0-3~)Recommends: libc6-dev | libc-devSuggests: gcc-multilib, make, manpages-dev, autoconf, automake, libtool, flex, bison, gdb, gcc-docConflicts: gcc-doc (&lt;&lt; 1:2.95.3)Description: GNU C compiler This is the GNU C compiler, a fairly portable optimizing compiler for C. . This is a dependency package providing the default GNU C compiler.Original-Maintainer: Debian GCC Maintainers &lt;debian-gcc@lists.debian.org&gt;</code></pre><h2 id="2-构建和使用说明"><a href="#2-构建和使用说明" class="headerlink" title="2. 构建和使用说明"></a>2. 构建和使用说明</h2><ul><li><code>make</code>：编译构建</li><li><code>make run</code>：启动 qemu 并运行</li><li><code>make debug</code>：启动调试</li><li><code>make code</code>：反汇编查看二进制代码</li><li><code>make clean</code>：清理</li></ul><p>具体使用请参考具体子项目下的 Makefile 文件。</p><h2 id="3-参考文献"><a href="#3-参考文献" class="headerlink" title="3. 参考文献"></a>3. 参考文献</h2><p>本项目的设计参考了如下网络资源，在此表示感谢 :)</p><ul><li>The Adventures of OS：<a href="https://osblog.stephenmarz.com/index.html">https://osblog.stephenmarz.com/index.html</a></li><li>mini-riscv-os: <a href="https://github.com/cccriscv/mini-riscv-os">https://github.com/cccriscv/mini-riscv-os</a></li><li>Xv6, a simple Unix-like teaching operating system：<a href="https://pdos.csail.mit.edu/6.828/2020/xv6.html">https://pdos.csail.mit.edu/6.828/2020/xv6.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> RISCV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WSL/git命令速查</title>
      <link href="/2024/06/29/Tools-WSL%E5%91%BD%E4%BB%A4%E9%80%9F%E6%9F%A5/"/>
      <url>/2024/06/29/Tools-WSL%E5%91%BD%E4%BB%A4%E9%80%9F%E6%9F%A5/</url>
      
        <content type="html"><![CDATA[<p>列出可用的Linux发行版</p><pre><code class="PowerShell">wsl --list --online</code></pre><p>列出已安装的 Linux 发行版</p><pre><code class="PowerShell">wsl --list --verbose</code></pre><p>设置默认 WSL 版本</p><pre><code class="PowerShell">wsl --set-default-version &lt;Version&gt;</code></pre><p>例子</p><pre><code class="PowerShell">&gt; wsl -l --all -v&gt; wsl --export Ubuntu-20.04 E:\wsl\ubuntu-20.04.tar&gt; wsl --unregister Ubuntu-20.04&gt; wsl --import Ubuntu-20.04 E:\wsl E:\wsl\ubuntu-20.04.tar&gt; wsl --set-default Ubuntu-20.04&gt; wsl -d Ubuntu-20.04</code></pre><h2 id="1-git-config"><a href="#1-git-config" class="headerlink" title="1.git config"></a>1.git config</h2><pre><code class="git">git config --list # 查看配置信息git config --global user.name &quot;your name&quot; # 设置全局用户名git config --global user.email &quot;your email&quot; # 设置全局邮箱</code></pre><h2 id="一个简单的流程"><a href="#一个简单的流程" class="headerlink" title="一个简单的流程"></a>一个简单的流程</h2><pre><code class="git">git init # 初始化仓库git init &lt;your repository name&gt; # 初始化仓库git clone https://github.com/xxx/repo.git # 克隆仓库git add temp # 添加文件到暂存区git add * # 添加所有文件到暂存区git commit -m &quot;scripts&quot; # 提交到本地仓库git push git rm -r temp # 删除文件git commit -m &quot;Delete file&quot;git pushgit reflog # 查看提交历史git checkout &lt;commit&gt; # 回退到某个提交git checkout HEAD</code></pre><h2 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a>分支管理</h2><pre><code class="git">$ git branch # 列出所有分支$ git branch &lt;branch name&gt; # 创建分支$ git branch -d &lt;branch name&gt; # 删除分支$ git checkout &lt;branch name&gt; # 切换到指定分支$ git checkout -b &lt;branch name&gt; # 创建并切换到指定分支</code></pre><h2 id="初始化配置虚拟机"><a href="#初始化配置虚拟机" class="headerlink" title="初始化配置虚拟机"></a>初始化配置虚拟机</h2><pre><code class="bash">&gt; sudo su&gt; apt install build-essential&gt; gcc –version&gt; apt install vim&gt; vim /etc/netplan/01-network-manager-all.yaml&gt; netplan apply----------------------------------&gt; apt-get install openssh-server&gt;  service ssh start&gt; ps -e|grep ssh&gt; systemctl restart ssh-----------------------------------&gt; lsblk&gt; cd /media/sheldon/USB-SDD&gt; cp srilm-1.7.3.tar.gz ~/&gt; cd ~&gt; mkdir srilm&gt; cd srilm&gt; mv ../srilm-1.7.3.tar.gz&gt; tar -xzf srilm-1.7.3.tar.gz&gt; cat INSTALL ----------------------------------&gt; mytrainset and testset&gt; /root/corpus----------------------------------&gt; vim Makefile&gt; export SRILM=/root/srilm&gt; make&gt; make SRILM=$(pwd) World -j $(proc)----------------------------------</code></pre>]]></content>
      
      
      <categories>
          
          <category> tools </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>RV-RISC-V Introduction</title>
      <link href="/2024/06/25/RV-Intro/"/>
      <url>/2024/06/25/RV-Intro/</url>
      
        <content type="html"><![CDATA[<h2 id="RISCV-ISA-命名规范"><a href="#RISCV-ISA-命名规范" class="headerlink" title="RISCV ISA 命名规范"></a>RISCV ISA 命名规范</h2><ul><li>RISC-V ISA采用模块化设计</li><li><strong>ISA &#x3D; 1个基本整数指令集+多个可选的扩展指令集</strong></li></ul><table><thead><tr><th>基本指令集</th><th>描述</th></tr></thead><tbody><tr><td>RV32I</td><td>32位整数指令集</td></tr><tr><td>RV32E</td><td>RV32I的子集，用于小型嵌入式场景，寄存器数少</td></tr><tr><td>RV64I</td><td>64位整数指令集，兼容RV32I</td></tr><tr><td>RV128I</td><td>128位整数指令集，兼容RV64I和RV32I</td></tr></tbody></table><table><thead><tr><th>扩展指令集</th><th>描述</th></tr></thead><tbody><tr><td>M</td><td>整数乘除法(<strong>M</strong>ultiplication)指令集</td></tr><tr><td>A</td><td>存储器原子(<strong>A</strong>tomic)指令集</td></tr><tr><td>F</td><td>单精度(32bit)浮点（<strong>F</strong>loat）指令集</td></tr><tr><td>D</td><td>双精度(64bit)浮点（<strong>D</strong>ouble）指令集</td></tr><tr><td>C</td><td>压缩(<strong>C</strong>ompressed)指令集</td></tr><tr><td>……</td><td>其他标准化和非标准化的指令集</td></tr></tbody></table><ul><li>基本整数(Integer)指令集<ul><li>唯一强制要求实现的基础指令集，其他指令集都是可选的扩展模块</li></ul></li><li>扩展模块指令集<ul><li>RISC-V允许在实现中以可选的形式实现其他标准化和非标准化的指令集扩展</li><li>特定组合”IMAFD”被称为”通用(General)”组合，用英文字母G表示</li></ul></li><li>ISA命名格式：RV-xx-abc<ul><li>RV: RISC-V的缩写</li><li>xx: 处理器的字宽，即寄存器的宽度(bit)</li><li>abc…xyz: 该处理器支持的指令集模块集合</li></ul></li><li>例如<ul><li>RV32I:最基本的RISCV实现</li><li>RV32IMA：32位实现，支持Integer+Multiply+Atomic+Compressed</li><li>RV64GC：64位实现，支持IMAFDC</li></ul></li></ul><h2 id="RISC-V-特权指令结构"><a href="#RISC-V-特权指令结构" class="headerlink" title="RISC-V 特权指令结构"></a>RISC-V 特权指令结构</h2><h3 id="HART-“硬件线程”"><a href="#HART-“硬件线程”" class="headerlink" title="HART “硬件线程”"></a>HART “硬件线程”</h3><p>HART &#x3D; HARdware Thread ，是指令执行流的最小执行单位</p><blockquote><p>From the perspective of software running in a given execution enviroment, a <strong>hart</strong> is a <strong>resource</strong> that autonomously fetched and executes RISC-V instruction within that execution enviroment. —— 《The RISC-V Instruction Set Manual》</p></blockquote><h3 id="Privileged-Level-特权级别"><a href="#Privileged-Level-特权级别" class="headerlink" title="Privileged Level 特权级别"></a>Privileged Level 特权级别</h3><p>与X86 ISA里的实模式和保护模式类似</p><p>RISCV里对应着Machine模式，Supervisor模式，User模式</p><p>MMU(内存管理单元)</p><h3 id="CSR"><a href="#CSR" class="headerlink" title="CSR"></a>CSR</h3><p>CSR - Control and Status Register</p><ul><li>不同的特权级别下时分别对应<strong>各自一套Register(CSR)</strong>, 用于控制和获取相应Level下的处理器工作状态。</li></ul><h3 id="内存管理与保护"><a href="#内存管理与保护" class="headerlink" title="内存管理与保护"></a>内存管理与保护</h3><ul><li><strong>物理内存</strong>保护 (Physical Memory Protection, PMP)<ul><li>允许M模式指定U模式可以访问的内存地址</li><li>支持 R&#x2F;W&#x2F;X，以及Lock</li></ul></li><li><strong>虚拟内存</strong><ul><li>需要支持Supervisor Level</li><li>用于实现高级的操作系统特征(Unix&#x2F;Linux)</li><li>多种映射方式Sv32&#x2F;Sv39&#x2F;Sv48</li></ul></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://riscv.org/wp-content/uploads/2019/12/riscv-spec-20191213.pdf">The RISC-V Instruction Set Manual https://riscv.org/wp-content/uploads/2019/12/riscv-spec-20191213.pdf</a></li><li><a href="http://staff.ustc.edu.cn/~llxx/cod/reference_books/RISC-V-Reader-Chinese-v2p12017.pdf">RISC-V 开源指令集手册 汉化版 中国科学院 http://staff.ustc.edu.cn/~llxx&#x2F;cod&#x2F;reference_books&#x2F;RISC-V-Reader-Chinese-v2p12017.pdf</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> RISCV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ML-Neural Network Pipeline</title>
      <link href="/2024/06/24/ML-Neural-Network-Pipeline/"/>
      <url>/2024/06/24/ML-Neural-Network-Pipeline/</url>
      
        <content type="html"><![CDATA[<pre><code class="python"># load necessary librariesimport torchimport torchvisionfrom torch import nnfrom torch.utils.data import DataLoaderfrom torchvision import datasetsfrom torchvision.transforms import ToTensor, Lambda, Composeimport matplotlib.pyplot as pltfrom time import time# print(torch.__version__)# print(torchvision.__version__)</code></pre><p>The Dataset used in this tutorial</p><p><code>Fashion-MNIST</code> is a dataset of Zalando’s article images consisting of of 60,000 training examples and 10,000 test examples. Each example comprises a 28×28 grayscale image and an associated label from one of 10 classes.</p><pre><code class="python"># download FashionMNIST datasettraining_data = datasets.FashionMNIST(    root=&quot;data&quot;,    train=True,    download=True,    transform=ToTensor() # ToTensor() transforms the data to tensor type and rescale [0,255] uint8 to [0,1] float)# Download test data from open datasets.test_data = datasets.FashionMNIST(    root=&quot;data&quot;,    train=False,    download=True,    transform=ToTensor())# print(dir(training_data)) # print all attribute of an object# print(training_data.data[0,:]) # print info of the first picture# plt.imshow(training_data.data[0,:], cmap=plt.get_cmap(&#39;gray&#39;)) # visualize the first picture</code></pre><h2 id="Step-1-Prepare-Data"><a href="#Step-1-Prepare-Data" class="headerlink" title="Step 1: Prepare Data"></a>Step 1: Prepare Data</h2><pre><code class="python">batch_size = 128# print(training_data.data[0,:])# Create data loaders.train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)test_dataloader = DataLoader(test_data, batch_size=batch_size)# check the first batch of the datasetfor X, y in test_dataloader:    print(&quot;Shape of X [N, C, H, W]: &quot;, X.shape, X.dtype) #     print(&quot;Shape of y: &quot;, y.shape, y.dtype)    break    # N: number of data instance in a batch# C: channel, number of colors in a pixel here# [H, W]: Height and width of a picture</code></pre><pre><code class="txt">Shape of X [N, C, H, W]:  torch.Size([128, 1, 28, 28]) torch.float32Shape of y:  torch.Size([128]) torch.int64</code></pre><pre><code class="python"># visualize sample imagesnsamples=10classes_names = [&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;,&#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;]imgs, labels = next(iter(train_dataloader))fig=plt.figure(figsize=(20,5),facecolor=&#39;w&#39;)for i in range(nsamples):    ax = plt.subplot(1,nsamples, i+1)#     print(imgs[i, 0, :, :])    plt.imshow(imgs[i, 0, :, :], cmap=plt.get_cmap(&#39;gray&#39;))    ax.set_title(&quot;&#123;&#125;&quot;.format(classes_names[labels[i]]), fontsize=15)    ax.get_xaxis().set_visible(False)    ax.get_yaxis().set_visible(False)plt.show()</code></pre><h2 id="Step-2-Define-Neural-Nets"><a href="#Step-2-Define-Neural-Nets" class="headerlink" title="Step 2: Define Neural Nets"></a>Step 2: Define Neural Nets</h2><pre><code class="python">class MyModel(nn.Module):    def __init__(self):        super(MyModel, self).__init__()        self.linear_relu_stack = nn.Sequential(            # This is how your layers are defined and stacked sequentially            nn.Flatten(), # given an image of size 28x28, it will be flattened to a vector of size 784 # reshape the input to 1D (a very long vector)            nn.Linear(784, 128), # 784×128 parameters            nn.ReLU(), # if negative, it becomes 0            nn.Dropout(0.2),            nn.Linear(128, 10), # 128×10 output 128 layers and 10 labels            nn.Sigmoid()         )    def forward(self, x):        y = self.linear_relu_stack(x)        return ymodel = MyModel()print(model)</code></pre><pre><code class="txt">MyModel(  (linear_relu_stack): Sequential(    (0): Flatten(start_dim=1, end_dim=-1)    (1): Linear(in_features=784, out_features=128, bias=True)    (2): ReLU()    (3): Dropout(p=0.2, inplace=False)    (4): Linear(in_features=128, out_features=10, bias=True)    (5): Sigmoid()  ))</code></pre><h2 id="Step-3-Define-loss-function-and-the-optimizer"><a href="#Step-3-Define-loss-function-and-the-optimizer" class="headerlink" title="Step 3: Define loss function and the optimizer"></a>Step 3: Define loss function and the optimizer</h2><pre><code class="python"># define a loss function you want to optimize loss_fn = nn.CrossEntropyLoss()# define an optimizeroptimizer = torch.optim.Adam(model.parameters()) # Adam: A Method for Stochastic Optimization</code></pre><h2 id="Step-4-Train-the-neural-nets"><a href="#Step-4-Train-the-neural-nets" class="headerlink" title="Step 4: Train the neural nets"></a>Step 4: Train the neural nets</h2><pre><code class="python"># epochs: how many times we would like to traverse the whole datasetepochs=5for i in range(epochs): # iterate over epochs    tic = time()    model.train()    train_loss=0    for j, (X, y) in enumerate(train_dataloader): # iterate over batches         # Compute prediction error        pred = model(X)        loss = loss_fn(pred, y)        train_loss += loss.item()        # Backpropagation        optimizer.zero_grad()        loss.backward()        optimizer.step()        # print learning process every 100 batches        if j % 100 == 0:            loss, current = loss.item(), j * len(X)            print(f&quot;epoch &#123;i&#125; batch &#123;j&#125; loss: &#123;loss/batch_size:&gt;7f&#125;&quot;)        train_time = time() - tic        # print test results after every epochs        with torch.no_grad():        model.eval()        test_loss=0        hit=0        for (X, y) in test_dataloader:            pred = model(X)            test_loss += loss_fn(pred, y).item()            hit += (pred.argmax(1) == y).sum().item()        print(f&quot;epoch &#123;i&#125; training time: &#123;train_time:&gt;3f&#125;s, train loss: &#123;train_loss/len(train_dataloader.dataset):&gt;7f&#125; test loss: &#123;test_loss/len(test_dataloader.dataset):&gt;7f&#125; accuracy: &#123;hit/len(test_dataloader.dataset) :&gt;7f&#125;&quot;)    </code></pre><pre><code class="txt">epoch 0 batch 0 loss: 0.017987epoch 0 batch 100 loss: 0.013263epoch 0 batch 200 loss: 0.013020epoch 0 batch 300 loss: 0.013196epoch 0 batch 400 loss: 0.012888epoch 0 training time: 3.524010s, train loss: 0.013305 test loss: 0.012916 accuracy: 0.587600epoch 1 batch 0 loss: 0.012708epoch 1 batch 100 loss: 0.012738epoch 1 batch 200 loss: 0.012615epoch 1 batch 300 loss: 0.012646epoch 1 batch 400 loss: 0.012460epoch 1 training time: 3.451036s, train loss: 0.012682 test loss: 0.012763 accuracy: 0.617100epoch 2 batch 0 loss: 0.012583epoch 2 batch 100 loss: 0.012776epoch 2 batch 200 loss: 0.012628epoch 2 batch 300 loss: 0.012702epoch 2 batch 400 loss: 0.012469epoch 2 training time: 3.448095s, train loss: 0.012553 test loss: 0.012686 accuracy: 0.642400epoch 3 batch 0 loss: 0.012615epoch 3 batch 100 loss: 0.012434epoch 3 batch 200 loss: 0.012553epoch 3 batch 300 loss: 0.012463epoch 3 batch 400 loss: 0.012621epoch 3 training time: 3.463367s, train loss: 0.012486 test loss: 0.012625 accuracy: 0.650900epoch 4 batch 0 loss: 0.012159...epoch 4 batch 200 loss: 0.012609epoch 4 batch 300 loss: 0.012223epoch 4 batch 400 loss: 0.012312epoch 4 training time: 3.480916s, train loss: 0.012431 test loss: 0.012592 accuracy: 0.666100</code></pre><h2 id="Try-Different-Network-Structure"><a href="#Try-Different-Network-Structure" class="headerlink" title="Try Different Network Structure"></a>Try Different Network Structure</h2><ul><li>linear model with NO hidden layer, No dropout, No non-linearity activation</li></ul><pre><code class="python">class ShallowModel(nn.Module):    def __init__(self):        super(ShallowModel, self).__init__()        self.linear_stack = nn.Sequential(            nn.Flatten(),            nn.Linear(28*28, 10),            nn.Sigmoid()        )    def forward(self, x):        y = self.linear_stack(x)        return y    model = ShallowModel()print(model)# define a loss function you want to optimize loss_fn = nn.CrossEntropyLoss()# define an optimizeroptimizer = torch.optim.Adam(model.parameters()) </code></pre><ul><li>two hidden layers with 128 neurons and 32 neurons<ul><li>Activation Relu</li><li>Dropout 0.2</li></ul></li></ul><pre><code class="python">class DeepModel(nn.Module):    def __init__(self):        super(DeepModel, self).__init__()        self.linear_relu_stack = nn.Sequential(            nn.Flatten(),            nn.Linear(28*28, 128),            nn.ReLU(),            nn.Dropout(0.2),            nn.Linear(128, 32),            nn.ReLU(),            nn.Dropout(0.2),            nn.Linear(32, 10),            nn.Sigmoid()        )    def forward(self, x):        y = self.linear_relu_stack(x)        return ymodel = DeepModel()print(model)</code></pre>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ML-Autoencoder</title>
      <link href="/2024/06/06/ML-Autoencoder/"/>
      <url>/2024/06/06/ML-Autoencoder/</url>
      
        <content type="html"><![CDATA[<p>We will build a complete autoencoder pipeline and compare the results with PCA on dimensionality reduction tasks.</p><pre><code class="python"># load necessary librariesimport torchfrom torch import nnfrom torch.utils.data import DataLoaderfrom torchvision import datasetsfrom torchvision.transforms import ToTensor, Lambda, Composeimport matplotlib.pyplot as pltfrom time import timefrom sklearn.decomposition import PCAimport numpy as npfrom sklearn.preprocessing import StandardScaler</code></pre><p><code>Fashion-MNIST</code> is a dataset of Zalando’s article images consisting of of 60,000 training examples and 10,000 test examples. Each example comprises a 28×28 grayscale image and an associated label from one of 10 classes.</p><pre><code class="python"># download FashionMNIST datasettraining_data = datasets.FashionMNIST(    root=&quot;data&quot;,    train=True,    download=True,    transform=ToTensor() # ToTensor() transforms the data to tensor type and rescale [0,255] uint8 to [0,1] float)# Download test data from open datasets.test_data = datasets.FashionMNIST(    root=&quot;data&quot;,    train=False,    download=True,    transform=ToTensor())nsamples = 10imgs = training_data.data[0:nsamples,:]imgs = np.expand_dims(imgs,1)imgs = torch.tensor(imgs)imgs = imgs/255print(imgs.shape)labels = training_data.train_labels[0:nsamples]classes_names = [&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;,&#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;]fig=plt.figure(figsize=(20,5),facecolor=&#39;w&#39;)for i in range(nsamples):    ax = plt.subplot(1,nsamples, i+1)    plt.imshow(imgs[i, 0, :, :], cmap=plt.get_cmap(&#39;gray&#39;))    ax.set_title(&quot;&#123;&#125;&quot;.format(classes_names[labels[i]]), fontsize=15)    ax.get_xaxis().set_visible(False)    ax.get_yaxis().set_visible(False)plt.show()# print(dir(training_data)) # print all attribute of an object# print(training_data.data[0:9,:]) # print info of the first picture# plt.imshow(training_data.data[0,:], cmap=plt.get_cmap(&#39;gray&#39;)) # visualize the first picture</code></pre><pre><code class="txt">torch.Size([10, 1, 28, 28])</code></pre><img src="/2024/06/06/ML-Autoencoder/Fashion-MNIST.png" class><h2 id="Step-1-Prepare-Data"><a href="#Step-1-Prepare-Data" class="headerlink" title="Step 1: Prepare Data"></a>Step 1: Prepare Data</h2><pre><code class="python">batch_size = 128# Create data loaders.train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)test_dataloader = DataLoader(test_data, batch_size=batch_size)# check the first batch of the datasetfor X, y in test_dataloader:    print(&quot;Shape of X [N, C, H, W]: &quot;, X.shape) #     print(&quot;Shape of y: &quot;, y.shape, y.dtype)    break    # N: number of data instance in a batch# C: channel, number of colors in a pixel here# [H, W]: Height and width of a picture</code></pre><pre><code class="txt">Shape of X [N, C, H, W]:  torch.Size([128, 1, 28, 28])Shape of y:  torch.Size([128]) torch.int64</code></pre><h2 id="Step-2-Define-Neural-Nets"><a href="#Step-2-Define-Neural-Nets" class="headerlink" title="Step 2: Define Neural Nets"></a>Step 2: Define Neural Nets</h2><pre><code class="python">class AutoEncoder(nn.Module):    def __init__(self):        super(AutoEncoder, self).__init__()        self.encoder_stack = nn.Sequential(            nn.Flatten(),            nn.Linear(28*28, 128),            nn.ReLU(),            nn.Linear(128, 16),            nn.ReLU(),            nn.Linear(16, 2),            nn.ReLU()        )        self.decoder_stack = nn.Sequential(            nn.Linear(2, 16),            nn.ReLU(),            nn.Linear(16, 128),            nn.ReLU(),            nn.Linear(128, 28*28),            nn.Sigmoid()        )    def forward(self, x):        z = self.encoder_stack(x)        x = self.decoder_stack(z)        return xmodel = AutoEncoder()print(model)</code></pre><pre><code class="txt">AutoEncoder(  (encoder_stack): Sequential(    (0): Flatten(start_dim=1, end_dim=-1)    (1): Linear(in_features=784, out_features=128, bias=True)    (2): ReLU()    (3): Linear(in_features=128, out_features=16, bias=True)    (4): ReLU()    (5): Linear(in_features=16, out_features=2, bias=True)    (6): ReLU()  )  (decoder_stack): Sequential(    (0): Linear(in_features=2, out_features=16, bias=True)    (1): ReLU()    (2): Linear(in_features=16, out_features=128, bias=True)    (3): ReLU()    (4): Linear(in_features=128, out_features=784, bias=True)    (5): Sigmoid()  ))</code></pre><h2 id="Step-3-Define-loss-function-and-the-optimizer"><a href="#Step-3-Define-loss-function-and-the-optimizer" class="headerlink" title="Step 3: Define loss function and the optimizer"></a>Step 3: Define loss function and the optimizer</h2><pre><code class="python">loss_fn = nn.MSELoss()optimizer = torch.optim.Adam(model.parameters())</code></pre><h2 id="Step-4-Train-the-neural-nets"><a href="#Step-4-Train-the-neural-nets" class="headerlink" title="Step 4: Train the neural nets"></a>Step 4: Train the neural nets</h2><pre><code class="python"># epochs: how many times we would like to traverse the whole datasetepochs=50for i in range(epochs): # over epochs    tic = time()    model.train()    train_loss = 0    for j, (X, y) in enumerate(train_dataloader): # over batches         # Compute prediction error        pred = model(X)        loss = loss_fn(pred, X.reshape(X.size(0),-1))        train_loss += loss        # Backpropagation        optimizer.zero_grad()        loss.backward()        optimizer.step()        # print learning process#         if j % 100 == 0:#             loss, current = loss.item(), j * len(X)#             print(f&quot;epoch &#123;i&#125; batch &#123;j&#125; loss: &#123;loss/batch_size:&gt;7f&#125;&quot;)    train_time = time() - tic        # print test results after every epochs        print(f&quot;epoch &#123;i&#125; training time: &#123;train_time:&gt;7f&#125;s, train loss: &#123;train_loss/len(train_dataloader.dataset):&gt;7f&#125;&quot;)    </code></pre><pre><code class="txt">epoch 0 training time: 5.038011s, train loss: 0.000559epoch 1 training time: 5.130287s, train loss: 0.000480epoch 2 training time: 5.422238s, train loss: 0.000448epoch 3 training time: 5.880179s, train loss: 0.000387epoch 4 training time: 5.521627s, train loss: 0.000376epoch 5 training time: 5.126778s, train loss: 0.000367epoch 6 training time: 5.206013s, train loss: 0.000362epoch 7 training time: 5.712925s, train loss: 0.000358epoch 8 training time: 5.457368s, train loss: 0.000355epoch 9 training time: 5.184317s, train loss: 0.000349epoch 10 training time: 5.153089s, train loss: 0.000344epoch 11 training time: 5.222293s, train loss: 0.000343epoch 12 training time: 5.173838s, train loss: 0.000340epoch 13 training time: 5.678778s, train loss: 0.000338epoch 14 training time: 5.614803s, train loss: 0.000336epoch 15 training time: 5.425945s, train loss: 0.000334epoch 16 training time: 5.560995s, train loss: 0.000332epoch 17 training time: 5.370713s, train loss: 0.000332epoch 18 training time: 5.385829s, train loss: 0.000330epoch 19 training time: 5.537868s, train loss: 0.000330epoch 20 training time: 5.746025s, train loss: 0.000328epoch 21 training time: 5.276327s, train loss: 0.000329epoch 22 training time: 5.439416s, train loss: 0.000326epoch 23 training time: 5.563997s, train loss: 0.000324epoch 24 training time: 5.372393s, train loss: 0.000323...epoch 46 training time: 5.110753s, train loss: 0.000297epoch 47 training time: 5.206847s, train loss: 0.000271epoch 48 training time: 5.143513s, train loss: 0.000262epoch 49 training time: 5.325914s, train loss: 0.000255</code></pre><h2 id="Step-5-Dimensionality-reduction-using-PCA"><a href="#Step-5-Dimensionality-reduction-using-PCA" class="headerlink" title="Step 5: Dimensionality reduction using PCA"></a>Step 5: Dimensionality reduction using PCA</h2><pre><code class="python"># retrieve image data as array for PCA, each image [28,28] become a vector [1, 784]X = training_data.data.reshape(60000,-1).numpy()X = X/X.max()# fit PCA pca = PCA(n_components=2)pca.fit(X)print(np.cumsum(pca.explained_variance_ratio_))# transform sampled imgs using PCA and recover imagesimgs_pca = imgs.reshape(nsamples,-1).numpy() # reshape sample images to vectorsimgs_reduced = pca.transform(imgs_pca)imgs_recovered = pca.inverse_transform(imgs_reduced)imgs_recovered = np.reshape(imgs_recovered, [nsamples,28,28]) # reshape vectors to sample images</code></pre><pre><code class="txt">[0.29039228 0.46794538]</code></pre><h2 id="Step-6-Visual-comparison-between-autoencoder-and-PCA"><a href="#Step-6-Visual-comparison-between-autoencoder-and-PCA" class="headerlink" title="Step 6: Visual comparison between autoencoder and PCA"></a>Step 6: Visual comparison between autoencoder and PCA</h2><pre><code class="python">fig=plt.figure(figsize=(20,5),facecolor=&#39;w&#39;)for i in range(nsamples):    ax = plt.subplot(1,nsamples, i+1)    plt.imshow(imgs[i, 0, :, :], cmap=plt.get_cmap(&#39;gray&#39;))    ax.set_title(&quot;&#123;&#125;&quot;.format(classes_names[labels[i]]), fontsize=15)    ax.get_xaxis().set_visible(False)    ax.get_yaxis().set_visible(False)plt.show()fig=plt.figure(figsize=(20,5),facecolor=&#39;w&#39;)pred = model(imgs)pred = pred.reshape(pred.size(0),1,28,28).detach().numpy()for i in range(nsamples):    ax = plt.subplot(1,nsamples, i+1)#     print(pred[i, 0, :, :])    plt.imshow(pred[i, 0, :, :], cmap=plt.get_cmap(&#39;gray&#39;))    ax.set_title(&quot;&#123;&#125;&quot;.format(classes_names[labels[i]]), fontsize=15)    ax.get_xaxis().set_visible(False)    ax.get_yaxis().set_visible(False)plt.show()fig=plt.figure(figsize=(20,5),facecolor=&#39;w&#39;)for i in range(nsamples):    ax = plt.subplot(1,nsamples, i+1)    plt.imshow(imgs_recovered[i, :, :], cmap=plt.get_cmap(&#39;gray&#39;))    ax.set_title(&quot;&#123;&#125;&quot;.format(classes_names[labels[i]]), fontsize=15)    ax.get_xaxis().set_visible(False)    ax.get_yaxis().set_visible(False)plt.show()</code></pre><img src="/2024/06/06/ML-Autoencoder/original.png" class><img src="/2024/06/06/ML-Autoencoder/autoencoder.png" class><img src="/2024/06/06/ML-Autoencoder/PCA.png" class>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PR-SVD</title>
      <link href="/2024/05/04/PR-SVD/"/>
      <url>/2024/05/04/PR-SVD/</url>
      
        <content type="html"><![CDATA[<p>Problem: Decompose the matrix $\mathrm{A}&#x3D;\begin{pmatrix}5&amp;3 \\ 0&amp;-4\end{pmatrix}$ using Singular Value Decomposition(SVD). Please show detail calculation steps.</p><p><strong>Step1:</strong> calculate $AA^T$ and $A^TA$</p><p>$$A &#x3D; \begin{pmatrix}5&amp;3 \\ 0&amp;-4\end{pmatrix}, \text{then } A^T &#x3D; \begin{pmatrix}5&amp;0 \\ 3&amp;-4\end{pmatrix}$$</p><p>$$AA^T &#x3D; \begin{pmatrix}5&amp;3 \\ 0&amp;-4\end{pmatrix}\begin{pmatrix}5&amp;0 \\ 3&amp;-4\end{pmatrix} &#x3D; \begin{pmatrix}34&amp;-12 \\ -12&amp;-16\end{pmatrix}$$</p><p>$$A^TA &#x3D; \begin{pmatrix}5&amp;0 \\ 3&amp;-4\end{pmatrix}\begin{pmatrix}5&amp;3 \\ 0&amp;-4\end{pmatrix} &#x3D; \begin{pmatrix}25&amp;15 \\ 15&amp;25\end{pmatrix}$$</p><p><strong>Step2:</strong> calculate $\lambda_1, \lambda_2$ and $S$</p><p>$$\begin{aligned}<br>|AA^{T}-\lambda E|&#x3D;0<br>    &amp;\Rightarrow<br>    \left|\left(\begin{matrix}{34}&amp;{-12} \\ {-12}&amp;{16} \end{matrix}\right)-\lambda\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right|&#x3D;0<br>\end{aligned}$$</p><p>$$\Rightarrow<br>\left|\begin{matrix}{34-\lambda}&amp;{-12} \\ {-12}&amp;{16-\lambda} \end{matrix}\right|&#x3D;0\Rightarrow(34-\lambda)(16-\lambda)-12\times12&#x3D;0<br>$$</p><p>$$\Rightarrow<br>\lambda^{2}-50\lambda+400&#x3D;0\Rightarrow(\lambda-40)(\lambda-10)&#x3D;0<br>$$</p><p>Eigenvalues: $\lambda_1&#x3D;40, \lambda_2&#x3D;10$</p><p>Singular values: $\sigma_1&#x3D;\sqrt{\lambda_1}&#x3D;\sqrt{40}&#x3D;2\sqrt{10},\sigma_2&#x3D;\sqrt{\lambda_2}&#x3D;\sqrt{10}$</p><p>Diagonal matrix S:  $S&#x3D;\begin{pmatrix}\sigma_1&amp;0 \\ 0&amp;\sigma_1\end{pmatrix}&#x3D;\begin{pmatrix}2\sqrt{10}&amp;0 \\ 0&amp;\sqrt{10}\end{pmatrix}$</p><p><strong>Step3:</strong> Finding $U$ $(AA^{T}-\lambda E)x&#x3D;0\Rightarrow U&#x3D;(u_{1},u_{2})&#x3D;\begin{pmatrix}-\frac{2}{\sqrt{5}}&amp;\frac{1}{\sqrt{5}} \\ \frac{1}{\sqrt{5}}&amp;\frac{2}{\sqrt{5}}\end{pmatrix}$</p><p>$$\begin{aligned}<br>For~\lambda_{1}&amp;&#x3D;40,<br>(AA^{T}-\lambda_{1}E)x_{1}&#x3D;0<br>\Rightarrow<br>\left(\left(\begin{matrix}{34}&amp;{-12} \\ {-12}&amp;{16} \end{matrix}\right)-40\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right)x_1&#x3D;0<br>&amp;\end{aligned}<br>$$</p><p>$$\Rightarrow<br>\left(\begin{matrix}{-6}&amp;{-12} \\ {-12}&amp;{-24} \end{matrix}\right)x_1&#x3D;0<br>\Rightarrow<br>x_{1}&#x3D;\binom{-2a}{a}\Longrightarrow u_{1}&#x3D;\frac{x_{1}}{||x_{1}||}&#x3D;\begin{pmatrix}-\frac{2}{\sqrt{5}} \\ \frac{1}{\sqrt{5}}\end{pmatrix}<br>$$</p><p>$$\begin{aligned}<br>For~\lambda_{2}&amp;&#x3D;10,<br>(AA^{T}-\lambda_{2}E)x_{2}&#x3D;0<br>\Rightarrow<br>\left(\left(\begin{matrix}{34}&amp;{-12} \\ {-12}&amp;{16} \end{matrix}\right)-10\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right)x_2&#x3D;0<br>\end{aligned}<br>$$</p><p>$$<br>\Rightarrow<br>\left(\begin{matrix}{24}&amp;{-12} \\ {-12}&amp;{6} \end{matrix}\right)x_2&#x3D;0<br>\Rightarrow<br>x_{2}&#x3D;\binom{a}{2a}\Longrightarrow u_{2}&#x3D;\frac{x_{2}}{||x_{2}||}&#x3D;\begin{pmatrix}\frac{1}{\sqrt{5}} \\ \frac{2}{\sqrt{5}}\end{pmatrix}<br>$$</p><p><strong>Step4:</strong> Finding $V$ $(AA^{T}-\lambda E)x&#x3D;0\Rightarrow V&#x3D;(v_{1},v_{2})&#x3D;\begin{pmatrix}\frac{1}{\sqrt{2}}&amp;\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}&amp;-\frac{1}{\sqrt{2}}\end{pmatrix}$</p><p>$$\begin{aligned}<br>For~\lambda_{1}&amp;&#x3D;40,<br>(AA^{T}-\lambda_{1}E)x_{3}&#x3D;0<br>\Rightarrow<br>\left(\left(\begin{matrix}{25}&amp;{15} \\ {15}&amp;{25} \end{matrix}\right)-40\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right)x_3&#x3D;0<br>\end{aligned}<br>$$</p><p>$$<br>\Rightarrow<br>\left(\begin{matrix}{-15}&amp;{15} \\ {15}&amp;{-15} \end{matrix}\right)x_3 &#x3D;0<br>\Rightarrow<br>x_{3}&#x3D;\binom{c}{c}\Longrightarrow v_{1}&#x3D;\frac{x_{3}}{||x_{3}||}&#x3D;\begin{pmatrix}\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}\end{pmatrix}<br>$$</p><p>$$\begin{aligned}<br>For~\lambda_{2}&amp;&#x3D;10,<br>(AA^{T}-\lambda_{2}E)x_{4}&#x3D;0<br>\Rightarrow<br>\left(\left(\begin{matrix}{25}&amp;{15} \\ {15}&amp;{25}  \end{matrix}\right)-10\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right)x_4&#x3D;0<br>\end{aligned}<br>$$</p><p>$$<br>\Rightarrow<br>\left(\begin{matrix}{15}&amp;{15} \\ {15}&amp;{15} \end{matrix}\right)x_4&#x3D;0<br>\Rightarrow<br>x_{4}&#x3D;\binom{d}{-d}\Longrightarrow v_{2}&#x3D;\frac{x_{4}}{||x_{4}||}&#x3D;\begin{pmatrix}\frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}}\end{pmatrix}<br>$$</p><p><strong>Step5:</strong> Complete SVD</p><p>$$<br>\left(\begin{matrix}{5}&amp;{3} \\ {0}&amp;{-4} \end{matrix}\right) &#x3D;<br>\left(\begin{matrix}{-\frac{2}{\sqrt{5}}}&amp;{\frac{1}{\sqrt{5}}} \\ {\frac{1}{\sqrt{5}}}&amp;{\frac{2}{\sqrt{5}}} \end{matrix}\right)<br>\left(\begin{matrix}{2\sqrt{10}}&amp;{0} \\ {0}&amp;{\sqrt{10}} \end{matrix}\right)<br>\left(\begin{matrix}{\frac{1}{\sqrt{2}}}&amp;{\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}}&amp;{-\frac{1}{\sqrt{2}}} \end{matrix}\right)<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> Math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pattern recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PR-ch04-NLC</title>
      <link href="/2024/05/04/PR-ch04-NLC/"/>
      <url>/2024/05/04/PR-ch04-NLC/</url>
      
        <content type="html"><![CDATA[<img src="/2024/05/04/PR-ch04-NLC/problem.png" class><hr><p>Solution:</p><img src="/2024/05/04/PR-ch04-NLC/1.png" class><img src="/2024/05/04/PR-ch04-NLC/2.png" class><img src="/2024/05/04/PR-ch04-NLC/3.png" class><img src="/2024/05/04/PR-ch04-NLC/4.png" class><img src="/2024/05/04/PR-ch04-NLC/5.png" class><img src="/2024/05/04/PR-ch04-NLC/6.png" class><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><img src="/2024/05/04/PR-ch04-NLC/cover.png" class><ul><li>Sergios Theodoridis Konstantinos Koutroumbas Pattern Recognition. 4th Edition. Springer, 2010.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pattern recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PR-ch03-LC</title>
      <link href="/2024/05/04/PR-ch03-LC/"/>
      <url>/2024/05/04/PR-ch03-LC/</url>
      
        <content type="html"><![CDATA[<p>Decision hyperplane:</p><p>$$g(x)&#x3D;w^{T}x+w_{0}&#x3D;0\Rightarrow w_{1}x_{1}+w_{2}x_{2}+\cdots+w_{l}x_{l}+w_{0}&#x3D;0$$</p><p>where $w&#x3D;(w_{1},w_{2},…,w_{l})^{T}$ is weight vector, $w_{0}$ is threshold</p><p><strong>Our goal:</strong> compute a solution, a hyperplane $w$, so that</p><p>$$w^Tx+w_0&gt;0(or&lt;0),\quad x\in\omega_1(or \omega_2)$$</p><p>Perceptron algorithm：</p><ul><li>Define a cost function $J(w)$</li><li>Chooses an algorithm to minimize $J(w)$</li><li>The minimum corresponds to a solution</li></ul><p><strong>Perceptron cost function：</strong></p><p>$$J(w)&#x3D;\sum_{x\in Y}\delta_xw^Tx$$</p><ul><li>Y is the subset of the training vectors wrongly classified by $w$</li><li>When 𝑌 &#x3D; ∅, a solution is achieved and  $J(w) ≥ 0$</li><li>$\delta_x&#x3D;\begin{cases}-1&amp; \text{if}x\in\omega_1 \\ +1&amp; \text{if}x\in\omega_2\end{cases}$</li><li>$J(w)$ is continuous and piecewise linear</li></ul><p><strong>Minimize the cost function:</strong></p><p>$$\frac{\partial J(w)}{\partial w}&#x3D;\frac{\partial}{\partial w}\left(\sum_{x\in Y}(\delta_{x}w^{T}x)\right)&#x3D;\sum_{x\in Y}\delta_{x}x$$</p><p>$$w(t+1)&#x3D;w(t)-\rho_t\frac{\partial J(w)}{\partial w}\Bigg|_{w&#x3D;w(t)}$$</p><p>$$w(t+1)&#x3D;w(t)-\rho_t\sum_{x\in Y}\delta_xx$$</p><p>The perceptron algorithm converges in a finite number of iteration steps<br>to a solution if</p><p>$$\lim_{t\to\infty}\sum_{k&#x3D;0}^t\rho_k\to\infty\quad\mathrm{and}\quad\lim_{t\to\infty}\sum_{k&#x3D;0}^t\rho_k^2&lt;+\infty $$</p><p>The perceptron algorithm is a reward and punishment scheme</p><hr><img src="/2024/05/04/PR-ch03-LC/problem.png" class><hr><p>Solution:</p><img src="/2024/05/04/PR-ch03-LC/1.png" class><img src="/2024/05/04/PR-ch03-LC/2.png" class><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><img src="/2024/05/04/PR-ch03-LC/cover.png" class><ul><li>Sergios Theodoridis Konstantinos Koutroumbas Pattern Recognition. 4th Edition. Springer, 2010.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pattern recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PR-ch02-Bayes</title>
      <link href="/2024/04/30/PR-ch02-Bayes/"/>
      <url>/2024/04/30/PR-ch02-Bayes/</url>
      
        <content type="html"><![CDATA[<hr><p><strong>Problem 2.2:</strong> In a two-class one-dimensional problem, the pdfs are the Gaussians $\mathcal{N}(0,\sigma^2)$ and $\mathcal{N}(1,\sigma^2)$ for the two classes, respectively. Show that the threshold $x_0$ minimizing the average risk is equal to<br>$$x_0&#x3D;1&#x2F;2-\sigma^2\ln\frac{\lambda_{21}P(\omega_2)}{\lambda_{12}P(\omega_1)}$$<br>where $\lambda_{11}&#x3D;\lambda_{22}&#x3D;0$ has been assumed.</p><hr><p>Solution: In a two-class problem:</p><p>$$R(\alpha_1|x)&#x3D;\lambda_{11}P(\omega_1|x)+\lambda_{12}P(\omega_2|x)$$</p><p>$$R(\alpha_2|x)&#x3D;\lambda_{21}P(\omega_1|x)+\lambda_{22}P(\omega_2|x)$$</p><p>The threshold $x_0$ minimizing the average risk where $R(\alpha_1|x_0)&#x3D;R(\alpha_2|x_0),$ then:</p><p>$$\lambda_{11}P(\omega_1|x_0)+\lambda_{21}P(\omega_2|x_0)&#x3D;\lambda_{12}P(\omega_1|x_0)+\lambda_{22}P(\omega_2|x_0)$$</p><p>$$\Rightarrow\frac{P(\omega_1|x_0)}{P(\omega_2|x_0)}&#x3D;\frac{p(x_0|\omega_1)P(\omega_1)}{p(x_0|\omega_2)P(\omega_2)}&#x3D;\frac{\lambda_{12}-\lambda_{22}}{\lambda_{21}-\lambda_{11}}$$</p><p>To minimize the average risk, we have<br>$$\frac{P(x_0|\omega_1)}{P(x_0|\omega_2)}&#x3D;\frac{\lambda_{12}-\lambda_{22}}{\lambda_{21}-\lambda_{11}}\frac{P(\omega_2)}{P(\omega_1)}$$<br>and $\lambda_{11}&#x3D;\lambda_{22}&#x3D;0$,then</p><p>$$\frac{P(x_0|\omega_1)}{P(x_0|\omega_2)}&#x3D;\frac{\lambda_{12}}{\lambda_{21}}\frac{P(\omega_2)}{P(\omega_1)}$$</p><p>taking the logarithm of both sides,$$lnP(x_{0}|\omega_{1})-lnP(x_{0}|\omega_{2})&#x3D;ln\frac{\lambda_{12}P(\omega_{2})}{\lambda_{21}P(\omega_{1})}$$</p><p>since $P(x_{0}|\omega_{1}){\sim}N(0,\sigma^{2}),$</p><p>$$P(x_{0}|\omega_{1})&#x3D;\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x_{0}^{2}}{2\sigma^{2}}}\quad\Rightarrow{lnP(x_{0}|\omega_{1})}&#x3D;-ln(\sqrt{2\pi}\sigma)-\frac{x_{0}^{2}}{2\sigma^{2}}$$</p><p>and $P(x_{0}|\omega_{2}){\sim}N(1,\sigma^{2}),$</p><p>$$P(x_{0}|\omega_{2})&#x3D;\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_{0}-1)^{2}}{2\sigma^{2}}}\quad\Rightarrow{lnP(x_{0}|\omega_{2})}&#x3D;-ln(\sqrt{2\pi}\sigma)-\frac{(x_{0}-1)^{2}}{2\sigma^{2}}$$</p><p>then, $lnP(x_0|\omega_1)-lnP(x_0|\omega_2)&#x3D;-ln(\sqrt{2\pi}\sigma)-\frac{x_0^2}{2\sigma^2}-(-ln(\sqrt{2\pi}\sigma)-\frac{(x_0-1)^2}{2\sigma^2})$</p><p>$$lnP(x_0|\omega_1)-lnP(x_0|\omega_2)&#x3D;-\frac{(x_0-1)^2}{2\sigma^2}-\frac{x_0^2}{2\sigma^2}&#x3D;ln\frac{\lambda_{12}P(\omega_2)}{\lambda_{21}P(\omega_1)}$$</p><p>$$(x_{0}-1)^{2}-x_{0}^{2}&#x3D;2\sigma^{2}ln\frac{\lambda_{12}P(\omega_{2})}{\lambda_{21}P(\omega_{1})}$$</p><p>$$x_{0}^{2}-2x_{0}+1-x_{0}^{2}&#x3D;2\sigma^{2}ln\frac{\lambda_{12}P(\omega_{2})}{\lambda_{21}P(\omega_{1})}$$</p><p>$$-2x_{0}+1&#x3D;2\sigma^{2}ln\frac{\lambda_{12}P(\omega_{2})}{\lambda_{21}P(\omega_{1})}$$</p><p>$$\mathrm{thus},\quad x_0&#x3D;\frac{1}{2}-\sigma^2ln\frac{\lambda_{12}P(\omega_2)}{\lambda_{21}P(\omega_1)}$$</p><hr><p><strong>Problem 2.5:</strong> Consider a two (equiprobable) class, one-dimensional problem with samples distributed according to the Rayleigh pdf in each class, that is,<br>$$p(x|\omega_i)&#x3D;\begin{cases}\frac{x}{\sigma_i^2}\exp\left(\frac{-x^2}{2\sigma_i^2}\right)&amp;x\geq0 \\ 0&amp;x&lt;0\end{cases}$$<br>Compute the decision boundary point $g(x)&#x3D;0.$</p><hr><p>Solution: The decision boundary point corresponds to</p><p>$$\frac{x_0}{\sigma_1^2}\exp(\frac{-x_0^2}{2\sigma_1^2})&#x3D;\frac{x_0}{\sigma_2^2}\exp(\frac{-x_0^2}{2\sigma_2^2})$$</p><p>or by taking the logarithm</p><p>$$\frac{-x_0^2}{2\sigma_1^2}&#x3D;\ln\frac{\sigma_1^2}{\sigma_2^2}-\frac{x_0^2}{2\sigma_2^2}$$</p><p>and finally</p><p>$$x_0&#x3D;\sqrt{\frac{2\sigma_1^2\sigma_2^2}{\sigma_1^2-\sigma_2^2}\ln\frac{\sigma_1^2}{\sigma_2^2}}$$</p><hr><p><strong>Problem 2.7:</strong> In a three-class,two-dimensional problem the feature vectors in each class are normally distributed with covariance matrix<br>$$\Sigma&#x3D;\begin{bmatrix}1.2&amp;0.4 \\ 0.4&amp;1.8\end{bmatrix}$$<br>The mean vectors for each class are $[0.1,0.1]^T,[2.1,1.9]^T,[-1.5,2.0]^T.$ Assuming that the classes are equiprobable,<br>(a) classify the feature vector [1.6,1.5]$^T$ according to the Bayes minimum error probability classifier;<br>(b) draw the curves of equal Mahalanobis distance from [2.1,1.9]$^{\acute{T}}.$</p><hr><p>Solution:</p><p>(a) It suffices to compute the Mahalanobis distance of $[1.6,1.5]^T$ from mean vectors of the classes. We have:</p><p>$$\Sigma^{-1}&#x3D;\left[\begin{array}{cc}0.9&amp;0.2 \\ -0.2&amp;0.6\end{array}\right]$$</p><p>$$|\Sigma|&#x3D;1.2\times1.8-0.4\times0.4&#x3D;2,\Sigma^{-1}&#x3D;\frac{1}{|\Sigma|}\Big[\begin{matrix}1.8&amp;-0.4 \\ -0.4&amp;1.2\end{matrix}\Big]&#x3D;\Big[\begin{matrix}0.9&amp;-0.2 \\ -0.2&amp;0.6\end{matrix}\Big]$$</p><p>Thus, $d_1^2&#x3D;2.361,d_2^2&#x3D;0.241,d_3^2&#x3D;9.416$</p><p>Hence $[1.6,1.5]^T$ is assigned to $\mathcal{w_2}$</p><p>(b) According to theory it suffices to compute the eigenvalues and eigenvectors of $\Sigma$. There are </p><p>$$\lambda_{1}&#x3D;1,\lambda_{2}&#x3D;2 \\ v_{1}&#x3D;[0.89,-0.45]^{T} \\ v_{2}&#x3D;[0.45,0.89]^{T}$$</p><p>Thus the ellipse, centered at $\mu_2$ and axis</p><p>$2\sqrt{\lambda_1}c\boldsymbol{v}_1$ and $2\sqrt{\lambda_2}c\boldsymbol{v}_2$</p><hr><p><strong>Problem 2.12:</strong> Consider a two-class, two-dimensional classification task, where the feature vectors in each of the classes $\omega_1,\omega_{2}$ are distributed according to</p><p>$$p(x|\omega_1)&#x3D;\frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\biggl(-\frac{1}{2\sigma_1^2}(x-\mu_1)^T(x-\mu_1)\biggr)$$</p><p>$$p(x|\omega_2)&#x3D;\dfrac{1}{\sqrt{2\pi\sigma_2^2}}\exp\biggl(-\dfrac{1}{2\sigma_2^2}(x-\mu_2)^T(x-\mu_2)\biggr)$$</p><p>with</p><p>$$\mu_{1}&#x3D;[1,1]^{T},\mu_{2}&#x3D;[1.5,1.5]^{T},\sigma_{1}^{2}&#x3D;\sigma_{2}^{2}&#x3D;0.2$$</p><p>Assume that $P(\omega_1)&#x3D;P(\omega_2)$ and design a Bayesian classifier<br>(a) that minimizes the error probability<br>(b) that minimizes the average risk with loss matrix</p><p>$$\Lambda&#x3D;\begin{bmatrix}0&amp;1 \\ 0.5&amp;0\end{bmatrix}$$</p><p>Using a pseudorandom number generator, produce 100 feature vectors from each class, according to the preceding pdfs. Use the classifiers designed to classify the generated vectors. What is the percentage error for each case? Repeat the experiments for $\mu_{2}&#x3D;[3.0,3.0]^{T}.$</p><hr><p>Solution:</p><p>(a) For the two-class classification, if $P(\omega_1)&#x3D;P(\omega_2)$ and $\lambda_{11}&#x3D;\lambda_{22}&#x3D;0$,</p><p>The Bayesian classifier is: $x\to\omega_{1}$ if $P(x|\omega_{1})&gt;P(x|\omega_{2})\frac{\lambda_{12}}{\lambda_{21}}$</p><p>If $\lambda_{12}&#x3D;\lambda_{21}$ , the Bayesian classifier minimizes the error probability. Thus, the Bayesian<br>classifier is: $x\to\omega_{1}$ if $P(x|\omega_{1})&gt;P(x|\omega_{2})$</p><p>We have</p><p>$$p(x|\omega_1)&#x3D;\frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\biggl(-\frac{1}{2\sigma_1^2}(x-\mu_1)^T(x-\mu_1)\biggr)$$</p><p>$$p(x|\omega_2)&#x3D;\dfrac{1}{\sqrt{2\pi\sigma_2^2}}\exp\biggl(-\dfrac{1}{2\sigma_2^2}(x-\mu_2)^T(x-\mu_2)\biggr)$$</p><p>From $\sigma_1^2&#x3D;\sigma_2^2&#x3D;\sigma^2,|x-\mu_1|^2&#x3D;(x-\mu_1)^T(x-\mu_1)$ and $|x-\mu_2|^2&#x3D;(x-\mu_2)^T(x-\mu_2)$, we have</p><p>$$P(x|\omega_1)&#x3D;\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{|x-\mu_1|^2}{2\sigma^2}\right),\quad P(x|\omega_2)&#x3D;\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{|x-\mu_2|^2}{2\sigma^2}\right)$$</p><p>$P(x|\omega_1)&gt;P(x|\omega_2)$ is equivalent to $\ln P(x|\omega_1)&gt;\ln P(x|\omega_2)$</p><p>$$\ln P(x|\omega_1)&#x3D;\ln\frac{1}{\sqrt{2\pi\sigma^2}}-\frac{|x-\mu_1|^2}{2\sigma^2},\quad\ln P(x|\omega_2)&#x3D;\ln\frac{1}{\sqrt{2\pi\sigma^2}}-\frac{|x-\mu_2|^2}{2\sigma^2}$$</p><p>$P(x|\omega_1)&gt;P(x|\omega_2)$ is equivalent to $|x-\mu_1|&lt;|x-\mu_2|$</p><p>Thus, for the Bayesian classifier minimizing the error probability is equivalent to the<br>classifier minimizing the Euclidean distance, namely,</p><p>$$x\to\omega_1\text{ if }|x-\mu_1|&lt;|x-\mu_2|$$</p><p>(b) In this case, $x$ is classified to $P(x|\omega_{1})&gt;P(x|\omega_{2})\frac{\lambda_{12}}{\lambda_{21}}$</p><p>where $\lambda_{12}&#x3D;1$ and $\lambda_{21}&#x3D;0.5$. Thus following similar arguments as in theory, for Bayesian<br>classification for normal distributions, we conclude that the decision hyperplane is</p><p>$$g_{12}(x)&#x3D;w^T(x-x_0) \\ w&#x3D;\mu_{1}-\mu_{2}$$</p><p>and</p><p>$$x_0&#x3D;\frac{1}{2}(\mu_1+\mu_2)-\sigma^2\ln\frac{\lambda_{21}P(\omega_1)}{\lambda_{12}P(\omega_2)}\frac{\mu_1-\mu_2}{|\mu_1-\mu_2|^2}$$</p><p>(c) The following MATLAB function takes as input the variance (s), the mean ( m) and the number of samples N. The output is a vector 1xN, whose ele- ments are the N samples of the 1-D Gaussian. For 2-D independent variables, combine two samples generated above, in a single vector</p><p>$$x&#x3D;[x_1,x_2]^T$$</p><pre><code class="matlab">Function x=gaussian(m,s,N);x=randn(1,N);x=x*sqrt(s)+m;</code></pre><hr><p><strong>Problem 2.17：</strong> In a heads or tails coin-tossing experiment the probability of occurrence of a head (1) is $q$ and that of a tail (0) is $1-q$. Let $x_i,i &#x3D;1,2,…,N$, be the resulting experimental outcomes, 𝑥𝑖 ∈ {0,1}. Show that the ML estimate of $q$ is</p><p>$$q_{ML}&#x3D;\frac1N\sum_{i&#x3D;1}^Nx_i$$</p><p>Hint: The likelihood function is</p><p>$$P(X;q)&#x3D;\prod_{i&#x3D;1}^Nq^{x_i}(1-q)^{(1-x_i)}$$</p><p>Then show that the ML results from the solution of the equation</p><p>$$q^{\sum_{i}x_{i}}(1-q)^{(N-\sum_{i}x_{i})}\left(\frac{\sum_{i}x_{i}}{q}-\frac{N-\sum_{i}x_{i}}{1-q}\right)&#x3D;0$$</p><hr><p>Solution:</p><img src="/2024/04/30/PR-ch02-Bayes/6.png" class><img src="/2024/04/30/PR-ch02-Bayes/7.png" class><hr><p><strong>Problem 2.29：</strong> Show that for the lognormal distribution</p><p>$$p(x)&#x3D;\frac{1}{\sigma x\sqrt{2\pi}}\exp\left(-\frac{(\ln{x}-\theta)^{2}}{2\sigma^{2}}\right),x&gt;0$$</p><p>The ML estimate is given by</p><p>$$\theta_{ML}&#x3D;\frac{1}{N}\sum_{k&#x3D;1}^{N}\ln{x_k}$$</p><hr><p>Solution:</p><img src="/2024/04/30/PR-ch02-Bayes/8.png" class><hr><p><strong>Problem:</strong> Consider two normal distributions in one dimension: $N(\mu_1,\sigma_1^2)$ and $N(\mu_2,\sigma_2^2).$ Imagine that we choose two random samples $x_1$ and $x_2$,one from each of the normal distributions and calculate their sum $x_3&#x3D;x_1+x_2.$ Suppose we do this repeatedly.</p><p>(a) Consider the resulting distribution of the values of $x_3.$ Show from frst principles<br>that this is also a normal distribution.</p><p>(b) What is the mean,  $\mu_{3}$, of your new distribution?</p><p>(c) What is the variance, $\sigma_3^2?$</p><p>(d) Repeat the above with two distributions in a multi-dimensional space, i.e., $N(\mu_1,\Sigma_1)$ and $N(\mu_2,\Sigma_2).$</p><hr><p>Solution:</p><img src="/2024/04/30/PR-ch02-Bayes/1.png" class><img src="/2024/04/30/PR-ch02-Bayes/2.png" class><img src="/2024/04/30/PR-ch02-Bayes/3.png" class><img src="/2024/04/30/PR-ch02-Bayes/4.png" class><img src="/2024/04/30/PR-ch02-Bayes/5.png" class><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><img src="/2024/04/30/PR-ch02-Bayes/cover.png" class><ul><li>Sergios Theodoridis Konstantinos Koutroumbas Pattern Recognition. 4th Edition. Springer, 2010.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pattern recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ML-Clustering-Kmeans</title>
      <link href="/2024/04/24/ML-Clustering-Kmeans/"/>
      <url>/2024/04/24/ML-Clustering-Kmeans/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In this notebook, we shall be looking at how the kmeans algorithm works. KMeans is an <strong>unsupervised learning</strong> algorithm that is used to cluster data in groups - without knowing which group given data elements belong to as are going to see.</p><pre><code class="python">import numpy as npimport sklearn.datasetsimport matplotlib.pyplot as pltimport matplotlib as mpl</code></pre><p>Let’s shall generate a random dataset of 1000 points clustered into 3 groups.</p><pre><code class="python"># 设置数据集中样本点的数量N = 1000# 生成具有3个中心点的聚类数据集X_, y_ = sklearn.datasets.make_blobs(n_samples=N+5, centers=3) # 从生成的数据集中提取前N个样本作为训练数据X, y = X_[:N], y_[:N]# 从生成的数据集中提取后面5个样本作为测试数据X_test, y_test = X_[N:], y_[N:]# 绘制数据集的散点图plt.figure(figsize=(8, 6))for cls in np.unique(y):    plt.scatter(X[y==cls][:, 0], X[y==cls][:, 1], s=2)plt.title(&quot;Plot of features of dataset X&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><!-- ![sampla dataset](./Kmeans/sample_dataset_X.png) --><img src="/2024/04/24/ML-Clustering-Kmeans/sample_dataset_X.png" class><h2 id="1-1-First-things-first"><a href="#1-1-First-things-first" class="headerlink" title="1.1 First things first"></a>1.1 First things first</h2><ol><li><p>We need to determine and set a value k, the number of clusters we <strong>think</strong> the data has. KMeans is unsupervised. So it is not the case that you will always know how many clusters (k) exist in the data. You will have to experiment with different values using certain techniques to find the best value of k. For our case, we know that there are 3 clusters, therefore we shall set k to 3. (You can try a different value after the points get clear).</p></li><li><p>We also need to <strong>find k&#x3D;3 random points</strong> that will represent the centers of our clusters if the clustering is successfull. These k random points are called centroids.Let’s work on these two steps next.</p></li></ol><pre><code class="python">k = 3N, feature_size = X.shape# 获取特征的最大值和最小值范围min_feature_range = np.min(X, axis=0)max_feature_range = np.max(X, axis=0)# 在上述范围内生成k个随机点centroids = np.zeros((len(max_feature_range), k))for i, (l, h) in enumerate(zip(min_feature_range, max_feature_range)):    # 使用random.uniform()函数从均匀分布中抽取样本    centroids[i, :] = np.random.uniform(low=l, high=h, size=k)# 转置centroids矩阵，使得每行表示一个聚类中心centroids = centroids.Tprint(centroids)# 绘制数据集的散点图和聚类中心plt.figure(figsize=(8, 6))for cls in np.unique(y):    plt.scatter(X[y==cls][:, 0], X[y==cls][:, 1], s=2)# 绘制聚类中心for i, (x_, y_) in enumerate(centroids):    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=&#39;r&#39;)plt.title(&quot;Plot of features of dataset X&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><!-- ![1.1](./Kmeans/1.1.png) --><img src="/2024/04/24/ML-Clustering-Kmeans/1.1.png" class><p>The points are scattered and may not be close to the cluster centers. There are other methods like the <strong>kmeans++</strong>.</p><h2 id="1-2-Calculate-distances-to-centroids"><a href="#1-2-Calculate-distances-to-centroids" class="headerlink" title="1.2 Calculate distances to centroids"></a>1.2 Calculate distances to centroids</h2><p>In this notebook, we shall be calculating the euclidean distance. We have 2 columns in X <em>(x1 and x2)</em> and we have to calculate the euclidean distance between each centroid and every data point in X. Centroids are of the form <em>(xx, yy)</em> i.e they have two points just like our dataset X has 2 columns.</p><p>计算每个质心和 X 中每个数据点之间的欧几里得距离。</p><p>We want to calculate something of the form $sqrt((x1-xx)^2 + (x2-yy)^2)$ for each data point&#x2F;row in X.<br>To accelerate operations, we shall be using a vectorized approach to calculate that.</p><p>使用矢量化方法来计算。</p><ol><li>We have k&#x3D;3 centroids, so we shall first duplicate <strong>X</strong> 3 times or k times. The shape of X is (1000, 2). The result of the duplication, <strong>Xc</strong> will be (1000, 6).</li><li>Then we shall flatten the centroids so that its a single vector, <strong>centroidsc</strong> of 6 elements to match our 6 columns in <strong>Xc</strong>.</li><li>We shall subtract <strong>Xc</strong> and <strong>centroidsc</strong> to give us a result <strong>D</strong> of shape (1000, 6). This step is equivalent to performing <strong>(x1-xx)</strong> and <strong>(x2-yy)</strong> for all centroids at once.</li></ol><ul><li>The first column of <strong>D</strong> corresponds to <strong>(x1-xx)</strong> where xx is the x of the first centroid.</li><li>The second column of <strong>D</strong> corresponds to <strong>(x2-yy)</strong> where yy is the y of the first centroid.</li><li>The third column of <strong>D</strong> corresponds to <strong>(x1-xx)</strong> where xx is the x of the <em>second</em> centroid.</li><li>The forth column of <strong>D</strong> corresponds to <strong>(x2-yy)</strong> where yy is the y of the <em>second</em> centroid. And so on.</li></ul><p>4.The next step is to square these results, add them and apply sqrt. This whole operation results in what is called the <em>L2 norm</em> and is all performed by the <strong>np.linalg.norm</strong> function.</p><p>Note that we have to calculate the norm over a given set of columns e.g the first and second columns’ norm corresponds to the first centroid, the third and forth to the 2nd centroid and the last 2 to the 3rd centroid. So in the end we have a (1000, 3) array containing euclidean distances of each of the 1000 data rows&#x2F;points in X in correspondence to each of the 3 centroids.</p><pre><code class="python">Xc = np.concatenate([X for c in centroids], axis=1) # duplicate X k timescentroidsc = centroids.ravel() # ravel to allow broadcast; Return a contiguous flattened array.D = (Xc - centroidsc) # raw diffNorms = np.zeros((N, k)) # distances to each centroidfor i in range(0, k):     m = i*feature_size    Norms[:, i] = np.linalg.norm(D[:, m:m+feature_size], axis=1) # Calculating the norms (Euclidean distance)</code></pre><h2 id="1-3-Attach-instances-or-rows-to-the-closest-centroid"><a href="#1-3-Attach-instances-or-rows-to-the-closest-centroid" class="headerlink" title="1.3 Attach instances or rows to the closest centroid"></a>1.3 Attach instances or rows to the closest centroid</h2><p>We shall now assign each data row in X the index of the centroid with which it has the shortest distance. We do that using <strong>np.argmin</strong> which returns the index of the minimum distance in our <em>Norms</em> array.</p><p>使用 <strong>np.argmin</strong> 来为 X 中的每个数据行分配与其距离最短的质心的索引，它返回 <em>Norms</em> 数组中最小距离的索引。</p><pre><code class="python"># sample of indices for smallest indices for sample_normsnp.argmin(sample_norms, axis=1)# we do this for all Normsypred = np.argmin(Norms, axis=1) #Returns the indices of the minimum values along an axis.plt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids):  # with underscore    p = plt.scatter(X[y==i][:, 0], X[y==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    anot = &#39;c&#39;+str(i) + &quot; at &quot; + str(np.round(Norms[0, i], 3))    plt.plot([X[0][0], x_], [X[0][1], y_])    plt.annotate(xy=(x_+.2, y_-.1), text=anot, color=&#39;k&#39;, size=10)    plt.scatter(X[0][0], X[0][1], marker=&#39;o&#39;, c=&#39;r&#39;, s=35)plt.title(&quot;Plot showing distance sample (point belongs to c&quot;+str(ypred[0])+&quot;)&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><!-- ![1.3](./Kmeans/1.3.png) --><img src="/2024/04/24/ML-Clustering-Kmeans/1.3.png" class><h2 id="1-4-Update-centroids"><a href="#1-4-Update-centroids" class="headerlink" title="1.4 Update centroids"></a>1.4 Update centroids</h2><p>The last step is to update the centroids by setting the new centroids at the mean positions of the points they were closest to i.e the data points they influence.</p><p>Below is a plot showing the un-updated centroids and their influence on the data points. We shall have to move the centroids so that they are at the center of the points they influence.</p><pre><code class="python">plt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids):    p = plt.scatter(X[ypred==i][:, 0], X[ypred==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=clr, size=14)plt.title(&quot;Plot showing influence of recent centroids on data points&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><!-- ![1.4.1](./Kmeans/1.4.1.png) --><img src="/2024/04/24/ML-Clustering-Kmeans/1.4.1.png" class><ul><li>To do that, we shall calculate the mean of the data points each centroid influences and put the centroid at that mean location.</li><li>If a centroid has no points it influences (yes, this can happen), we leave the centroid where it is.</li></ul><pre><code class="python">centroids_ = []for i in range(k):    if len(X[ypred == i]) == 0:        centroids_.append(centroids[i]) # use old    else:        centroids_.append(np.mean(X[ypred == i], axis=0))centroids_ = np.array(centroids_)centroids_#After the update, we have the following plotplt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids_):  # with underscore    p = plt.scatter(X[ypred==i][:, 0], X[ypred==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=clr, size=14)plt.title(&quot;Plot showing mean-centered centroids &quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><img src="/2024/04/24/ML-Clustering-Kmeans/1.4.2.png" class><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><ul><li>The mean-centered points may look good, however the data they influence may still be bad</li></ul><h2 id="1-5-Repeat-the-above-steps"><a href="#1-5-Repeat-the-above-steps" class="headerlink" title="1.5 Repeat the above steps"></a>1.5 Repeat the above steps</h2><ul><li>Now we repeat the steps above until the centroids don’t update or move (significantly) anymore. That will be the case when <strong>(centroids_ - centroids)^2</strong> is a low value below a certain threshold. A good threshold has to be as low as possible i.e close to or equal to zero.</li><li>You can run the following code multiple times and observe <strong>shift</strong> value (which is <strong>changes of centroids</strong> ) carefully</li><li>Here, we are using <strong>^2</strong> to make larger shifts&#x2F;updates significant. It doesn’t have to be that way. A norm can also work.</li></ul><pre><code class="python">shift = np.sum((centroids_ - centroids)**2)print(shift)# Below we repeat all the previous steps in one runcentroids = centroids_Xc = np.concatenate([X for c in centroids], axis=1) # duplicate k timescentroidsc = centroids.ravel() # ravel to allow broadcastD = (Xc - centroidsc) # raw diffNorms = np.zeros((N, k)) # distances to each clusterfor i in range(0, k):    m = i*feature_size    Norms[:, i] = np.linalg.norm(D[:, m:m+feature_size], axis=1)# Choose the nearest cluster for every point, and save the result in ypred which is used in next code segment# We can use np.argmin functionypred = np.argmin(Norms, axis=1) # new clusters are mean X along centroids_ = []for i in range(k):    if len(X[ypred == i]) == 0:        centroids_.append(centroids[i]) # use old    else:        centroids_.append(np.mean(X[ypred == i], axis=0))centroids_ = np.array(centroids_)centroids_plt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids_):  # with underscore    p = plt.scatter(X[ypred==i][:, 0], X[ypred==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=clr, size=14)plt.title(&quot;Plot showing mean-centered centroids &quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><img src="/2024/04/24/ML-Clustering-Kmeans/1.5.png" class>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shortest Path Ⅱ</title>
      <link href="/2024/04/17/Shortest-Path-%E2%85%A1/"/>
      <url>/2024/04/17/Shortest-Path-%E2%85%A1/</url>
      
        <content type="html"><![CDATA[<p>In the previous article <a href="https://sheldoncoder1337.github.io/2024/04/17/Shortest-Path-%E2%85%A0/">Shortest-Path-Ⅰ</a>, we have already learn how to use Networkx and Pandana <code>shortest_path</code> API to find the shortest path on <a href="https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc">New York</a> <a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page">New York Taxi Trip</a> dataset. And after comparing the performance between Dijkstra and Constraction Hierarchy algorithm, we could found that CH have a much better performance than classic Dijkstra algorithm.</p><p>In this blog, let’s try to fix the Carpool problem.</p><h2 id="Location-Statistics-Heat-Map-Visualization"><a href="#Location-Statistics-Heat-Map-Visualization" class="headerlink" title="Location Statistics &amp; Heat Map Visualization"></a>Location Statistics &amp; Heat Map Visualization</h2><pre><code class="python">import pandas as pdimport plotly.express as px# Data with latitude/longitude and valuesdf = pd.read_csv(&#39;https://raw.githubusercontent.com/R-CoderDotCom/data/main/sample_datasets/population_galicia.csv&#39;)fig = px.density_mapbox(df, lat = &#39;latitude&#39;, lon = &#39;longitude&#39;, z = &#39;tot_pob&#39;,                        radius = 7,                        center = dict(lat = 42.83, lon = -8.35),                        zoom = 6,                        mapbox_style = &#39;open-street-map&#39;,                        color_continuous_scale = &#39;rainbow&#39;,                        opacity = 0.5)fig.show()</code></pre><h2 id="Carpool-problem"><a href="#Carpool-problem" class="headerlink" title="Carpool problem"></a>Carpool problem</h2><p>With the rise of taxi-hailing mobile programs (such as uber), a New York cab driver is used to take orders from online platform. Given the initial location of the driver and 2-3 orders (e.g., each order is a 6-tuple, like a record in the NY Taxi data), your task is to find a feasible route to pick up all the orders.</p><p>For instance, the driver is now at location Time Square, he is assigned to pick up three passengers.</p><ul><li>passengerA: JFK_Airport to East_Chelsea</li><li>passengerB: West_Village to East_Chelsea</li><li>passengerC: Battery_Park_City to Queens_Plaza</li></ul><p>One feasible solution is to report the route from</p><ul><li>Time Square -&gt; JFK_Airport -&gt; East_Chelsea -&gt; West_Village -&gt; East_Chelsea -&gt; Battery_Park_City -&gt; Queens_Plaza</li></ul><h3 id="Our-target"><a href="#Our-target" class="headerlink" title="Our target"></a>Our target</h3><ol><li>Write a function to determine the route and the total distance of the route.</li><li>Plot the route on the map.</li></ol><p>Obviously, the feasible solution is far from optimal as East_Chelsea is the common locations of two pessagers. Thereby,  </p><h3 id="Bonus-task"><a href="#Bonus-task" class="headerlink" title="Bonus task"></a>Bonus task</h3><ul><li>Try to find the best route based on the given orders. For the bonus part, please explain your methodology and your mark will be given based on the soundness of your idea, the quality of analysis, and the implementation.</li></ul><pre><code class="python"></code></pre>]]></content>
      
      
      <categories>
          
          <category> Algo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm, find-the-shortest-path </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shortest Path Ⅰ</title>
      <link href="/2024/04/17/Shortest-Path-%E2%85%A0/"/>
      <url>/2024/04/17/Shortest-Path-%E2%85%A0/</url>
      
        <content type="html"><![CDATA[<p>In this series, I will introduce some third-party libraries such as osmnx, pandana, geopandas and compare the performance between <strong>NetworkX(Dijkstra)</strong> and <strong>Pandana( Constraction Hierarchy)</strong>. Finally, I will show how to use these libraries to solve a <strong>Carpool(拼车) problem</strong>. The data set used in this article is from <a href="https://www.openstreetmap.org/">OpenStreetMap</a> - New York City Taxi Trip data set.</p><h2 id="preliminary"><a href="#preliminary" class="headerlink" title="preliminary"></a>preliminary</h2><p>You are highly recommended to use Conda to setup a new virtual environment.</p><pre><code class="bash">conda create -n geospatial python==3.8conda activate geospatialpip install geopandas network osmnet osmnx pandas pandana</code></pre><p>If you received an error like “spatialindex_c-64.dll is missing”, try to use the following commands to resolve it.</p><pre><code class="bash">pip uninstall rtreepip install rtree</code></pre><pre><code class="python">import warningswarnings.filterwarnings(&#39;ignore&#39;)warnings.simplefilter(&#39;ignore&#39;)import osmnx as oximport numpy as npimport geopandas as gpdimport pandanaimport pandas as pdfrom time import timeimport matplotlib.pyplot as pltfrom IPython.display import display, clear_outputimport networkx as nximport momepy</code></pre><h2 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h2><pre><code class="python">def extract_graph(place=&#39;New York&#39;):    # try Chinese    # G = ox.graph_from_place(&#39;纽约&#39;, network_type=&#39;drive&#39;)    ox.config(log_console=True, use_cache=True)    G = ox.graph_from_place(place, network_type=&#39;drive&#39;)    return Gplace = &#39;New York&#39;G = extract_graph(place)ox.plot_graph(G, bgcolor=&quot;w&quot;, node_size=1, node_color=&quot;yellow&quot;, edge_color=&quot;#aaa&quot;)print(&quot;node count:&quot;, len(G.nodes()))print(&quot;edge count:&quot;, len(G.edges()))</code></pre><!-- ![New York Taxi Trip](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/New-York-Taxi-Trip.png?raw=true) --><img src="/2024/04/17/Shortest-Path-%E2%85%A0/New-York-Taxi-Trip.png" class><p>There are total node 55344 nodes and 139582 edges.</p><p>We process <a href="https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc">New York</a> <a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page">New York Taxi Trip</a>  and provide Trips.txt (<a href="https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/sources/Shortest-Path/Trips.txt">Appendix</a>)</p><p>Trips.txt contains New York Taxi trajectory information for 10,000 lines, each containing six columns of information, the region name where the passengers are picked up(PName),the lon and lat of the region in which they are picked up(PLon PLat),the region name they are delivered(Dname) and in which the passenger is delivered(DLon DLat).</p><p>For example:</p><table><thead><tr><th>PName</th><th>PLon</th><th>PLat</th><th>DName</th><th>DLon</th><th>DLat</th></tr></thead><tbody><tr><td>Lincoln_Square_East</td><td>-73.97382133</td><td>40.73788468</td><td>Upper_East_Side_North</td><td>-73.91715837</td><td>40.8541322</td></tr><tr><td>Upper_East_Side_North</td><td>-73.91715837</td><td>40.8541322</td><td>Central_Harlem_North</td><td>-73.99804922</td><td>40.71156838</td></tr></tbody></table><h2 id="Find-the-Shortest-Path"><a href="#Find-the-Shortest-Path" class="headerlink" title="Find the Shortest Path"></a>Find the Shortest Path</h2><p>There are two ways to find the shortest path, please check the docs for more details:</p><ol><li><a href="https://networkx.org/documentation/stable/reference/algorithms/shortest_paths.html">NetworkX</a></li><li><a href="https://udst.github.io/pandana/">Pandana(CH)</a></li></ol><h3 id="NetworkX-Dijkstra"><a href="#NetworkX-Dijkstra" class="headerlink" title="NetworkX(Dijkstra)"></a>NetworkX(Dijkstra)</h3><pre><code class="python"># The first trip record is from Lincoln_Square_East to Upper_East_Side_Northnx_Lincoln_Square_East_id = ox.distance.nearest_nodes(G,Lincoln_Square_East_Location.x,Lincoln_Square_East_Location.y)[0]nx_Upper_East_Side_North_id = ox.distance.nearest_nodes(G,Upper_East_Side_North_Location.x,Upper_East_Side_North_Location.y)[0]# NetworkX shortest pathdef SP_NX(G,SID,TID):    return nx.shortest_path(G, source=SID, target=TID, method=&quot;dijkstra&quot;, weight=&#39;length&#39;)     #displayNX_PATH=SP_NX(G,nx_Lincoln_Square_East_id,nx_Upper_East_Side_North_id)    fig , ax = ox.plot_graph(G, bgcolor=&quot;w&quot;, node_size=1, node_color=&quot;gray&quot;, edge_color=&quot;#aaa&quot;,show=False,close=False)ax.scatter(-73.97382133,40.73788468,c=&#39;yellow&#39;,marker=&quot;s&quot;,alpha=1,zorder=4)ax.scatter(-73.91715837,40.8541322,c=&#39;blue&#39;,alpha=1,zorder=3)ox.plot_graph_route(G,NX_PATH,ax=ax,orig_dest_size=0,route_alpha=0.5,route_colors=&#39;r&#39;,route_linewidths=2,show=False,close=False)</code></pre><!-- ![shortest path networkx Dijkstra](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/Shortest-Path-NetworkX.png?raw=true) --><img src="/2024/04/17/Shortest-Path-%E2%85%A0/Shortest-Path-NetworkX.png" class><h3 id="Pandana-CH"><a href="#Pandana-CH" class="headerlink" title="Pandana(CH)"></a>Pandana(CH)</h3><pre><code class="python"># trans road network to pandana formatnodes,edges = ox.graph_to_gdfs(G,nodes=True,edges=True)edges = edges.reset_index()G_pan = pandana.Network(nodes[&#39;x&#39;], nodes[&#39;y&#39;], edges[&#39;u&#39;], edges[&#39;v&#39;], edges[[&#39;length&#39;]],twoway=False)# The first trip record is from Lincoln_Square_East to Upper_East_Side_NorthLincoln_Square_East_Location = pd.DataFrame(&#123;&#39;longitude&#39;:[-73.97382133], &#39;latitude&#39;: [40.73788468]&#125;)Lincoln_Square_East_Location = gpd.points_from_xy(Lincoln_Square_East_Location.longitude, Lincoln_Square_East_Location.latitude, crs=&quot;EPSG:4326&quot;)Upper_East_Side_North_Location = pd.DataFrame(&#123;&#39;longitude&#39;:[-73.91715837], &#39;latitude&#39;: [40.8541322]&#125;)Upper_East_Side_North_Location = gpd.points_from_xy(Upper_East_Side_North_Location.longitude, Upper_East_Side_North_Location.latitude, crs=&quot;EPSG:4326&quot;)pan_Lincoln_Square_East_id = G_pan.get_node_ids(Lincoln_Square_East_Location.x,Lincoln_Square_East_Location.y).iloc[0]pan_Upper_East_Side_North_id = G_pan.get_node_ids(Upper_East_Side_North_Location.x,Upper_East_Side_North_Location.y).iloc[0]# pandana shortest pathdef SP_PAN(G_pan,SID,TID):    return G_pan.shortest_path(SID,TID) #displayPAN_PATH=SP_PAN(G_pan,pan_Lincoln_Square_East_id,pan_Upper_East_Side_North_id)    fig , ax = ox.plot_graph(G, bgcolor=&quot;w&quot;, node_size=1, node_color=&quot;gray&quot;, edge_color=&quot;#aaa&quot;,show=False,close=False)ax.scatter(-73.97382133,40.73788468,c=&#39;yellow&#39;,marker=&quot;s&quot;,alpha=1,zorder=4)ax.scatter(-73.91715837,40.8541322,c=&#39;blue&#39;,alpha=1,zorder=3)ox.plot_graph_route(G,PAN_PATH,ax=ax,orig_dest_size=0,route_alpha=0.5,route_colors=&#39;r&#39;,route_linewidths=2,show=False,close=False)</code></pre><!-- ![shortest path pandana CH](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/Shortest-Path-Pandana-CH.png?raw=true) --><img src="/2024/04/17/Shortest-Path-%E2%85%A0/Shortest-Path-Pandana-CH.png" class><h3 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h3><pre><code class="python"># you should upload trips.txt to your jupyter notebook first pickup_name=[]pickup_lon=[]pickup_lat=[]disengaged_name=[]disengaged_lon=[]disengaged_lat=[]import csv # opening the CSV filewith open(&#39;trips.txt&#39;, mode =&#39;r&#39;)as file:     # reading the CSV file  csvFile = csv.reader(file)    # displaying the contents of the CSV file  for lines in csvFile:        pickup_name.append(lines[0])        pickup_lon.append(lines[1])        pickup_lat.append(lines[2])        disengaged_name.append(lines[3])        disengaged_lon.append(lines[4])        disengaged_lat.append(lines[5])pickup_info = pd.DataFrame(&#123;&#39;pickup_name&#39;:pickup_name,&#39;longitude&#39;:pickup_lon, &#39;latitude&#39;: pickup_lat&#125;)disengaged_info = pd.DataFrame(&#123;&#39;disengaged_name&#39;:disengaged_name,&#39;longitude&#39;:disengaged_lon, &#39;latitude&#39;: disengaged_lat&#125;)pickup_Location = gpd.points_from_xy(pickup_info.longitude, pickup_info.latitude, crs=&quot;EPSG:4326&quot;)disengaged_Location = gpd.points_from_xy(disengaged_info.longitude, disengaged_info.latitude, crs=&quot;EPSG:4326&quot;)pickup_id = G_pan.get_node_ids(pickup_Location.x,pickup_Location.y)disengaged_id = G_pan.get_node_ids(disengaged_Location.x,disengaged_Location.y)nx_pickup_id = list(ox.distance.nearest_nodes(G,pickup_Location.x,pickup_Location.y))nx_disengaged_id = list(ox.distance.nearest_nodes(G,disengaged_Location.x,disengaged_Location.y))time_PAN=[]time_NX=[]test=[1,5,10,50,100,200,300,500,1000] # the query sizeNX_BATCH_PATH=[]PAN_BATCH_PATH=[]# This is the loop for evaluating the time of NetworkXfor i in range(len(test)):        tik = time()    for j in range(test[i]):         NX_BATCH_PATH.append(nx.shortest_path(G,source=nx_pickup_id[j],target=nx_disengaged_id[j],method=&#39;dijkstra&#39;,weight=&#39;length&#39;))        tok = time()    time_NX.append(tok-tik)    print(&#39;when query size = &#39;,test[i],end=&#39; , &#39;)    print(&#39;Time of Networkx is : &#39;,time_NX[-1],end=&#39;s\n&#39;)# This is the loop for evaluating the time of Pandanafor i in range(len(test)):    tik = time()    for j in range(test[i]):        PAN_BATCH_PATH.append(G_pan.shortest_path(pickup_id[j],disengaged_id[j]))        tok = time()    time_PAN.append(tok-tik)    print(&#39;when query size = &#39;,test[i],end=&#39; , &#39;)    print(&#39;Time of Pandana is : &#39;,time_PAN[-1],end=&#39;s\n&#39;)fig = plt.figure()ax = fig.add_subplot(1, 1, 1) clear_output(wait = True)ax.plot(test,time_PAN,label=&#39;Panadana&#39;)ax.plot(test,time_NX,label=&#39;Networkx&#39;)plt.ylabel(&#39;computing time(s)&#39;)plt.xlabel(&#39;Number of Query&#39;)plt.legend()fig.show()</code></pre><!-- ![shortest path comparison](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/shortest-path-comparison.png?raw=true) --><img src="/2024/04/17/Shortest-Path-%E2%85%A0/shortest-path-comparison.png" class><p>Here, we use Batch evaluation between Dijkstra (NetworkX) and CH (Pandana), and the results shows that CH algor is much faster than classical Dijskra.</p>]]></content>
      
      
      <categories>
          
          <category> Algo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm, find-the-shortest-path </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo hand book</title>
      <link href="/2024/04/16/Tools-hexo-hand-book/"/>
      <url>/2024/04/16/Tools-hexo-hand-book/</url>
      
        <content type="html"><![CDATA[<h2 id="发布文章"><a href="#发布文章" class="headerlink" title="发布文章"></a>发布文章</h2><p>进入博客所在目录，右键打开Git Bash Here，创建博文：</p><pre><code class="bash">hexo new &quot;article title&quot;</code></pre><p>然后 source 文件夹中会出现一个 My New Post.md 文件，就可以使用 Markdown 编辑器在该文件中撰写文章了。</p><p>写完后运行下面代码将文章渲染并部署到 GitHub Pages 上完成发布。以后每次发布文章都是这两条命令。</p><pre><code class="bash">hexo g   # 生成页面hexo d   # 部署发布</code></pre><p>也可以不使用命令自己创建 .md 文件，只需在文件开头手动加入如下格式 Front-matter 即可，写完后运行 hexo g 和 hexo d 发布。</p><pre><code class="markdown">---title: Hello World # 标题date: 2019/3/26 hh:mm:ss # 时间categories: # 分类- Diarytags: # 标签- PS3- Games---摘要&lt;!--more--&gt;正文</code></pre><h2 id="网站设置"><a href="#网站设置" class="headerlink" title="网站设置"></a>网站设置</h2><p>包括网站名称、描述、作者、链接样式等，全部在网站目录下的 _config.yml 文件中，参考官方文档按需要编辑。</p><p>注意：冒号后要加一个空格！</p><h2 id="更换主题"><a href="#更换主题" class="headerlink" title="更换主题"></a>更换主题</h2><p>在 Themes | Hexo 选择一个喜欢的主题，比如 NexT，进入网站目录打开 Git Bash Here 下载主题：</p><pre><code class="bash">git clone https://github.com/iissnan/hexo-theme-next themes/next</code></pre><p>然后修改 _config.yml 中的 theme 为新主题名称 next，发布。（有的主题需要将 _config.yml 替换为主题自带的，参考主题说明。）</p><h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><pre><code class="bash">hexo new &quot;name&quot;       # 新建文章hexo new page &quot;name&quot;  # 新建页面hexo g                # 生成页面hexo d                # 部署hexo g -d             # 生成页面并部署hexo s                # 本地预览hexo clean            # 清除缓存和已生成的静态文件hexo help             # 帮助</code></pre><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><p>1、Hexo 设置显示文章摘要，首页不显示全文</p><p>Hexo 主页文章列表默认会显示文章全文，浏览时很不方便，可以在文章中插入</p><pre><code class="markdown">&lt;!--more--&gt;</code></pre><p>进行分段。</p><p>该代码前面的内容会作为摘要显示，而后面的内容会替换为 “Read More” 隐藏起来。</p><p>2、设置网站图标</p><p>进入 themes&#x2F;主题 文件夹，打开 _config.yml 配置文件，找到 favicon 修改，一般格式为：favicon: 图标地址。（不同主题可能略有差别）</p><p>3、修改并部署后没有效果</p><p>使用 hexo clean 清理后重新部署。</p><p>4、markdown图片引入没有效果</p><ul><li>统一改为Github仓库图片链接，例子：</li></ul><pre><code class="markdwon">![&quot;图片标题&quot;](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/temp/sheldon.png?raw=true)</code></pre><ul><li>使用模板语言</li></ul><pre><code class="markdown">&#123;% asset_img sheldon.png "图片标题" %&#125;</code></pre><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>Hexo 是一种纯静态的博客，我们必须要在本地完成文章的编辑再部署到 GitHub 上，依赖于本地环境。不能像 WordPress 或 Typecho 那样的动态博客一样能直接在浏览器中完成撰文和发布。</p><p>可以说是一种比较极客的写博客方式，但是优势也是明显的——免费稳定省心，比较适合爱折腾研究的用户，或者没有在线发文需求的朋友。</p><p><a href="https://hexo.io/">Hexo</a>!  Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>]]></content>
      
      
      <categories>
          
          <category> tools </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ML-Clustering-Practice</title>
      <link href="/2024/04/10/ML-Clustering-Practice/"/>
      <url>/2024/04/10/ML-Clustering-Practice/</url>
      
        <content type="html"><![CDATA[<p>The notebook performs three tasks on the dataset <code>Wine</code>.</p><ul><li>Task 1: Implement and try different clustering techniques</li><li>Task 2: Evaluate the clustering results against ground truth</li><li>Task 3: Dimensionality reduction and visualization</li></ul><h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><pre><code class="python"># Load necessary libraries.import numpy as npimport matplotlib.pyplot as pltfrom sklearn.cluster import KMeans, AgglomerativeClusteringfrom sklearn.mixture import GaussianMixturefrom sklearn.metrics import silhouette_samples, silhouette_scorefrom sklearn.metrics import adjusted_rand_score, normalized_mutual_info_scorefrom scipy.cluster.hierarchy import dendrogramimport pandas as pdfrom sklearn.decomposition import PCAfrom sklearn.manifold import TSNEfrom sklearn.preprocessing import StandardScalerfrom sklearn.datasets import load_winenp.random.seed(20240410)def plot_clusters(X, y):    plt.scatter(X[:, 0], X[:, 1], c=y, marker=&#39;o&#39;, cmap=plt.cm.coolwarm)    plt.xlabel(&#39;x1&#39;)    plt.ylabel(&#39;x2&#39;)# load datasetwine = load_wine()print(wine.feature_names)print(wine.DESCR)X, y = load_wine(return_X_y=True)# feature normalizationscaler = StandardScaler()X = scaler.fit_transform(X)</code></pre><pre><code class="txt">...Wine recognition dataset------------------------**Data Set Characteristics:**    :Number of Instances: 178 (50 in each of three classes)    :Number of Attributes: 13 numeric, predictive attributes and the class    :Attribute Information:    - Alcohol    - Malic acid    - Ash    - Alcalinity of ash      - Magnesium    - Total phenols    - Flavanoids    - Nonflavanoid phenols    - Proanthocyanins    - Color intensity    - Hue    - OD280/OD315 of diluted wines    - Proline    - class:            - class_0            - class_1            - class_2    :Summary Statistics:        ============================= ==== ===== ======= =====                                   Min   Max   Mean     SD    ============================= ==== ===== ======= =====    Alcohol:                      11.0  14.8    13.0   0.8    Malic Acid:                   0.74  5.80    2.34  1.12    Ash:                          1.36  3.23    2.36  0.27    Alcalinity of Ash:            10.6  30.0    19.5   3.3    Magnesium:                    70.0 162.0    99.7  14.3    Total Phenols:                0.98  3.88    2.29  0.63    Flavanoids:                   0.34  5.08    2.03  1.00    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12    Proanthocyanins:              0.41  3.58    1.59  0.57    Colour Intensity:              1.3  13.0     5.1   2.3    Hue:                          0.48  1.71    0.96  0.23    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71    Proline:                       278  1680     746   315    ============================= ==== ===== ======= =====...</code></pre><h2 id="Task-1-Implement-and-try-different-clustering-techniques"><a href="#Task-1-Implement-and-try-different-clustering-techniques" class="headerlink" title="Task 1: Implement and try different clustering techniques"></a>Task 1: Implement and try different clustering techniques</h2><ul><li>Implement the following clustering methods and tune their hyperparameters accordingly. Set the random_state for ALL methods when applicable as random_state&#x3D;20240410 to ensure the reproducibility.<ul><li>kmeans, using silhouette analysis to find the optimal number of clusters, ranging from 2 to 5 (both included), same for other methods</li><li>Hierarchical (Agglomerative) clustering, using silhouette analysis to find the optimal number of clusters and linkage functions</li><li>Gaussian Mixture Model, using silhouette analysis to find the optimal number of clusters and covariance types</li></ul></li></ul><h3 id="Kmeans"><a href="#Kmeans" class="headerlink" title="Kmeans"></a>Kmeans</h3><ul><li>kmeans, using silhouette analysis to find the optimal number of clusters, ranging from 2 to 5 (both included), same for other methods</li></ul><pre><code class="python"># kmeansn_clusters = range(2,6)highest_scores = 0for n in n_clusters:    kmeans = KMeans(n_clusters=n, random_state=20240410)    kmeans.fit(X)    cluster_labels = kmeans.labels_    silhouette_avg = silhouette_score(X, cluster_labels)    if silhouette_avg &gt; highest_scores:        highest_scores = silhouette_avg        best_n = nkmeans = KMeans(n_clusters=best_n, random_state=20240410)kmeans.fit(X)cluster_labels = kmeans.labels_silhouette_avg = silhouette_score(X, cluster_labels)print(f&quot;The best parameters are: n_clusters=&#123;best_n&#125;&quot;)print(&quot;The average silhouette_score is :&quot;, silhouette_avg)</code></pre><pre><code class="txt">The best parameters are: n_clusters=3The average silhouette_score is : 0.2848589191898987</code></pre><h3 id="Agglomerative-clustering"><a href="#Agglomerative-clustering" class="headerlink" title="Agglomerative clustering"></a>Agglomerative clustering</h3><ul><li>Hierarchical (Agglomerative) clustering, using silhouette analysis to find the optimal number of clusters and linkage functions</li></ul><pre><code class="python"># agglomerative clusteringlinkages = [&#39;ward&#39;, &#39;complete&#39;, &#39;average&#39;, &#39;single&#39;]n_clusters = range(2,6)highest_scores = 0for n in n_clusters:    for link in linkages:        agglo = AgglomerativeClustering(n_clusters=n, linkage = link)        agglo.fit(X)        cluster_labels = agglo.labels_        silhouette_avg = silhouette_score(X, cluster_labels)        if silhouette_avg &gt; highest_scores:            highest_scores = silhouette_avg            best_n = n            best_linkage = linkagglo = AgglomerativeClustering(n_clusters=best_n, linkage = best_linkage)agglo.fit(X)cluster_labels = agglo.labels_silhouette_avg = silhouette_score(X, cluster_labels)print(f&quot;The best parameters are: n_clusters=&#123;best_n&#125;, linkage=&#123;best_linkage&#125;&quot;)print(&quot;The average silhouette_score is :&quot;, silhouette_avg)</code></pre><pre><code class="txt">The best parameters are: n_clusters=3, linkage=wardThe average silhouette_score is : 0.2774439826952266</code></pre><h3 id="Gaussian-mixture-model"><a href="#Gaussian-mixture-model" class="headerlink" title="Gaussian mixture model"></a>Gaussian mixture model</h3><pre><code class="python"># gaussian mixture modelcovariance_type = [&#39;full&#39;,&#39;tied&#39;,&#39;diag&#39;,&#39;spherical&#39;]n_clusters = range(2,6)highest_scores = 0for n in n_clusters:    for covar in covariance_type:        gmm = GaussianMixture(n_components=n, covariance_type=covar, random_state=20240410)        cluster_labels = gmm.fit_predict(X)        silhouette_avg = silhouette_score(X, cluster_labels)        # print(f&quot;Covar: &#123;covar&#125; cluster: &#123;n&#125; Score: &#123;silhouette_avg&#125;&quot;)        if silhouette_avg &gt; highest_scores:            highest_scores = silhouette_avg            best_n = n            best_covar = covargmm = GaussianMixture(n_components=best_n, covariance_type=best_covar, random_state=20240410)cluster_labels = gmm.fit_predict(X)silhouette_avg = silhouette_score(X, cluster_labels)print(f&quot;The best parameters are: n_clusters=&#123;best_n&#125;, covariance_type=&#123;best_covar&#125;&quot;)print(&quot;The average silhouette_score is :&quot;, silhouette_avg)</code></pre><pre><code class="txt">The best parameters are: n_clusters=3, covariance_type=fullThe average silhouette_score is : 0.28356363134288903</code></pre><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul><li>What are the best-performing hyperparameters for each method?<ul><li>kmeans:<ul><li>n_clusters&#x3D;3, aver_silhouette_score: 0.2848589191898987</li></ul></li><li>Hierarchical (Agglomerative):<ul><li>n_clusters&#x3D;3, linkage&#x3D;ward, aver_silhouette_score: 0.2774439826952266</li></ul></li><li>Gaussian Mixture Model:<ul><li>n_clusters&#x3D;3, covariance_type&#x3D;full, aver_silhouette_score is : 0.28356363134288903</li></ul></li></ul></li></ul><h2 id="Task-2-Evaluate-the-clustering-results-against-ground-truth"><a href="#Task-2-Evaluate-the-clustering-results-against-ground-truth" class="headerlink" title="Task 2: Evaluate the clustering results against ground truth"></a>Task 2: Evaluate the clustering results against ground truth</h2><ul><li>Select the best-performing hyperparameters for each method, set the random_state for ALL methods when applicable as random_state&#x3D;20240410 to ensure the reproducibility.</li><li>Evaluate the clustering results from the three methods (kmeans, agglomerative clustering, and Gaussian mixture model) against ground truth y using<ul><li><ol><li>adjusted rand index and</li></ol></li><li><ol start="2"><li>normalized mutual information.</li></ol></li></ul></li></ul><pre><code class="python"># evaluation using ARI and NMIplt.figure(figsize=(12, 4))kmeans = KMeans(n_clusters=3, random_state=20240410)kmeans.fit(X)cluster_labels = kmeans.labels_ari = adjusted_rand_score(cluster_labels, y)nmi = normalized_mutual_info_score(cluster_labels, y)print(&quot;kmeans ARI is:&quot;, ari,  &quot;NMI is :&quot;, nmi)plt.subplot(1,3,1, title=&quot;kmeans&quot;)plot_clusters(X, cluster_labels)agglo = AgglomerativeClustering(n_clusters=3, linkage = &#39;ward&#39;)agglo.fit(X)cluster_labels = agglo.labels_ari = adjusted_rand_score(cluster_labels, y)nmi = normalized_mutual_info_score(cluster_labels, y)print(&quot;Agglo ARI is:&quot;, ari,  &quot;NMI is :&quot;, nmi)plt.subplot(1,3,2, title=&quot;agglo&quot;)plot_clusters(X, cluster_labels)gmm = GaussianMixture(n_components=3, covariance_type=&#39;full&#39;, random_state=20240410)cluster_labels = gmm.fit_predict(X)ari = adjusted_rand_score(cluster_labels, y)nmi = normalized_mutual_info_score(cluster_labels, y)print(&quot;GMM ARI is:&quot;, ari,  &quot;NMI is :&quot;, nmi)plt.subplot(1,3,3, title=&quot;gmm&quot;)plot_clusters(X, cluster_labels)</code></pre><pre><code class="txt">kmeans ARI is: 0.8974949815093207 NMI is : 0.8758935341223069Agglo ARI is: 0.7899332213582837 NMI is : 0.7864652657004837GMM ARI is: 0.8974949815093207 NMI is : 0.8758935341223069</code></pre><!-- ![Wine-Task2-Clustering](./Wine-Task2-Clustering.png) --><ul><li>Which one is the best performing method?<ul><li>both kmeans and GMM are best performing methods with (ARI&#x3D;0.8974949815093207,NMI&#x3D;0.8758935341223069)</li></ul></li></ul><h2 id="Task-3-Dimensionality-reduction-and-visualization"><a href="#Task-3-Dimensionality-reduction-and-visualization" class="headerlink" title="Task 3:  Dimensionality reduction and visualization"></a>Task 3:  Dimensionality reduction and visualization</h2><h3 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h3><ul><li>The dataset contains 13 features, making it hard to visualize. Perform PCA to reduce it to a 2D space and visualize it using plot_clusters.</li></ul><pre><code class="python"># PCA visualizationpca = PCA(n_components=2, random_state=20240410)X_reduced = pca.fit_transform(X)print(&quot;Singular values are:&quot;, pca.singular_values_)print(&quot;Absolute explained variance are:&quot;, pca.explained_variance_)print(&quot;Ratio of explained variance are:&quot;, pca.explained_variance_ratio_)print(&quot;Cumulative ratio of explained variance are:&quot;, np.cumsum(pca.explained_variance_ratio_))plot_clusters(X_reduced, y)</code></pre><pre><code class="txt">Singular values are: [28.94203422 21.08225141]Absolute explained variance are: [4.73243698 2.51108093]Ratio of explained variance are: [0.36198848 0.1920749 ]Cumulative ratio of explained variance are: [0.36198848 0.55406338]</code></pre><ul><li>What is the ratio of explained variance captured by the 2D space?<ul><li>0.55406338</li></ul></li></ul><pre><code class="python">pca = PCA(random_state=20240410)X_pca = pca.fit_transform(X)cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)n_components = np.where(cumulative_variance_ratio &gt;= 0.8)[0][0] + 1print(f&quot;The number of PCs to retain &gt;80% data info is: &#123;n_components&#125;&quot;)</code></pre><pre><code class="txt">The number of PCs to retain &gt;80% data info is: 5</code></pre><h3 id="T-SNE"><a href="#T-SNE" class="headerlink" title="T-SNE"></a>T-SNE</h3><ul><li>If the ratio of captured variance is too low (&lt;0.8 for example), it means a large loss of information by using linear transformation. So you may want to try t-SNE for visualization. Can you use t-SNE to project X to 2D and visualize clusters with ground truth y? Set random_state&#x3D;20240410 to ensure the reproducibility</li></ul><pre><code class="python"># TSNE visualizationtsne = TSNE(n_components=2, random_state=20240410)X_reduced = tsne.fit_transform(X)  plt.figure(figsize=(12, 6))plt.subplot(1,2,1, title=&quot;original&quot;)plot_clusters(X, y)plt.subplot(1,2,2, title=&quot;2D-X_reduced&quot;)plot_clusters(X_reduced, y)</code></pre><!-- ![TSNE](./Wine-Task3-TSNE.png) --><h3 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h3><ul><li>Visualize the clustering results (predicted labels) from the three methods (kmeans, agglomerative clustering, and Gaussian mixture model). Compare the clustering results visually against the ground truth, and think about whether the visualization corresponds to your answer to the last question in Task 2.</li></ul><pre><code class="python">plt.figure(figsize=(12, 12))plt.subplot(2,2,1, title=&quot;original&quot;)plot_clusters(X, y)kmeans = KMeans(n_clusters=3, init=&#39;random&#39;, random_state=20240410)kmeans.fit(X)cluster_labels = kmeans.labels_ari = adjusted_rand_score(cluster_labels, y)nmi = normalized_mutual_info_score(cluster_labels, y)print(&quot;kmeans ARI is:&quot;, ari,  &quot;NMI is :&quot;, nmi)plt.subplot(2,2,2, title=&quot;kmeans&quot;)plot_clusters(X, cluster_labels)agglo = AgglomerativeClustering(n_clusters=3, linkage = &#39;ward&#39;)agglo.fit(X)cluster_labels = agglo.labels_ari = adjusted_rand_score(cluster_labels, y)nmi = normalized_mutual_info_score(cluster_labels, y)print(&quot;Agglo ARI is:&quot;, ari,  &quot;NMI is :&quot;, nmi)plt.subplot(2,2,3, title=&quot;agglo&quot;)plot_clusters(X, cluster_labels)gmm = GaussianMixture(n_components=3, covariance_type=&#39;full&#39;, random_state=20240410)cluster_labels = gmm.fit_predict(X)ari = adjusted_rand_score(cluster_labels, y)nmi = normalized_mutual_info_score(cluster_labels, y)print(&quot;GMM ARI is:&quot;, ari,  &quot;NMI is :&quot;, nmi)plt.subplot(2,2,4, title=&quot;gmm&quot;)plot_clusters(X, cluster_labels)</code></pre><!-- ![Wine-Task3-Clustering-Compare](./Wine-Task3-Clustering-Compare.png) -->]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ML-Introduction</title>
      <link href="/2024/04/01/ML-Introduction/"/>
      <url>/2024/04/01/ML-Introduction/</url>
      
        <content type="html"><![CDATA[<blockquote><p>“Field of study that gives computers the ability to learn without being explicitly programmed” — Arthur Samuel (1959)</p></blockquote><h2 id="The-Goal-of-these-ML-notes"><a href="#The-Goal-of-these-ML-notes" class="headerlink" title="The Goal of these ML notes"></a>The Goal of these ML notes</h2><ul><li>Understand basic machine learning principles and algorithms</li><li>Get familiar with popular ML methodologies<ul><li>Data representations</li><li>Models</li><li>Learning algorithms</li><li>Experimental methodologies</li></ul></li><li>Present ML solutions to real-world problems</li><li>Master the basic methods and tools of common machine learning algorithms</li></ul><h2 id="Content-of-the-notes"><a href="#Content-of-the-notes" class="headerlink" title="Content of the notes"></a>Content of the notes</h2><ul><li>Chapter1: Regression<ul><li>Regression</li><li>Ridge and Lasso Regression</li><li>Nearest Neighbor and Kernel Regression</li></ul></li><li>Chapter2: Classification<ul><li>Logistic Regression</li><li>Decision Trees</li><li>Boosting and Bagging</li><li>SVM and Naïve Bayes</li><li>Practical Issues for Classification</li></ul></li><li>Chapter3: Clustering<ul><li>Clustering and K-means</li><li>Hierarchical Clustering and DBscan</li><li>Mixture Models</li></ul></li><li>Chapter4: Deep Learning<ul><li>Deep Learning (Convolutional Neural Networks)</li><li>Deep Learning (Generative Neural Networks)</li></ul></li></ul><h2 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h2><ul><li>Download and install Anaconda Individual Edition</li><li><a href="https://www.anaconda.com/products/individual">https://www.anaconda.com/products/individual</a></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li>Chris Bishop. “Pattern Recognition and Machine Learning ”. Springer 2006.</li><li>Andreas C. Müller, Sarah Guido, Introduction to Machine Learning with Python,O’Reilly Media, Inc.</li><li>周志华《机器学习》清华大学出版社，2016.</li><li>Tom Mitchell. “Machine Learning ”. McGraw-Hill, 1997</li></ul>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DA-Array</title>
      <link href="/2024/01/01/DA-01-Array/"/>
      <url>/2024/01/01/DA-01-Array/</url>
      
        <content type="html"><![CDATA[<h2 id="数组-Array"><a href="#数组-Array" class="headerlink" title="数组 Array"></a>数组 Array</h2><table><thead><tr><th>Operation</th><th>Time Complexity</th></tr></thead><tbody><tr><td>访问(Access)</td><td>O(1)</td></tr><tr><td>搜索(Search)</td><td>O(n)</td></tr><tr><td>插入(Insert)</td><td>O(n)</td></tr><tr><td>删除(Delete)</td><td>O(n)</td></tr></tbody></table><h2 id="C"><a href="#C" class="headerlink" title="C"></a>C</h2><pre><code class="c">// create an arrayint arr[5] = &#123; 0 &#125;; // &#123;0, 0, 0, 0, 0&#125;int nums[5] = &#123; 1, 3, 2, 5, 4 &#125;;// Add elementarr[0] = 1;// Delete elementarr[0] = 0;// Search elementfor (int i = 0; i &lt; 5; i++) &#123;    if (arr[i] == 1) &#123;        printf(&quot;%d&quot;, i);        break;    &#125;&#125;// Change elementarr[0] = 2;</code></pre><h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><pre><code class="python">arr = []</code></pre><h2 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h2><pre><code class="java"></code></pre><h2 id="Leetcode"><a href="#Leetcode" class="headerlink" title="Leetcode"></a>Leetcode</h2><h3 id="485-Max-Consecutive-Ones"><a href="#485-Max-Consecutive-Ones" class="headerlink" title="485.Max Consecutive Ones"></a>485.Max Consecutive Ones</h3><h3 id="283-Move-Zeroes"><a href="#283-Move-Zeroes" class="headerlink" title="283.Move Zeroes"></a>283.Move Zeroes</h3><h3 id="27-Remove-Element"><a href="#27-Remove-Element" class="headerlink" title="27.Remove Element"></a>27.Remove Element</h3>]]></content>
      
      
      <categories>
          
          <category> DA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data structure &amp; Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
