<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>svd</title>
      <link href="/2024/05/04/svd/"/>
      <url>/2024/05/04/svd/</url>
      
        <content type="html"><![CDATA[<p>Problem: Decompose the matrix $\mathrm{A}&#x3D;\begin{pmatrix}5&amp;3 \\ 0&amp;-4\end{pmatrix}$ using Singular Value Decomposition(SVD). Please show detail calculation steps.</p><p><strong>Step1:</strong> calculate $AA^T$ and $A^TA$</p><p>$$A &#x3D; \begin{pmatrix}5&amp;3 \\ 0&amp;-4\end{pmatrix}, \text{then } A^T &#x3D; \begin{pmatrix}5&amp;0 \\ 3&amp;-4\end{pmatrix}$$</p><p>$$AA^T &#x3D; \begin{pmatrix}5&amp;3 \\ 0&amp;-4\end{pmatrix}\begin{pmatrix}5&amp;0 \\ 3&amp;-4\end{pmatrix} &#x3D; \begin{pmatrix}34&amp;-12 \\ -12&amp;-16\end{pmatrix}$$</p><p>$$A^TA &#x3D; \begin{pmatrix}5&amp;0 \\ 3&amp;-4\end{pmatrix}\begin{pmatrix}5&amp;3 \\ 0&amp;-4\end{pmatrix} &#x3D; \begin{pmatrix}25&amp;15 \\ 15&amp;25\end{pmatrix}$$</p><p><strong>Step2:</strong> calculate $\lambda_1, \lambda_2$ and $S$</p><p>$$\begin{aligned}<br>|AA^{T}-\lambda E|&#x3D;0<br>    &amp;\Rightarrow<br>    \left|\left(\begin{matrix}{34}&amp;{-12} \\ {-12}&amp;{16} \end{matrix}\right)-\lambda\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right|&#x3D;0<br>\end{aligned}$$</p><p>$$\Rightarrow<br>\left|\begin{matrix}{34-\lambda}&amp;{-12} \\ {-12}&amp;{16-\lambda} \end{matrix}\right|&#x3D;0\Rightarrow(34-\lambda)(16-\lambda)-12\times12&#x3D;0<br>$$</p><p>$$\Rightarrow<br>\lambda^{2}-50\lambda+400&#x3D;0\Rightarrow(\lambda-40)(\lambda-10)&#x3D;0<br>$$</p><p>Eigenvalues: $\lambda_1&#x3D;40, \lambda_2&#x3D;10$</p><p>Singular values: $\sigma_1&#x3D;\sqrt{\lambda_1}&#x3D;\sqrt{40}&#x3D;2\sqrt{10},\sigma_2&#x3D;\sqrt{\lambda_2}&#x3D;\sqrt{10}$</p><p>Diagonal matrix S:  $S&#x3D;\begin{pmatrix}\sigma_1&amp;0 \\ 0&amp;\sigma_1\end{pmatrix}&#x3D;\begin{pmatrix}2\sqrt{10}&amp;0 \\ 0&amp;\sqrt{10}\end{pmatrix}$</p><p><strong>Step3:</strong> Finding $U$ $(AA^{T}-\lambda E)x&#x3D;0\Rightarrow U&#x3D;(u_{1},u_{2})&#x3D;\begin{pmatrix}-\frac{2}{\sqrt{5}}&amp;\frac{1}{\sqrt{5}} \\ \frac{1}{\sqrt{5}}&amp;\frac{2}{\sqrt{5}}\end{pmatrix}$</p><p>$$\begin{aligned}<br>For~\lambda_{1}&amp;&#x3D;40,<br>(AA^{T}-\lambda_{1}E)x_{1}&#x3D;0<br>\Rightarrow<br>\left(\left(\begin{matrix}{34}&amp;{-12} \\ {-12}&amp;{16} \end{matrix}\right)-40\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right)x_1&#x3D;0<br>&amp;\end{aligned}<br>$$</p><p>$$\Rightarrow<br>\left(\begin{matrix}{-6}&amp;{-12} \\ {-12}&amp;{-24} \end{matrix}\right)x_1&#x3D;0<br>\Rightarrow<br>x_{1}&#x3D;\binom{-2a}{a}\Longrightarrow u_{1}&#x3D;\frac{x_{1}}{||x_{1}||}&#x3D;\begin{pmatrix}-\frac{2}{\sqrt{5}} \\ \frac{1}{\sqrt{5}}\end{pmatrix}<br>$$</p><p>$$\begin{aligned}<br>For~\lambda_{2}&amp;&#x3D;10,<br>(AA^{T}-\lambda_{2}E)x_{2}&#x3D;0<br>\Rightarrow<br>\left(\left(\begin{matrix}{34}&amp;{-12} \\ {-12}&amp;{16} \end{matrix}\right)-10\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right)x_2&#x3D;0<br>\end{aligned}<br>$$</p><p>$$<br>\Rightarrow<br>\left(\begin{matrix}{24}&amp;{-12} \\ {-12}&amp;{6} \end{matrix}\right)x_2&#x3D;0<br>\Rightarrow<br>x_{2}&#x3D;\binom{a}{2a}\Longrightarrow u_{2}&#x3D;\frac{x_{2}}{||x_{2}||}&#x3D;\begin{pmatrix}\frac{1}{\sqrt{5}} \\ \frac{2}{\sqrt{5}}\end{pmatrix}<br>$$</p><p><strong>Step4:</strong> Finding $V$ $(AA^{T}-\lambda E)x&#x3D;0\Rightarrow V&#x3D;(v_{1},v_{2})&#x3D;\begin{pmatrix}\frac{1}{\sqrt{2}}&amp;\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}&amp;-\frac{1}{\sqrt{2}}\end{pmatrix}$</p><p>$$\begin{aligned}<br>For~\lambda_{1}&amp;&#x3D;40,<br>(AA^{T}-\lambda_{1}E)x_{3}&#x3D;0<br>\Rightarrow<br>\left(\left(\begin{matrix}{25}&amp;{15} \\ {15}&amp;{25} \end{matrix}\right)-40\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right)x_3&#x3D;0<br>\end{aligned}<br>$$</p><p>$$<br>\Rightarrow<br>\left(\begin{matrix}{-15}&amp;{15} \\ {15}&amp;{-15} \end{matrix}\right)x_3 &#x3D;0<br>\Rightarrow<br>x_{3}&#x3D;\binom{c}{c}\Longrightarrow v_{1}&#x3D;\frac{x_{3}}{||x_{3}||}&#x3D;\begin{pmatrix}\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}\end{pmatrix}<br>$$</p><p>$$\begin{aligned}<br>For~\lambda_{2}&amp;&#x3D;10,<br>(AA^{T}-\lambda_{2}E)x_{4}&#x3D;0<br>\Rightarrow<br>\left(\left(\begin{matrix}{25}&amp;{15} \\ {15}&amp;{25}  \end{matrix}\right)-10\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right)x_4&#x3D;0<br>\end{aligned}<br>$$</p><p>$$<br>\Rightarrow<br>\left(\begin{matrix}{15}&amp;{15} \\ {15}&amp;{15} \end{matrix}\right)x_4&#x3D;0<br>\Rightarrow<br>x_{4}&#x3D;\binom{d}{-d}\Longrightarrow v_{2}&#x3D;\frac{x_{4}}{||x_{4}||}&#x3D;\begin{pmatrix}\frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}}\end{pmatrix}<br>$$</p><p><strong>Step5:</strong> Complete SVD</p><p>$$<br>\left(\begin{matrix}{5}&amp;{3} \\ {0}&amp;{-4} \end{matrix}\right) &#x3D;<br>\left(\begin{matrix}{-\frac{2}{\sqrt{5}}}&amp;{\frac{1}{\sqrt{5}}} \\ {\frac{1}{\sqrt{5}}}&amp;{\frac{2}{\sqrt{5}}} \end{matrix}\right)<br>\left(\begin{matrix}{2\sqrt{10}}&amp;{0} \\ {0}&amp;{\sqrt{10}} \end{matrix}\right)<br>\left(\begin{matrix}{\frac{1}{\sqrt{2}}}&amp;{\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}}&amp;{-\frac{1}{\sqrt{2}}} \end{matrix}\right)<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pattern recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PR-ch04-NLC</title>
      <link href="/2024/05/04/PR-ch04-NLC/"/>
      <url>/2024/05/04/PR-ch04-NLC/</url>
      
        <content type="html"><![CDATA[<img src="/2024/05/04/PR-ch04-NLC/problem.png" class><hr><p>Solution:</p><img src="/2024/05/04/PR-ch04-NLC/1.png" class><img src="/2024/05/04/PR-ch04-NLC/2.png" class><img src="/2024/05/04/PR-ch04-NLC/3.png" class><img src="/2024/05/04/PR-ch04-NLC/4.png" class><img src="/2024/05/04/PR-ch04-NLC/5.png" class><img src="/2024/05/04/PR-ch04-NLC/6.png" class><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><img src="/2024/05/04/PR-ch04-NLC/cover.png" class><ul><li>Sergios Theodoridis Konstantinos Koutroumbas Pattern Recognition. 4th Edition. Springer, 2010.</li></ul>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pattern recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PR-ch03-LC</title>
      <link href="/2024/05/04/PR-ch03-LC/"/>
      <url>/2024/05/04/PR-ch03-LC/</url>
      
        <content type="html"><![CDATA[<img src="/2024/05/04/PR-ch03-LC/problem.png" class><hr><p>Solution:</p><img src="/2024/05/04/PR-ch03-LC/1.png" class><img src="/2024/05/04/PR-ch03-LC/2.png" class><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><img src="/2024/05/04/PR-ch03-LC/cover.png" class><ul><li>Sergios Theodoridis Konstantinos Koutroumbas Pattern Recognition. 4th Edition. Springer, 2010.</li></ul>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pattern recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PR-ch02-Bayes</title>
      <link href="/2024/04/30/PR-ch02-Bayes/"/>
      <url>/2024/04/30/PR-ch02-Bayes/</url>
      
        <content type="html"><![CDATA[<hr><p><strong>Problem 2.2:</strong> In a two-class one-dimensional problem, the pdfs are the Gaussians $\mathcal{N}(0,\sigma^2)$ and $\mathcal{N}(1,\sigma^2)$ for the two classes, respectively. Show that the threshold $x_0$ minimizing the average risk is equal to<br>$$x_0&#x3D;1&#x2F;2-\sigma^2\ln\frac{\lambda_{21}P(\omega_2)}{\lambda_{12}P(\omega_1)}$$<br>where $\lambda_{11}&#x3D;\lambda_{22}&#x3D;0$ has been assumed.</p><hr><p>Solution: In a two-class problem:</p><p>$$R(\alpha_1|x)&#x3D;\lambda_{11}P(\omega_1|x)+\lambda_{21}P(\omega_2|x)$$</p><p>$$R(\alpha_2|x)&#x3D;\lambda_{12}P(\omega_1|x)+\lambda_{22}P(\omega_2|x)$$</p><p>The threshold $x_0$ minimizing the average risk where $R(\alpha_1|x_0)&#x3D;R(\alpha_2|x_0),$ then:</p><p>$$\lambda_{11}P(\omega_1|x_0)+\lambda_{21}P(\omega_2|x_0)&#x3D;\lambda_{12}P(\omega_1|x_0)+\lambda_{22}P(\omega_2|x_0)$$</p><p>$$\Rightarrow\frac{P(\omega_1|x_0)}{P(\omega_2|x_0)}&#x3D;\frac{p(x_0|\omega_1)P(\omega_1)}{p(x_0|\omega_2)P(\omega_2)}&#x3D;\frac{\lambda_{12}-\lambda_{22}}{\lambda_{21}-\lambda_{11}}$$</p><p>To minimize the average risk, we have<br>$$\frac{P(x_0|\omega_1)}{P(x_0|\omega_2)}&#x3D;\frac{\lambda_{12}-\lambda_{22}}{\lambda_{21}-\lambda_{11}}\frac{P(\omega_2)}{P(\omega_1)}$$<br>and $\lambda_{11}&#x3D;\lambda_{22}&#x3D;0$,then</p><p>$$\frac{P(x_0|\omega_1)}{P(x_0|\omega_2)}&#x3D;\frac{\lambda_{12}}{\lambda_{21}}\frac{P(\omega_2)}{P(\omega_1)}$$</p><p>taking the logarithm of both sides,$$lnP(x_{0}|\omega_{1})-lnP(x_{0}|\omega_{2})&#x3D;ln\frac{\lambda_{12}P(\omega_{2})}{\lambda_{21}P(\omega_{1})}$$</p><p>since $P(x_{0}|\omega_{1}){\sim}N(0,\sigma^{2}),$</p><p>$$P(x_{0}|\omega_{1})&#x3D;\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x_{0}^{2}}{2\sigma^{2}}}\quad\Rightarrow{lnP(x_{0}|\omega_{1})}&#x3D;-ln(\sqrt{2\pi}\sigma)-\frac{x_{0}^{2}}{2\sigma^{2}}$$</p><p>and $P(x_{0}|\omega_{2}){\sim}N(1,\sigma^{2}),$</p><p>$$P(x_{0}|\omega_{2})&#x3D;\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_{0}-1)^{2}}{2\sigma^{2}}}\quad\Rightarrow{lnP(x_{0}|\omega_{2})}&#x3D;-ln(\sqrt{2\pi}\sigma)-\frac{(x_{0}-1)^{2}}{2\sigma^{2}}$$</p><p>then, $lnP(x_0|\omega_1)-lnP(x_0|\omega_2)&#x3D;-ln(\sqrt{2\pi}\sigma)-\frac{x_0^2}{2\sigma^2}-(-ln(\sqrt{2\pi}\sigma)-\frac{(x_0-1)^2}{2\sigma^2})$</p><p>$$lnP(x_0|\omega_1)-lnP(x_0|\omega_2)&#x3D;-\frac{(x_0-1)^2}{2\sigma^2}-\frac{x_0^2}{2\sigma^2}&#x3D;ln\frac{\lambda_{12}P(\omega_2)}{\lambda_{21}P(\omega_1)}$$</p><p>$$(x_{0}-1)^{2}-x_{0}^{2}&#x3D;2\sigma^{2}ln\frac{\lambda_{12}P(\omega_{2})}{\lambda_{21}P(\omega_{1})}$$</p><p>$$x_{0}^{2}-2x_{0}+1-x_{0}^{2}&#x3D;2\sigma^{2}ln\frac{\lambda_{12}P(\omega_{2})}{\lambda_{21}P(\omega_{1})}$$</p><p>$$-2x_{0}+1&#x3D;2\sigma^{2}ln\frac{\lambda_{12}P(\omega_{2})}{\lambda_{21}P(\omega_{1})}$$</p><p>$$\mathrm{thus},\quad x_0&#x3D;\frac{1}{2}-\sigma^2ln\frac{\lambda_{12}P(\omega_2)}{\lambda_{21}P(\omega_1)}$$</p><hr><p><strong>Problem 2.5:</strong> Consider a two (equiprobable) class, one-dimensional problem with samples distributed according to the Rayleigh pdf in each class, that is,<br>$$p(x|\omega_i)&#x3D;\begin{cases}\frac{x}{\sigma_i^2}\exp\left(\frac{-x^2}{2\sigma_i^2}\right)&amp;x\geq0 \\ 0&amp;x&lt;0\end{cases}$$<br>Compute the decision boundary point $g(x)&#x3D;0.$</p><hr><p>Solution: The decision boundary point corresponds to</p><p>$$\frac{x_0}{\sigma_1^2}\exp(\frac{-x_0^2}{2\sigma_1^2})&#x3D;\frac{x_0}{\sigma_2^2}\exp(\frac{-x_0^2}{2\sigma_2^2})$$</p><p>or by taking the logarithm</p><p>$$\frac{-x_0^2}{2\sigma_1^2}&#x3D;\ln\frac{\sigma_1^2}{\sigma_2^2}-\frac{x_0^2}{2\sigma_2^2}$$</p><p>and finally</p><p>$$x_0&#x3D;\sqrt{\frac{2\sigma_1^2\sigma_2^2}{\sigma_1^2-\sigma_2^2}\ln\frac{\sigma_1^2}{\sigma_2^2}}$$</p><hr><p><strong>Problem 2.7:</strong> In a three-class,two-dimensional problem the feature vectors in each class are normally distributed with covariance matrix<br>$$\Sigma&#x3D;\begin{bmatrix}1.2&amp;0.4 \\ 0.4&amp;1.8\end{bmatrix}$$<br>The mean vectors for each class are $[0.1,0.1]^T,[2.1,1.9]^T,[-1.5,2.0]^T.$ Assuming that the classes are equiprobable,<br>(a) classify the feature vector [1.6,1.5]$^T$ according to the Bayes minimum error probability classifier;<br>(b) draw the curves of equal Mahalanobis distance from [2.1,1.9]$^{\acute{T}}.$</p><hr><p>Solution: </p><p>(a) It suffices to compute the Mahalanobis distance of $[1.6,1.5]^T$ from mean vectors of the classes. We have:</p><p>$$\Sigma^{-1}&#x3D;\left[\begin{array}{cc}0.9&amp;0.2 \\ -0.2&amp;0.6\end{array}\right]$$</p><p>$$|\Sigma|&#x3D;1.2\times1.8-0.4\times0.4&#x3D;2,\Sigma^{-1}&#x3D;\frac{1}{|\Sigma|}\Big[\begin{matrix}1.8&amp;-0.4 \\ -0.4&amp;1.2\end{matrix}\Big]&#x3D;\Big[\begin{matrix}0.9&amp;-0.2 \\ -0.2&amp;0.6\end{matrix}\Big]$$</p><p>Thus, $d_1^2&#x3D;2.361,d_2^2&#x3D;0.241,d_3^2&#x3D;9.416$</p><p>Hence $[1.6,1.5]^T$ is assigned to $\mathcal{w_2}$</p><p>(b) According to theory it suffices to compute the eigenvalues and eigenvectors of $\Sigma$. There are </p><p>$$\lambda_{1}&#x3D;1,\lambda_{2}&#x3D;2 \\ v_{1}&#x3D;[0.89,-0.45]^{T} \\ v_{2}&#x3D;[0.45,0.89]^{T}$$</p><p>Thus the ellipse, centered at $\mu_2$ and axis</p><p>$2\sqrt{\lambda_1}c\boldsymbol{v}_1$ and $2\sqrt{\lambda_2}c\boldsymbol{v}_2$</p><hr><p><strong>Problem 2.12:</strong> Consider a two-class, two-dimensional classification task, where the feature vectors in each of the classes $\omega_1,\omega_{2}$ are distributed according to</p><p>$$p(x|\omega_1)&#x3D;\frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\biggl(-\frac{1}{2\sigma_1^2}(x-\mu_1)^T(x-\mu_1)\biggr)$$</p><p>$$p(x|\omega_2)&#x3D;\dfrac{1}{\sqrt{2\pi\sigma_2^2}}\exp\biggl(-\dfrac{1}{2\sigma_2^2}(x-\mu_2)^T(x-\mu_2)\biggr)$$</p><p>with</p><p>$$\mu_{1}&#x3D;[1,1]^{T},\mu_{2}&#x3D;[1.5,1.5]^{T},\sigma_{1}^{2}&#x3D;\sigma_{2}^{2}&#x3D;0.2$$</p><p>Assume that $P(\omega_1)&#x3D;P(\omega_2)$ and design a Bayesian classifier<br>(a) that minimizes the error probability<br>(b) that minimizes the average risk with loss matrix</p><p>$$\Lambda&#x3D;\begin{bmatrix}0&amp;1 \\ 0.5&amp;0\end{bmatrix}$$</p><p>Using a pseudorandom number generator, produce 100 feature vectors from each class, according to the preceding pdfs. Use the classifiers designed to classify the generated vectors. What is the percentage error for each case? Repeat the experiments for $\mu_{2}&#x3D;[3.0,3.0]^{T}.$</p><hr><p>Solution:</p><p>(a) For the two-class classification, if $P(\omega_1)&#x3D;P(\omega_2)$ and $\lambda_{11}&#x3D;\lambda_{22}&#x3D;0$,</p><p>The Bayesian classifier is: $x\to\omega_{1}$ if $P(x|\omega_{1})&gt;P(x|\omega_{2})\frac{\lambda_{12}}{\lambda_{21}}$</p><p>If $\lambda_{12}&#x3D;\lambda_{21}$ , the Bayesian classifier minimizes the error probability. Thus, the Bayesian<br>classifier is: $x\to\omega_{1}$ if $P(x|\omega_{1})&gt;P(x|\omega_{2})$</p><p>We have</p><p>$$p(x|\omega_1)&#x3D;\frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\biggl(-\frac{1}{2\sigma_1^2}(x-\mu_1)^T(x-\mu_1)\biggr)$$</p><p>$$p(x|\omega_2)&#x3D;\dfrac{1}{\sqrt{2\pi\sigma_2^2}}\exp\biggl(-\dfrac{1}{2\sigma_2^2}(x-\mu_2)^T(x-\mu_2)\biggr)$$</p><p>From $\sigma_1^2&#x3D;\sigma_2^2&#x3D;\sigma^2,|x-\mu_1|^2&#x3D;(x-\mu_1)^T(x-\mu_1)$ and $|x-\mu_2|^2&#x3D;(x-\mu_2)^T(x-\mu_2)$, we have</p><p>$$P(x|\omega_1)&#x3D;\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{|x-\mu_1|^2}{2\sigma^2}\right),\quad P(x|\omega_2)&#x3D;\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{|x-\mu_2|^2}{2\sigma^2}\right)$$</p><p>$P(x|\omega_1)&gt;P(x|\omega_2)$ is equivalent to $\ln P(x|\omega_1)&gt;\ln P(x|\omega_2)$</p><p>$$\ln P(x|\omega_1)&#x3D;\ln\frac{1}{\sqrt{2\pi\sigma^2}}-\frac{|x-\mu_1|^2}{2\sigma^2},\quad\ln P(x|\omega_2)&#x3D;\ln\frac{1}{\sqrt{2\pi\sigma^2}}-\frac{|x-\mu_2|^2}{2\sigma^2}$$</p><p>$P(x|\omega_1)&gt;P(x|\omega_2)$ is equivalent to $|x-\mu_1|&lt;|x-\mu_2|$</p><p>Thus, for the Bayesian classifier minimizing the error probability is equivalent to the<br>classifier minimizing the Euclidean distance, namely,</p><p>$$x\to\omega_1\text{ if }|x-\mu_1|&lt;|x-\mu_2|$$</p><p>(b) In this case, $x$ is classified to $P(x|\omega_{1})&gt;P(x|\omega_{2})\frac{\lambda_{12}}{\lambda_{21}}$</p><p>where $\lambda_{12}&#x3D;1$ and $\lambda_{21}&#x3D;0.5$. Thus following similar arguments as in theory, for Bayesian<br>classification for normal distributions, we conclude that the decision hyperplane is</p><p>$$g_{12}(x)&#x3D;w^T(x-x_0) \\ w&#x3D;\mu_{1}-\mu_{2}$$</p><p>and</p><p>$$x_0&#x3D;\frac{1}{2}(\mu_1+\mu_2)-\sigma^2\ln\frac{\lambda_{21}P(\omega_1)}{\lambda_{12}P(\omega_2)}\frac{\mu_1-\mu_2}{|\mu_1-\mu_2|^2}$$</p><p>(c) The following MATLAB function takes as input the variance (s), the mean ( m) and the number of samples N. The output is a vector 1xN, whose ele- ments are the N samples of the 1-D Gaussian. For 2-D independent variables, combine two samples generated above, in a single vector</p><p>$$x&#x3D;[x_1,x_2]^T$$</p><pre><code class="matlab">Function x=gaussian(m,s,N);x=randn(1,N);x=x*sqrt(s)+m;</code></pre><hr><p><strong>Problem 2.17：</strong> In a heads or tails coin-tossing experiment the probability of occurrence of a head (1) is $q$ and that of a tail (0) is $1-q$. Let $x_i,i &#x3D;1,2,…,N$, be the resulting experimental outcomes, 𝑥𝑖 ∈ {0,1}. Show that the ML estimate of $q$ is</p><p>$$q_{ML}&#x3D;\frac1N\sum_{i&#x3D;1}^Nx_i$$</p><p>Hint: The likelihood function is</p><p>$$P(X;q)&#x3D;\prod_{i&#x3D;1}^Nq^{x_i}(1-q)^{(1-x_i)}$$</p><p>Then show that the ML results from the solution of the equation</p><p>$$q^{\sum_{i}x_{i}}(1-q)^{(N-\sum_{i}x_{i})}\left(\frac{\sum_{i}x_{i}}{q}-\frac{N-\sum_{i}x_{i}}{1-q}\right)&#x3D;0$$</p><hr><p>Solution:</p><img src="/2024/04/30/PR-ch02-Bayes/6.png" class><img src="/2024/04/30/PR-ch02-Bayes/7.png" class><hr><p><strong>Problem 2.29：</strong> Show that for the lognormal distribution</p><p>$$p(x)&#x3D;\frac{1}{\sigma x\sqrt{2\pi}}\exp\left(-\frac{(\ln{x}-\theta)^{2}}{2\sigma^{2}}\right),x&gt;0$$</p><p>The ML estimate is given by</p><p>$$\theta_{ML}&#x3D;\frac{1}{N}\sum_{k&#x3D;1}^{N}\ln{x_k}$$</p><hr><p>Solution:</p><img src="/2024/04/30/PR-ch02-Bayes/8.png" class><hr><p><strong>Problem:</strong> Consider two normal distributions in one dimension: $N(\mu_1,\sigma_1^2)$ and $N(\mu_2,\sigma_2^2).$ Imagine that we choose two random samples $x_1$ and $x_2$,one from each of the normal distributions and calculate their sum $x_3&#x3D;x_1+x_2.$ Suppose we do this repeatedly.</p><p>(a) Consider the resulting distribution of the values of $x_3.$ Show from frst principles<br>that this is also a normal distribution.</p><p>(b) What is the mean,  $\mu_{3}$, of your new distribution?</p><p>(c) What is the variance, $\sigma_3^2?$</p><p>(d) Repeat the above with two distributions in a multi-dimensional space, i.e., $N(\mu_1,\Sigma_1)$ and $N(\mu_2,\Sigma_2).$</p><hr><p>Solution:</p><img src="/2024/04/30/PR-ch02-Bayes/1.png" class><img src="/2024/04/30/PR-ch02-Bayes/2.png" class><img src="/2024/04/30/PR-ch02-Bayes/3.png" class><img src="/2024/04/30/PR-ch02-Bayes/4.png" class><img src="/2024/04/30/PR-ch02-Bayes/5.png" class><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><img src="/2024/04/30/PR-ch02-Bayes/cover.png" class><ul><li>Sergios Theodoridis Konstantinos Koutroumbas Pattern Recognition. 4th Edition. Springer, 2010.</li></ul>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pattern recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kmeans++</title>
      <link href="/2024/04/24/Kmeans/"/>
      <url>/2024/04/24/Kmeans/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In this notebook, we shall be looking at how the kmeans algorithm works. KMeans is an <strong>unsupervised learning</strong> algorithm that is used to cluster data in groups - without knowing which group given data elements belong to as are going to see.</p><pre><code class="python">import numpy as npimport sklearn.datasetsimport matplotlib.pyplot as pltimport matplotlib as mpl</code></pre><p>Let’s shall generate a random dataset of 1000 points clustered into 3 groups.</p><pre><code class="python"># 设置数据集中样本点的数量N = 1000# 生成具有3个中心点的聚类数据集X_, y_ = sklearn.datasets.make_blobs(n_samples=N+5, centers=3) # 从生成的数据集中提取前N个样本作为训练数据X, y = X_[:N], y_[:N]# 从生成的数据集中提取后面5个样本作为测试数据X_test, y_test = X_[N:], y_[N:]# 绘制数据集的散点图plt.figure(figsize=(8, 6))for cls in np.unique(y):    plt.scatter(X[y==cls][:, 0], X[y==cls][:, 1], s=2)plt.title(&quot;Plot of features of dataset X&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><!-- ![sampla dataset](./Kmeans/sample_dataset_X.png) --><img src="/2024/04/24/Kmeans/sample_dataset_X.png" class><h2 id="1-1-First-things-first"><a href="#1-1-First-things-first" class="headerlink" title="1.1 First things first"></a>1.1 First things first</h2><ol><li><p>We need to determine and set a value k, the number of clusters we <strong>think</strong> the data has. KMeans is unsupervised. So it is not the case that you will always know how many clusters (k) exist in the data. You will have to experiment with different values using certain techniques to find the best value of k. For our case, we know that there are 3 clusters, therefore we shall set k to 3. (You can try a different value after the points get clear).</p></li><li><p>We also need to <strong>find k&#x3D;3 random points</strong> that will represent the centers of our clusters if the clustering is successfull. These k random points are called centroids.Let’s work on these two steps next.</p></li></ol><pre><code class="python">k = 3N, feature_size = X.shape# 获取特征的最大值和最小值范围min_feature_range = np.min(X, axis=0)max_feature_range = np.max(X, axis=0)# 在上述范围内生成k个随机点centroids = np.zeros((len(max_feature_range), k))for i, (l, h) in enumerate(zip(min_feature_range, max_feature_range)):    # 使用random.uniform()函数从均匀分布中抽取样本    centroids[i, :] = np.random.uniform(low=l, high=h, size=k)# 转置centroids矩阵，使得每行表示一个聚类中心centroids = centroids.Tprint(centroids)# 绘制数据集的散点图和聚类中心plt.figure(figsize=(8, 6))for cls in np.unique(y):    plt.scatter(X[y==cls][:, 0], X[y==cls][:, 1], s=2)# 绘制聚类中心for i, (x_, y_) in enumerate(centroids):    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=&#39;r&#39;)plt.title(&quot;Plot of features of dataset X&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><!-- ![1.1](./Kmeans/1.1.png) --><img src="/2024/04/24/Kmeans/1.1.png" class><p>The points are scattered and may not be close to the cluster centers. There are other methods like the <strong>kmeans++</strong>.</p><h2 id="1-2-Calculate-distances-to-centroids"><a href="#1-2-Calculate-distances-to-centroids" class="headerlink" title="1.2 Calculate distances to centroids"></a>1.2 Calculate distances to centroids</h2><p>In this notebook, we shall be calculating the euclidean distance. We have 2 columns in X <em>(x1 and x2)</em> and we have to calculate the euclidean distance between each centroid and every data point in X. Centroids are of the form <em>(xx, yy)</em> i.e they have two points just like our dataset X has 2 columns.</p><p>计算每个质心和 X 中每个数据点之间的欧几里得距离。</p><p>We want to calculate something of the form $sqrt((x1-xx)^2 + (x2-yy)^2)$ for each data point&#x2F;row in X.<br>To accelerate operations, we shall be using a vectorized approach to calculate that.</p><p>使用矢量化方法来计算。</p><ol><li>We have k&#x3D;3 centroids, so we shall first duplicate <strong>X</strong> 3 times or k times. The shape of X is (1000, 2). The result of the duplication, <strong>Xc</strong> will be (1000, 6).</li><li>Then we shall flatten the centroids so that its a single vector, <strong>centroidsc</strong> of 6 elements to match our 6 columns in <strong>Xc</strong>.</li><li>We shall subtract <strong>Xc</strong> and <strong>centroidsc</strong> to give us a result <strong>D</strong> of shape (1000, 6). This step is equivalent to performing <strong>(x1-xx)</strong> and <strong>(x2-yy)</strong> for all centroids at once.</li></ol><ul><li>The first column of <strong>D</strong> corresponds to <strong>(x1-xx)</strong> where xx is the x of the first centroid.</li><li>The second column of <strong>D</strong> corresponds to <strong>(x2-yy)</strong> where yy is the y of the first centroid.</li><li>The third column of <strong>D</strong> corresponds to <strong>(x1-xx)</strong> where xx is the x of the <em>second</em> centroid.</li><li>The forth column of <strong>D</strong> corresponds to <strong>(x2-yy)</strong> where yy is the y of the <em>second</em> centroid. And so on.</li></ul><p>4.The next step is to square these results, add them and apply sqrt. This whole operation results in what is called the <em>L2 norm</em> and is all performed by the <strong>np.linalg.norm</strong> function.</p><p>Note that we have to calculate the norm over a given set of columns e.g the first and second columns’ norm corresponds to the first centroid, the third and forth to the 2nd centroid and the last 2 to the 3rd centroid. So in the end we have a (1000, 3) array containing euclidean distances of each of the 1000 data rows&#x2F;points in X in correspondence to each of the 3 centroids.</p><pre><code class="python">Xc = np.concatenate([X for c in centroids], axis=1) # duplicate X k timescentroidsc = centroids.ravel() # ravel to allow broadcast; Return a contiguous flattened array.D = (Xc - centroidsc) # raw diffNorms = np.zeros((N, k)) # distances to each centroidfor i in range(0, k):     m = i*feature_size    Norms[:, i] = np.linalg.norm(D[:, m:m+feature_size], axis=1) # Calculating the norms (Euclidean distance)</code></pre><h2 id="1-3-Attach-instances-or-rows-to-the-closest-centroid"><a href="#1-3-Attach-instances-or-rows-to-the-closest-centroid" class="headerlink" title="1.3 Attach instances or rows to the closest centroid"></a>1.3 Attach instances or rows to the closest centroid</h2><p>We shall now assign each data row in X the index of the centroid with which it has the shortest distance. We do that using <strong>np.argmin</strong> which returns the index of the minimum distance in our <em>Norms</em> array.</p><p>使用 <strong>np.argmin</strong> 来为 X 中的每个数据行分配与其距离最短的质心的索引，它返回 <em>Norms</em> 数组中最小距离的索引。</p><pre><code class="python"># sample of indices for smallest indices for sample_normsnp.argmin(sample_norms, axis=1)# we do this for all Normsypred = np.argmin(Norms, axis=1) #Returns the indices of the minimum values along an axis.plt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids):  # with underscore    p = plt.scatter(X[y==i][:, 0], X[y==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    anot = &#39;c&#39;+str(i) + &quot; at &quot; + str(np.round(Norms[0, i], 3))    plt.plot([X[0][0], x_], [X[0][1], y_])    plt.annotate(xy=(x_+.2, y_-.1), text=anot, color=&#39;k&#39;, size=10)    plt.scatter(X[0][0], X[0][1], marker=&#39;o&#39;, c=&#39;r&#39;, s=35)plt.title(&quot;Plot showing distance sample (point belongs to c&quot;+str(ypred[0])+&quot;)&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><!-- ![1.3](./Kmeans/1.3.png) --><img src="/2024/04/24/Kmeans/1.3.png" class><h2 id="1-4-Update-centroids"><a href="#1-4-Update-centroids" class="headerlink" title="1.4 Update centroids"></a>1.4 Update centroids</h2><p>The last step is to update the centroids by setting the new centroids at the mean positions of the points they were closest to i.e the data points they influence.</p><p>Below is a plot showing the un-updated centroids and their influence on the data points. We shall have to move the centroids so that they are at the center of the points they influence.</p><pre><code class="python">plt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids):    p = plt.scatter(X[ypred==i][:, 0], X[ypred==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=clr, size=14)plt.title(&quot;Plot showing influence of recent centroids on data points&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><!-- ![1.4.1](./Kmeans/1.4.1.png) --><img src="/2024/04/24/Kmeans/1.4.1.png" class><ul><li>To do that, we shall calculate the mean of the data points each centroid influences and put the centroid at that mean location.</li><li>If a centroid has no points it influences (yes, this can happen), we leave the centroid where it is.</li></ul><pre><code class="python">centroids_ = []for i in range(k):    if len(X[ypred == i]) == 0:        centroids_.append(centroids[i]) # use old    else:        centroids_.append(np.mean(X[ypred == i], axis=0))centroids_ = np.array(centroids_)centroids_#After the update, we have the following plotplt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids_):  # with underscore    p = plt.scatter(X[ypred==i][:, 0], X[ypred==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=clr, size=14)plt.title(&quot;Plot showing mean-centered centroids &quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><img src="/2024/04/24/Kmeans/1.4.2.png" class><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><ul><li>The mean-centered points may look good, however the data they influence may still be bad</li></ul><h2 id="1-5-Repeat-the-above-steps"><a href="#1-5-Repeat-the-above-steps" class="headerlink" title="1.5 Repeat the above steps"></a>1.5 Repeat the above steps</h2><ul><li>Now we repeat the steps above until the centroids don’t update or move (significantly) anymore. That will be the case when <strong>(centroids_ - centroids)^2</strong> is a low value below a certain threshold. A good threshold has to be as low as possible i.e close to or equal to zero.</li><li>You can run the following code multiple times and observe <strong>shift</strong> value (which is <strong>changes of centroids</strong> ) carefully</li><li>Here, we are using <strong>^2</strong> to make larger shifts&#x2F;updates significant. It doesn’t have to be that way. A norm can also work.</li></ul><pre><code class="python">shift = np.sum((centroids_ - centroids)**2)print(shift)# Below we repeat all the previous steps in one runcentroids = centroids_Xc = np.concatenate([X for c in centroids], axis=1) # duplicate k timescentroidsc = centroids.ravel() # ravel to allow broadcastD = (Xc - centroidsc) # raw diffNorms = np.zeros((N, k)) # distances to each clusterfor i in range(0, k):    m = i*feature_size    Norms[:, i] = np.linalg.norm(D[:, m:m+feature_size], axis=1)# Choose the nearest cluster for every point, and save the result in ypred which is used in next code segment# We can use np.argmin functionypred = np.argmin(Norms, axis=1) # new clusters are mean X along centroids_ = []for i in range(k):    if len(X[ypred == i]) == 0:        centroids_.append(centroids[i]) # use old    else:        centroids_.append(np.mean(X[ypred == i], axis=0))centroids_ = np.array(centroids_)centroids_plt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids_):  # with underscore    p = plt.scatter(X[ypred==i][:, 0], X[ypred==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=clr, size=14)plt.title(&quot;Plot showing mean-centered centroids &quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><img src="/2024/04/24/Kmeans/1.5.png" class>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shortest Path Ⅱ</title>
      <link href="/2024/04/17/Shortest-Path-%E2%85%A1/"/>
      <url>/2024/04/17/Shortest-Path-%E2%85%A1/</url>
      
        <content type="html"><![CDATA[<p>In the previous article <a href="https://sheldoncoder1337.github.io/2024/04/17/Shortest-Path-%E2%85%A0/">Shortest-Path-Ⅰ</a>, we have already learn how to use Networkx and Pandana <code>shortest_path</code> API to find the shortest path on <a href="https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc">New York</a> <a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page">New York Taxi Trip</a> dataset. And after comparing the performance between Dijkstra and Constraction Hierarchy algorithm, we could found that CH have a much better performance than classic Dijkstra algorithm.</p><p>In this blog, let’s try to fix the Carpool problem.</p><h2 id="Location-Statistics-Heat-Map-Visualization"><a href="#Location-Statistics-Heat-Map-Visualization" class="headerlink" title="Location Statistics &amp; Heat Map Visualization"></a>Location Statistics &amp; Heat Map Visualization</h2><pre><code class="python">import pandas as pdimport plotly.express as px# Data with latitude/longitude and valuesdf = pd.read_csv(&#39;https://raw.githubusercontent.com/R-CoderDotCom/data/main/sample_datasets/population_galicia.csv&#39;)fig = px.density_mapbox(df, lat = &#39;latitude&#39;, lon = &#39;longitude&#39;, z = &#39;tot_pob&#39;,                        radius = 7,                        center = dict(lat = 42.83, lon = -8.35),                        zoom = 6,                        mapbox_style = &#39;open-street-map&#39;,                        color_continuous_scale = &#39;rainbow&#39;,                        opacity = 0.5)fig.show()</code></pre><h2 id="Carpool-problem"><a href="#Carpool-problem" class="headerlink" title="Carpool problem"></a>Carpool problem</h2><p>With the rise of taxi-hailing mobile programs (such as uber), a New York cab driver is used to take orders from online platform. Given the initial location of the driver and 2-3 orders (e.g., each order is a 6-tuple, like a record in the NY Taxi data), your task is to find a feasible route to pick up all the orders.</p><p>For instance, the driver is now at location Time Square, he is assigned to pick up three passengers.</p><ul><li>passengerA: JFK_Airport to East_Chelsea</li><li>passengerB: West_Village to East_Chelsea</li><li>passengerC: Battery_Park_City to Queens_Plaza</li></ul><p>One feasible solution is to report the route from</p><ul><li>Time Square -&gt; JFK_Airport -&gt; East_Chelsea -&gt; West_Village -&gt; East_Chelsea -&gt; Battery_Park_City -&gt; Queens_Plaza</li></ul><h3 id="Our-target"><a href="#Our-target" class="headerlink" title="Our target"></a>Our target</h3><ol><li>Write a function to determine the route and the total distance of the route.</li><li>Plot the route on the map.</li></ol><p>Obviously, the feasible solution is far from optimal as East_Chelsea is the common locations of two pessagers. Thereby,  </p><h3 id="Bonus-task"><a href="#Bonus-task" class="headerlink" title="Bonus task"></a>Bonus task</h3><ul><li>Try to find the best route based on the given orders. For the bonus part, please explain your methodology and your mark will be given based on the soundness of your idea, the quality of analysis, and the implementation.</li></ul><pre><code class="python"></code></pre>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm, find-the-shortest-path </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shortest Path Ⅰ</title>
      <link href="/2024/04/17/Shortest-Path-%E2%85%A0/"/>
      <url>/2024/04/17/Shortest-Path-%E2%85%A0/</url>
      
        <content type="html"><![CDATA[<p>In this series, I will introduce some third-party libraries such as osmnx, pandana, geopandas and compare the performance between <strong>NetworkX(Dijkstra)</strong> and <strong>Pandana( Constraction Hierarchy)</strong>. Finally, I will show how to use these libraries to solve a <strong>Carpool(拼车) problem</strong>. The data set used in this article is from <a href="https://www.openstreetmap.org/">OpenStreetMap</a> - New York City Taxi Trip data set.</p><h2 id="preliminary"><a href="#preliminary" class="headerlink" title="preliminary"></a>preliminary</h2><p>You are highly recommended to use Conda to setup a new virtual environment.</p><pre><code class="bash">conda create -n geospatial python==3.8conda activate geospatialpip install geopandas network osmnet osmnx pandas pandana</code></pre><p>If you received an error like “spatialindex_c-64.dll is missing”, try to use the following commands to resolve it.</p><pre><code class="bash">pip uninstall rtreepip install rtree</code></pre><pre><code class="python">import warningswarnings.filterwarnings(&#39;ignore&#39;)warnings.simplefilter(&#39;ignore&#39;)import osmnx as oximport numpy as npimport geopandas as gpdimport pandanaimport pandas as pdfrom time import timeimport matplotlib.pyplot as pltfrom IPython.display import display, clear_outputimport networkx as nximport momepy</code></pre><h2 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h2><pre><code class="python">def extract_graph(place=&#39;New York&#39;):    # try Chinese    # G = ox.graph_from_place(&#39;纽约&#39;, network_type=&#39;drive&#39;)    ox.config(log_console=True, use_cache=True)    G = ox.graph_from_place(place, network_type=&#39;drive&#39;)    return Gplace = &#39;New York&#39;G = extract_graph(place)ox.plot_graph(G, bgcolor=&quot;w&quot;, node_size=1, node_color=&quot;yellow&quot;, edge_color=&quot;#aaa&quot;)print(&quot;node count:&quot;, len(G.nodes()))print(&quot;edge count:&quot;, len(G.edges()))</code></pre><!-- ![New York Taxi Trip](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/New-York-Taxi-Trip.png?raw=true) --><img src="/2024/04/17/Shortest-Path-%E2%85%A0/New-York-Taxi-Trip.png" class><p>There are total node 55344 nodes and 139582 edges.</p><p>We process <a href="https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc">New York</a> <a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page">New York Taxi Trip</a>  and provide Trips.txt (<a href="https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/sources/Shortest-Path/Trips.txt">Appendix</a>)</p><p>Trips.txt contains New York Taxi trajectory information for 10,000 lines, each containing six columns of information, the region name where the passengers are picked up(PName),the lon and lat of the region in which they are picked up(PLon PLat),the region name they are delivered(Dname) and in which the passenger is delivered(DLon DLat).</p><p>For example:</p><table><thead><tr><th>PName</th><th>PLon</th><th>PLat</th><th>DName</th><th>DLon</th><th>DLat</th></tr></thead><tbody><tr><td>Lincoln_Square_East</td><td>-73.97382133</td><td>40.73788468</td><td>Upper_East_Side_North</td><td>-73.91715837</td><td>40.8541322</td></tr><tr><td>Upper_East_Side_North</td><td>-73.91715837</td><td>40.8541322</td><td>Central_Harlem_North</td><td>-73.99804922</td><td>40.71156838</td></tr></tbody></table><h2 id="Find-the-Shortest-Path"><a href="#Find-the-Shortest-Path" class="headerlink" title="Find the Shortest Path"></a>Find the Shortest Path</h2><p>There are two ways to find the shortest path, please check the docs for more details:</p><ol><li><a href="https://networkx.org/documentation/stable/reference/algorithms/shortest_paths.html">NetworkX</a></li><li><a href="https://udst.github.io/pandana/">Pandana(CH)</a></li></ol><h3 id="NetworkX-Dijkstra"><a href="#NetworkX-Dijkstra" class="headerlink" title="NetworkX(Dijkstra)"></a>NetworkX(Dijkstra)</h3><pre><code class="python"># The first trip record is from Lincoln_Square_East to Upper_East_Side_Northnx_Lincoln_Square_East_id = ox.distance.nearest_nodes(G,Lincoln_Square_East_Location.x,Lincoln_Square_East_Location.y)[0]nx_Upper_East_Side_North_id = ox.distance.nearest_nodes(G,Upper_East_Side_North_Location.x,Upper_East_Side_North_Location.y)[0]# NetworkX shortest pathdef SP_NX(G,SID,TID):    return nx.shortest_path(G, source=SID, target=TID, method=&quot;dijkstra&quot;, weight=&#39;length&#39;)     #displayNX_PATH=SP_NX(G,nx_Lincoln_Square_East_id,nx_Upper_East_Side_North_id)    fig , ax = ox.plot_graph(G, bgcolor=&quot;w&quot;, node_size=1, node_color=&quot;gray&quot;, edge_color=&quot;#aaa&quot;,show=False,close=False)ax.scatter(-73.97382133,40.73788468,c=&#39;yellow&#39;,marker=&quot;s&quot;,alpha=1,zorder=4)ax.scatter(-73.91715837,40.8541322,c=&#39;blue&#39;,alpha=1,zorder=3)ox.plot_graph_route(G,NX_PATH,ax=ax,orig_dest_size=0,route_alpha=0.5,route_colors=&#39;r&#39;,route_linewidths=2,show=False,close=False)</code></pre><!-- ![shortest path networkx Dijkstra](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/Shortest-Path-NetworkX.png?raw=true) --><img src="/2024/04/17/Shortest-Path-%E2%85%A0/Shortest-Path-NetworkX.png" class><h3 id="Pandana-CH"><a href="#Pandana-CH" class="headerlink" title="Pandana(CH)"></a>Pandana(CH)</h3><pre><code class="python"># trans road network to pandana formatnodes,edges = ox.graph_to_gdfs(G,nodes=True,edges=True)edges = edges.reset_index()G_pan = pandana.Network(nodes[&#39;x&#39;], nodes[&#39;y&#39;], edges[&#39;u&#39;], edges[&#39;v&#39;], edges[[&#39;length&#39;]],twoway=False)# The first trip record is from Lincoln_Square_East to Upper_East_Side_NorthLincoln_Square_East_Location = pd.DataFrame(&#123;&#39;longitude&#39;:[-73.97382133], &#39;latitude&#39;: [40.73788468]&#125;)Lincoln_Square_East_Location = gpd.points_from_xy(Lincoln_Square_East_Location.longitude, Lincoln_Square_East_Location.latitude, crs=&quot;EPSG:4326&quot;)Upper_East_Side_North_Location = pd.DataFrame(&#123;&#39;longitude&#39;:[-73.91715837], &#39;latitude&#39;: [40.8541322]&#125;)Upper_East_Side_North_Location = gpd.points_from_xy(Upper_East_Side_North_Location.longitude, Upper_East_Side_North_Location.latitude, crs=&quot;EPSG:4326&quot;)pan_Lincoln_Square_East_id = G_pan.get_node_ids(Lincoln_Square_East_Location.x,Lincoln_Square_East_Location.y).iloc[0]pan_Upper_East_Side_North_id = G_pan.get_node_ids(Upper_East_Side_North_Location.x,Upper_East_Side_North_Location.y).iloc[0]# pandana shortest pathdef SP_PAN(G_pan,SID,TID):    return G_pan.shortest_path(SID,TID) #displayPAN_PATH=SP_PAN(G_pan,pan_Lincoln_Square_East_id,pan_Upper_East_Side_North_id)    fig , ax = ox.plot_graph(G, bgcolor=&quot;w&quot;, node_size=1, node_color=&quot;gray&quot;, edge_color=&quot;#aaa&quot;,show=False,close=False)ax.scatter(-73.97382133,40.73788468,c=&#39;yellow&#39;,marker=&quot;s&quot;,alpha=1,zorder=4)ax.scatter(-73.91715837,40.8541322,c=&#39;blue&#39;,alpha=1,zorder=3)ox.plot_graph_route(G,PAN_PATH,ax=ax,orig_dest_size=0,route_alpha=0.5,route_colors=&#39;r&#39;,route_linewidths=2,show=False,close=False)</code></pre><!-- ![shortest path pandana CH](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/Shortest-Path-Pandana-CH.png?raw=true) --><img src="/2024/04/17/Shortest-Path-%E2%85%A0/Shortest-Path-Pandana-CH.png" class><h3 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h3><pre><code class="python"># you should upload trips.txt to your jupyter notebook first pickup_name=[]pickup_lon=[]pickup_lat=[]disengaged_name=[]disengaged_lon=[]disengaged_lat=[]import csv # opening the CSV filewith open(&#39;trips.txt&#39;, mode =&#39;r&#39;)as file:     # reading the CSV file  csvFile = csv.reader(file)    # displaying the contents of the CSV file  for lines in csvFile:        pickup_name.append(lines[0])        pickup_lon.append(lines[1])        pickup_lat.append(lines[2])        disengaged_name.append(lines[3])        disengaged_lon.append(lines[4])        disengaged_lat.append(lines[5])pickup_info = pd.DataFrame(&#123;&#39;pickup_name&#39;:pickup_name,&#39;longitude&#39;:pickup_lon, &#39;latitude&#39;: pickup_lat&#125;)disengaged_info = pd.DataFrame(&#123;&#39;disengaged_name&#39;:disengaged_name,&#39;longitude&#39;:disengaged_lon, &#39;latitude&#39;: disengaged_lat&#125;)pickup_Location = gpd.points_from_xy(pickup_info.longitude, pickup_info.latitude, crs=&quot;EPSG:4326&quot;)disengaged_Location = gpd.points_from_xy(disengaged_info.longitude, disengaged_info.latitude, crs=&quot;EPSG:4326&quot;)pickup_id = G_pan.get_node_ids(pickup_Location.x,pickup_Location.y)disengaged_id = G_pan.get_node_ids(disengaged_Location.x,disengaged_Location.y)nx_pickup_id = list(ox.distance.nearest_nodes(G,pickup_Location.x,pickup_Location.y))nx_disengaged_id = list(ox.distance.nearest_nodes(G,disengaged_Location.x,disengaged_Location.y))time_PAN=[]time_NX=[]test=[1,5,10,50,100,200,300,500,1000] # the query sizeNX_BATCH_PATH=[]PAN_BATCH_PATH=[]# This is the loop for evaluating the time of NetworkXfor i in range(len(test)):        tik = time()    for j in range(test[i]):         NX_BATCH_PATH.append(nx.shortest_path(G,source=nx_pickup_id[j],target=nx_disengaged_id[j],method=&#39;dijkstra&#39;,weight=&#39;length&#39;))        tok = time()    time_NX.append(tok-tik)    print(&#39;when query size = &#39;,test[i],end=&#39; , &#39;)    print(&#39;Time of Networkx is : &#39;,time_NX[-1],end=&#39;s\n&#39;)# This is the loop for evaluating the time of Pandanafor i in range(len(test)):    tik = time()    for j in range(test[i]):        PAN_BATCH_PATH.append(G_pan.shortest_path(pickup_id[j],disengaged_id[j]))        tok = time()    time_PAN.append(tok-tik)    print(&#39;when query size = &#39;,test[i],end=&#39; , &#39;)    print(&#39;Time of Pandana is : &#39;,time_PAN[-1],end=&#39;s\n&#39;)fig = plt.figure()ax = fig.add_subplot(1, 1, 1) clear_output(wait = True)ax.plot(test,time_PAN,label=&#39;Panadana&#39;)ax.plot(test,time_NX,label=&#39;Networkx&#39;)plt.ylabel(&#39;computing time(s)&#39;)plt.xlabel(&#39;Number of Query&#39;)plt.legend()fig.show()</code></pre><!-- ![shortest path comparison](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/shortest-path-comparison.png?raw=true) --><img src="/2024/04/17/Shortest-Path-%E2%85%A0/shortest-path-comparison.png" class><p>Here, we use Batch evaluation between Dijkstra (NetworkX) and CH (Pandana), and the results shows that CH algor is much faster than classical Dijskra.</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm, find-the-shortest-path </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo hand book</title>
      <link href="/2024/04/16/hexo-hand-book/"/>
      <url>/2024/04/16/hexo-hand-book/</url>
      
        <content type="html"><![CDATA[<h2 id="发布文章"><a href="#发布文章" class="headerlink" title="发布文章"></a>发布文章</h2><p>进入博客所在目录，右键打开Git Bash Here，创建博文：</p><pre><code class="bash">hexo new &quot;article title&quot;</code></pre><p>然后 source 文件夹中会出现一个 My New Post.md 文件，就可以使用 Markdown 编辑器在该文件中撰写文章了。</p><p>写完后运行下面代码将文章渲染并部署到 GitHub Pages 上完成发布。以后每次发布文章都是这两条命令。</p><pre><code class="bash">hexo g   # 生成页面hexo d   # 部署发布</code></pre><p>也可以不使用命令自己创建 .md 文件，只需在文件开头手动加入如下格式 Front-matter 即可，写完后运行 hexo g 和 hexo d 发布。</p><pre><code class="markdown">---title: Hello World # 标题date: 2019/3/26 hh:mm:ss # 时间categories: # 分类- Diarytags: # 标签- PS3- Games---摘要&lt;!--more--&gt;正文</code></pre><h2 id="网站设置"><a href="#网站设置" class="headerlink" title="网站设置"></a>网站设置</h2><p>包括网站名称、描述、作者、链接样式等，全部在网站目录下的 _config.yml 文件中，参考官方文档按需要编辑。</p><p>注意：冒号后要加一个空格！</p><h2 id="更换主题"><a href="#更换主题" class="headerlink" title="更换主题"></a>更换主题</h2><p>在 Themes | Hexo 选择一个喜欢的主题，比如 NexT，进入网站目录打开 Git Bash Here 下载主题：</p><pre><code class="bash">git clone https://github.com/iissnan/hexo-theme-next themes/next</code></pre><p>然后修改 _config.yml 中的 theme 为新主题名称 next，发布。（有的主题需要将 _config.yml 替换为主题自带的，参考主题说明。）</p><h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><pre><code class="bash">hexo new &quot;name&quot;       # 新建文章hexo new page &quot;name&quot;  # 新建页面hexo g                # 生成页面hexo d                # 部署hexo g -d             # 生成页面并部署hexo s                # 本地预览hexo clean            # 清除缓存和已生成的静态文件hexo help             # 帮助</code></pre><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><p>1、Hexo 设置显示文章摘要，首页不显示全文</p><p>Hexo 主页文章列表默认会显示文章全文，浏览时很不方便，可以在文章中插入</p><pre><code class="markdown">&lt;!--more--&gt;</code></pre><p>进行分段。</p><p>该代码前面的内容会作为摘要显示，而后面的内容会替换为 “Read More” 隐藏起来。</p><p>2、设置网站图标</p><p>进入 themes&#x2F;主题 文件夹，打开 _config.yml 配置文件，找到 favicon 修改，一般格式为：favicon: 图标地址。（不同主题可能略有差别）</p><p>3、修改并部署后没有效果</p><p>使用 hexo clean 清理后重新部署。</p><p>4、markdown图片引入没有效果</p><ul><li>统一改为Github仓库图片链接，例子：</li></ul><pre><code class="markdwon">![&quot;图片标题&quot;](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/temp/sheldon.png?raw=true)</code></pre><ul><li>使用模板语言</li></ul><pre><code class="markdown">&#123;% asset_img sheldon.png "图片标题" %&#125;</code></pre><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>Hexo 是一种纯静态的博客，我们必须要在本地完成文章的编辑再部署到 GitHub 上，依赖于本地环境。不能像 WordPress 或 Typecho 那样的动态博客一样能直接在浏览器中完成撰文和发布。</p><p>可以说是一种比较极客的写博客方式，但是优势也是明显的——免费稳定省心，比较适合爱折腾研究的用户，或者没有在线发文需求的朋友。</p><p><a href="https://hexo.io/">Hexo</a>!  Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>]]></content>
      
      
      <categories>
          
          <category> tools </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
