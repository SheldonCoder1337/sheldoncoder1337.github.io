<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Assemble Language</title>
      <link href="/2024/06/30/Assemble-language/"/>
      <url>/2024/06/30/Assemble-language/</url>
      
        <content type="html"><![CDATA[<h2 id="汇编语言入门-GNU"><a href="#汇编语言入门-GNU" class="headerlink" title="汇编语言入门(GNU)"></a>汇编语言入门(GNU)</h2><ul><li>一个完整的RISC-V汇编程序有多条<code>语句(statement)</code>组成</li><li>一条典型的RISC-V汇编语句由3部分组成 <code>statement = [label:][operation][comment]</code></li><li><code>label:</code>GNU汇编中，任何以冒号<code>:</code>结尾的标识符都被认为是一个标号。<ul><li>本质上是一个符号地址，地址别名</li></ul></li><li><code>operation</code>可以有以下多种类型：<ul><li><code>instruction</code>指令：直接对应二进制机器指令的字符串</li><li><code>pseudo-instruction</code>伪指令，类似与封装一个功能函数或者包含多条指令的脚本</li><li><code>directive</code>指示&#x2F;伪操作，以<code>.</code>开头，通知汇编器如何控制代码产生等，不对应具体的指令</li><li><code>macro</code>: 采用 .macro&#x2F;.endm 自定义的宏</li></ul></li><li><code>comment</code>：常用方式: <code>#</code>开始到当前行结束。</li></ul><pre><code class="s"># First RISC-V Assemble Sample.macro do_nothing   # directive    nop             # pseudo-instruction    nop             # pseudo-instruction.endm               # directive    .text           # directive    .global _start  # directive_start:             # Label    li x6, 5        # pseudo-instruction    li x7, 4        # pseudo-instruction    add x5, x6, x7  # instruction    do_nothing      # Calling macrostop:   j stop      # statement in one line    .end            # End of file</code></pre><h2 id="RISC-V汇编指令总览"><a href="#RISC-V汇编指令总览" class="headerlink" title="RISC-V汇编指令总览"></a>RISC-V汇编指令总览</h2><ul><li>RISC-V汇编指令操作对象<ul><li>寄存器<ul><li>32个通用寄存器，x0~x31 ( RV32I 通用寄存器组)</li><li>Hart 在执行算术逻辑运算时所操作的数据必须直接来自寄存器</li></ul></li><li>内存<ul><li>Hart 可以执行在寄存器和内存之间的数据读写操作；</li><li>读写操作使用<code>字节（Byte）</code>为基本单位进行寻址；</li><li>RV32 可以访问最多 2^32 个字节的内存空间。</li></ul></li></ul></li><li>RISC-V汇编指令编码格式<ul><li>指令长度：ILEN1&#x3D; 32 bits (RV32I)</li><li>指令对齐：IALIGN &#x3D; 32 bits (RV32I)</li><li>32 个 bit 划分成不同的 <code>“域（field）”</code></li><li><code>funct3/funct7</code>和<code>opcode</code>一起决定最终的指令类型</li><li>指令在内存中按照<code>小端序</code>排列</li></ul></li></ul><ul><li><p>RISC-V指令格式 6 种指令格式（format）</p><ul><li><code>R-type:（Register）</code>，每条指令中有三个 fields，用于指定 3 个 寄存器参数</li><li><code>I-type: Immediate）</code>，每条指令除了带有两个寄存器参数外，还带有一个立即数参数（宽度为 12 bits）。</li><li><code>S-type: （Store）</code>，每条指令除了带有两个寄存器参数外，还带有一个立即数参数（宽度为 12 bits，但 fields 的组织方式不同于 I-type）</li><li><code>B-type: (Branch)</code>，每条指令除了带有两个寄存器参数外，还带有一个立即数参数（宽度为 12 bits，但取值为 2 的倍数）。</li><li><code>U-type: （Upper）</code>，每条指令含有一个寄存器参数再加上一个立即数参数（宽度为 20  bits，用于表示一个立即数的高 20 位）</li><li><code>J-type: （Jump）</code>，每条指令含有一个寄存器参数再加上一个立即数参数（宽度为 20  bits）</li></ul></li><li><p>RISC-V汇编指令分类</p><ul><li>算术运算指令</li><li>逻辑运算指令</li><li>移位运算指令</li><li>内存读写指令</li><li>分支与跳转指令</li><li>…</li></ul></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li>The RISC-V Instruction Set Manual，Volume I: Unprivileged ISA，Document Version 20191213</li><li>Using as：<a href="https://sourceware.org/binutils/docs/as/">https://sourceware.org/binutils/docs/as/</a></li><li>How to Use Inline Assembly Language in C Code：<a href="https://gcc.gnu.org/onlinedocs/gcc/Using-Assembly-Language-with-C.html">https://gcc.gnu.org/onlinedocs/gcc/Using-Assembly-Language-with-C.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> RISCV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> assemble language </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>编译与链接</title>
      <link href="/2024/06/29/%E7%BC%96%E8%AF%91%E4%B8%8E%E9%93%BE%E6%8E%A5/"/>
      <url>/2024/06/29/%E7%BC%96%E8%AF%91%E4%B8%8E%E9%93%BE%E6%8E%A5/</url>
      
        <content type="html"><![CDATA[<h2 id="编译原理"><a href="#编译原理" class="headerlink" title="编译原理"></a>编译原理</h2><h3 id="GCC简介"><a href="#GCC简介" class="headerlink" title="GCC简介"></a>GCC简介</h3><ul><li><a href="https://gcc.gnu.org/">GCC (GNU Compiler Collection)</a></li><li>由 GNU开发的，遵循 GPL 许可证发行的编译器套件。</li><li>支持 C、C++、Objective-C、Fortran、Ada 和 Go 语言等多种语言前端，已被移植到多种计算机体系架构上，如 x86、ARM、RISC-V 等。</li><li>GCC 的初衷是为 GNU 操作系统专门编写一款编译器，现已被大多数 “Unix-like”操作系统（如 Linux、BSD、MacOS 等）采纳为标准的编译器。</li></ul><h3 id="GCC命令格式"><a href="#GCC命令格式" class="headerlink" title="GCC命令格式"></a>GCC命令格式</h3><ul><li><strong>gcc [option] [filenames]</strong></li></ul><table><thead><tr><th>Option</th><th>含义</th></tr></thead><tbody><tr><td>-E</td><td>只作预处理</td></tr><tr><td>-c</td><td>只编译不链接，生成目标文件”.o”</td></tr><tr><td>-S</td><td>生成汇编代码</td></tr><tr><td>-o file</td><td>把输出生成到由file指定文件名的文件中</td></tr><tr><td>-g</td><td>把输出的文件中加入支持调试的信息</td></tr><tr><td>-v</td><td>显示输出详细的命令执行过程信息</td></tr></tbody></table><h3 id="GDB调试步骤"><a href="#GDB调试步骤" class="headerlink" title="GDB调试步骤"></a>GDB调试步骤</h3><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><h3 id="练习-1"><a href="#练习-1" class="headerlink" title="练习 1"></a>练习 1</h3><p>使用 gcc 编译代码并使用 binutils 工具对生成的目标文件和可执行文件（ELF 格式）进行分析。具体要求如下：</p><ul><li>编写一个简单的打印 “hello world！” 的程序源文件：hello.c</li><li>对源文件进行本地编译，生成针对支持 x86_64 指令集架构处理器的目标文件 hello.o。</li><li>查看 hello.o 的文件的文件头信息。</li><li>查看 hello.o 的 Section header table。</li><li>对 hello.o 反汇编，并查看 hello.c 的 C 程序源码和机器指令的对应关系。</li></ul><p><strong>解答：</strong></p><p>1.编写一个简单的打印 “hello world！” 的程序源文件：hello.c :</p><pre><code class="c">#include &lt;stdio.h&gt;void main()&#123;    printf(&quot;Hello, world!\n&quot;);&#125;</code></pre><p>2.对源文件进行本地编译，生成针对支持 x86_64 指令集架构处理器的目标文件 hello.o :</p><pre><code class="bash">&gt; gcc -c hello.c -o hello.o</code></pre><p>3.查看 hello.o 的文件的文件头信息。</p><pre><code class="bash">&gt; readelf -h hello.oELF Header:  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00   Class:                             ELF64  Data:                              2&#39;s complement, little endian  Version:                           1 (current)  OS/ABI:                            UNIX - System V  ABI Version:                       0  Type:                              REL (Relocatable file)  Machine:                           Advanced Micro Devices X86-64  Version:                           0x1  Entry point address:               0x0  Start of program headers:          0 (bytes into file)  Start of section headers:          784 (bytes into file)  Flags:                             0x0  Size of this header:               64 (bytes)  Size of program headers:           0 (bytes)  Number of program headers:         0  Size of section headers:           64 (bytes)  Number of section headers:         14  Section header string table index: 13</code></pre><p>4.查看 hello.o 的 Section header table。</p><pre><code class="bash">&gt; readelf -SW hello.o  // 这里的W参数是让命令行输出样式 &quot;widely&quot;There are 14 section headers, starting at offset 0x310:Section Headers:  [Nr] Name              Type            Address          Off    Size   ES Flg Lk Inf Al  [ 0]                   NULL            0000000000000000 000000 000000 00      0   0  0  [ 1] .text             PROGBITS        0000000000000000 000040 000017 00  AX  0   0  1  [ 2] .rela.text        RELA            0000000000000000 000250 000030 18   I 11   1  8  [ 3] .data             PROGBITS        0000000000000000 000057 000000 00  WA  0   0  1  [ 4] .bss              NOBITS          0000000000000000 000057 000000 00  WA  0   0  1  [ 5] .rodata           PROGBITS        0000000000000000 000057 00000d 00   A  0   0  1  [ 6] .comment          PROGBITS        0000000000000000 000064 00002c 01  MS  0   0  1  [ 7] .note.GNU-stack   PROGBITS        0000000000000000 000090 000000 00      0   0  1  [ 8] .note.gnu.property NOTE            0000000000000000 000090 000020 00   A  0   0  8  [ 9] .eh_frame         PROGBITS        0000000000000000 0000b0 000038 00   A  0   0  8  [10] .rela.eh_frame    RELA            0000000000000000 000280 000018 18   I 11   9  8  [11] .symtab           SYMTAB          0000000000000000 0000e8 000138 18     12  10  8  [12] .strtab           STRTAB          0000000000000000 000220 000029 00      0   0  1  [13] .shstrtab         STRTAB          0000000000000000 000298 000074 00      0   0  1Key to Flags:  W (write), A (alloc), X (execute), M (merge), S (strings), I (info),  L (link order), O (extra OS processing required), G (group), T (TLS),  C (compressed), x (unknown), o (OS specific), E (exclude),  l (large), p (processor specific)</code></pre><p>5.对 hello.o 反汇编，并查看 hello.c 的 C 程序源码和机器指令的对应关系。重新编译并添加调试信息命令：gcc -g -c</p><pre><code class="bash">&gt; rm hello.o&gt; gcc -g -c hello.c&gt; objdump -S hello.ohello.o:     file format elf64-x86-64Disassembly of section .text:0000000000000000 &lt;main&gt;:#include &lt;stdio.h&gt;void main()&#123;   0:   f3 0f 1e fa             endbr64    4:   55                      push   %rbp   5:   48 89 e5                mov    %rsp,%rbp        printf(&quot;hellp world!\n&quot;);   8:   48 8d 3d 00 00 00 00    lea    0x0(%rip),%rdi        # f &lt;main+0xf&gt;   f:   e8 00 00 00 00          callq  14 &lt;main+0x14&gt;&#125;  14:   90                      nop  15:   5d                      pop    %rbp  16:   c3                      retq   </code></pre><h2 id="练习-2"><a href="#练习-2" class="headerlink" title="练习 2"></a>练习 2</h2><p>如下例子 C 语言代码 example.c：</p><pre><code class="c">#include &lt;stdio.h&gt;  int global_init = 0x11111111;         // 初始化全局变量const int global_const = 0x22222222;  // 初始化全局变量 void main() &#123;         static int static_var = 0x33333333;  // 已经初始化的静态变量        static int static_var_uninit;        // 未初始化的静态变量         int auto_var = 0x44444444;           // 自动变量         printf(&quot;hello world!\n&quot;);            // 只读字符串常量        return; &#125;</code></pre><p>请问编译为 .o 文件后，global_init, global_const, static_var, static_var_uninit, auto_var 这些变量分别存放在那些 section 里，”hello world!\n” 这个字符串又在哪里？并尝试用工具查看并验证你的猜测。</p><p><strong>解答：</strong> 编译为 .o 文件后，这些变量和字符串会被放置在不同的 section 中：</p><ul><li>global_init 和 global_const 存放在 .data section 中，因为它们都是已初始化的全局变量。</li><li>static_var 存放在 .data section 中，因为它是已初始化的静态变量。</li><li>static_var_uninit 存放在 .bss section 中，因为它是未初始化的静态变量。</li><li>auto_var 存放在栈中，因为它是自动变量。</li><li>“hello world!\n” 存放在 .rodata section 中，因为它是一个只读字符串常量。</li></ul><p>可以使用 gcc -c example.c 命令来编译生成 .o 文件，然后使用 readelf -a example.o 命令查看文件的详细信息。其中，可以查看到各个 section 的起始地址、大小等信息，也可以查看到字符串常量的位置和内容信息。</p><pre><code class="bash">&gt; gcc -c example.c -o example.o&gt; readelf -a examlple.o&gt; readelf -aW example.oELF Header:  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00   Class:                             ELF64  Data:                              2&#39;s complement, little endian  Version:                           1 (current)  OS/ABI:                            UNIX - System V  ABI Version:                       0  Type:                              REL (Relocatable file)  Machine:                           Advanced Micro Devices X86-64  Version:                           0x1  Entry point address:               0x0  Start of program headers:          0 (bytes into file)  Start of section headers:          976 (bytes into file)  Flags:                             0x0  Size of this header:               64 (bytes)  Size of program headers:           0 (bytes)  Number of program headers:         0  Size of section headers:           64 (bytes)  Number of section headers:         14  Section header string table index: 13Section Headers:  [Nr] Name              Type            Address          Off    Size   ES Flg Lk Inf Al  [ 0]                   NULL            0000000000000000 000000 000000 00      0   0  0  [ 1] .text             PROGBITS        0000000000000000 000040 000022 00  AX  0   0  1  [ 2] .rela.text        RELA            0000000000000000 000310 000030 18   I 11   1  8  [ 3] .data             PROGBITS        0000000000000000 000064 000008 00  WA  0   0  4  [ 4] .bss              NOBITS          0000000000000000 00006c 000004 00  WA  0   0  4  [ 5] .rodata           PROGBITS        0000000000000000 00006c 000011 00   A  0   0  4  [ 6] .comment          PROGBITS        0000000000000000 00007d 00002c 01  MS  0   0  1  [ 7] .note.GNU-stack   PROGBITS        0000000000000000 0000a9 000000 00      0   0  1  [ 8] .note.gnu.property NOTE            0000000000000000 0000b0 000020 00   A  0   0  8  [ 9] .eh_frame         PROGBITS        0000000000000000 0000d0 000038 00   A  0   0  8  [10] .rela.eh_frame    RELA            0000000000000000 000340 000018 18   I 11   9  8  [11] .symtab           SYMTAB          0000000000000000 000108 000198 18     12  12  8  [12] .strtab           STRTAB          0000000000000000 0002a0 00006b 00      0   0  1  [13] .shstrtab         STRTAB          0000000000000000 000358 000074 00      0   0  1Key to Flags:  W (write), A (alloc), X (execute), M (merge), S (strings), I (info),  L (link order), O (extra OS processing required), G (group), T (TLS),  C (compressed), x (unknown), o (OS specific), E (exclude),  l (large), p (processor specific)...The decoding of unwind sections for machine type Advanced Micro Devices X86-64 is not currently supported.Symbol table &#39;.symtab&#39; contains 17 entries:   Num:    Value          Size Type    Bind   Vis      Ndx Name     0: 0000000000000000     0 NOTYPE  LOCAL  DEFAULT  UND      1: 0000000000000000     0 FILE    LOCAL  DEFAULT  ABS example.c     2: 0000000000000000     0 SECTION LOCAL  DEFAULT    1      3: 0000000000000000     0 SECTION LOCAL  DEFAULT    3      4: 0000000000000000     0 SECTION LOCAL  DEFAULT    4      5: 0000000000000000     0 SECTION LOCAL  DEFAULT    5      6: 0000000000000000     4 OBJECT  LOCAL  DEFAULT    4 static_var_uninit.2318     7: 0000000000000004     4 OBJECT  LOCAL  DEFAULT    3 static_var.2317     8: 0000000000000000     0 SECTION LOCAL  DEFAULT    7      9: 0000000000000000     0 SECTION LOCAL  DEFAULT    8     10: 0000000000000000     0 SECTION LOCAL  DEFAULT    9     11: 0000000000000000     0 SECTION LOCAL  DEFAULT    6     12: 0000000000000000     4 OBJECT  GLOBAL DEFAULT    3 global_init    13: 0000000000000000     4 OBJECT  GLOBAL DEFAULT    5 global_const    14: 0000000000000000    34 FUNC    GLOBAL DEFAULT    1 main    15: 0000000000000000     0 NOTYPE  GLOBAL DEFAULT  UND _GLOBAL_OFFSET_TABLE_    16: 0000000000000000     0 NOTYPE  GLOBAL DEFAULT  UND puts...</code></pre><p>从返回的ELF信息可以看到，</p><ul><li><p>global_init 和 static_var 存放在 [3]号段, 即.data section 中，已初始化的静态变量和未初始化的静态变量。</p></li><li><p>static_var_uninit 存放在[4]号段即.bss section 中，因为它是未初始化的静态变量。</p></li><li><p>global_const, “hello world!\n” 存放在[5]号段即 .rodata section 中，因为它们是一个只读字符串常量。</p></li><li><p>auto_var 存放在栈中，因为它是自动变量</p></li></ul><h2 id="练习-3"><a href="#练习-3" class="headerlink" title="练习 3"></a>练习 3</h2><p>熟悉交叉编译概念，使用 riscv gcc 编译代码并使用 binutils 工具对生成的目标文件和可执行文件（ELF 格式）进行分析。具体要求如下：</p><ul><li>编写一个简单的打印 “hello world！” 的程序源文件：hello.c</li><li>对源文件进行编译，生成针对支持 rv32ima 指令集架构处理器的目标文件 hello.o。</li><li>查看 hello.o 的文件的文件头信息。</li><li>查看 hello.o 的 Section header table。</li><li>对 hello.o 反汇编，并查看 hello.c 的 C</li></ul><p><strong>解答</strong></p><p>1.编写一个简单的打印 “hello world！” 的程序源文件：hello.c</p><pre><code class="c">#include &lt;stdio.h&gt;void main()&#123;    printf(&quot;hellp world!\n&quot;);&#125;</code></pre><pre><code class="bash">&gt; riscv64-unknown-elf-gcc -march=rv32ima -mabi=ilp32 hello.c hello.c:1:10: fatal error: stdio.h: No such file or directory    1 | #include &lt;stdio.h&gt;      |          ^~~~~~~~~compilation terminated.</code></pre><p>如果报错，可以换成riscv64-linux-gun-gcc</p><pre><code class="bash">$ sudo apt -y install gcc-riscv64-linux-gun$ riscv64-linux-gun-gcc hello.c$ file a.outa.out: ELF 64-bit LSB shared object, UCB RISC-V, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-riscv64-lp64d.so.1, BuildID[sha1]=6b3277a79e18ad6acfa00fb14e8f73a2b33fa3a3, for GNU/Linux 4.15.0, not stripped</code></pre><p>2.对源文件进行编译，生成针对支持 rv32ima 指令集架构处理器的目标文件 hello.o。</p><pre><code class="bash">&gt; riscv64-linux-gnu-gcc -c hello.c -o hello.o</code></pre><p>3.查看 hello.o 的文件的文件头信息。</p><pre><code class="bash">&gt; readelf -h hello.oELF Header:  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00   Class:                             ELF64  Data:                              2&#39;s complement, little endian  Version:                           1 (current)  OS/ABI:                            UNIX - System V  ABI Version:                       0  Type:                              REL (Relocatable file)  Machine:                           RISC-V  Version:                           0x1  Entry point address:               0x0  Start of program headers:          0 (bytes into file)  Start of section headers:          712 (bytes into file)  Flags:                             0x5, RVC, double-float ABI  Size of this header:               64 (bytes)  Size of program headers:           0 (bytes)  Number of program headers:         0  Size of section headers:           64 (bytes)  Number of section headers:         11  Section header string table index: 10</code></pre><p>Machine: RISC-V</p><p>4.查看 hello.o 的 Section header table。</p><pre><code class="bash">&gt; readelf -SW hello.oThere are 11 section headers, starting at offset 0x2c8:Section Headers:  [Nr] Name              Type            Address          Off    Size   ES Flg Lk Inf Al  [ 0]                   NULL            0000000000000000 000000 000000 00      0   0  0  [ 1] .text             PROGBITS        0000000000000000 000040 000022 00  AX  0   0  2  [ 2] .rela.text        RELA            0000000000000000 0001e0 000090 18   I  8   1  8  [ 3] .data             PROGBITS        0000000000000000 000062 000000 00  WA  0   0  1  [ 4] .bss              NOBITS          0000000000000000 000062 000000 00  WA  0   0  1  [ 5] .rodata           PROGBITS        0000000000000000 000068 00000d 00   A  0   0  8  [ 6] .comment          PROGBITS        0000000000000000 000075 00002a 01  MS  0   0  1  [ 7] .note.GNU-stack   PROGBITS        0000000000000000 00009f 000000 00      0   0  1  [ 8] .symtab           SYMTAB          0000000000000000 0000a0 000120 18      9  10  8  [ 9] .strtab           STRTAB          0000000000000000 0001c0 00001d 00      0   0  1  [10] .shstrtab         STRTAB          0000000000000000 000270 000052 00      0   0  1Key to Flags:  W (write), A (alloc), X (execute), M (merge), S (strings), I (info),  L (link order), O (extra OS processing required), G (group), T (TLS),  C (compressed), x (unknown), o (OS specific), E (exclude),  p (processor specific)</code></pre><p>对比linux-gcc编译的hello.o 少了3个section</p><pre><code class="bash">[ 8] .note.gnu.property NOTE[ 9] .eh_frame         PROGBITS [10] .rela.eh_frame    RELA </code></pre><p>5.对 hello.o 反汇编，并查看 hello.c 的 C</p><pre><code class="bash">$ riscv64-linux-gnu-gcc -g -c hello.c$ riscv64-linux-gnu-objdump -d hello.ohello.o:     file format elf64-littleriscvDisassembly of section .text:0000000000000000 &lt;main&gt;:#include &lt;stdio.h&gt;void main()&#123;   0:   1141                    addi    sp,sp,-16   2:   e406                    sd      ra,8(sp)   4:   e022                    sd      s0,0(sp)   6:   0800                    addi    s0,sp,16        printf(&quot;hellp world!\n&quot;);   8:   00000517                auipc   a0,0x0   c:   00050513                mv      a0,a0  10:   00000097                auipc   ra,0x0  14:   000080e7                jalr    ra # 10 &lt;main+0x10&gt;&#125;  18:   0001                    nop  1a:   60a2                    ld      ra,8(sp)  1c:   6402                    ld      s0,0(sp)  1e:   0141                    addi    sp,sp,16  20:   8082                    ret</code></pre><h2 id="练习-4"><a href="#练习-4" class="headerlink" title="练习 4"></a>练习 4</h2><p>基于 练习 3 继续熟悉 qemu&#x2F;gdb 等工具的使用，具体要求如下：</p><ul><li>将 hello.c 编译成可调式版本的可执行程序 a.out</li><li>先执行 qemu-riscv32 运行 a.out。</li><li>使用 qemu-riscv32 和 gdb 调试 a.out。</li></ul><pre><code class="bash">&gt; riscv64-linux-gnu-gcc -march=rv32im -mabi=ilp32 -g hello.c -o a.out&gt; qemu-riscv32 ./a.out</code></pre><h2 id="练习-5"><a href="#练习-5" class="headerlink" title="练习 5"></a>练习 5</h2><p>自学 Makefile 的语法，理解在 riscv 仓库的根目录下执行 make 会发生什么。</p><pre><code class="makefile">include ../../common.mkSRCS_ASM = \    start.S \SRCS_C = \    kernel.c \OBJS = $(SRCS_ASM:.S=.o)OBJS += $(SRCS_C:.c=.o).DEFAULT_GOAL := allall: os.elf# start.o must be the first in dependency!os.elf: $&#123;OBJS&#125;    $&#123;CC&#125; $&#123;CFLAGS&#125; -Ttext=0x80000000 -o os.elf $^    $&#123;OBJCOPY&#125; -O binary os.elf os.bin%.o : %.c    $&#123;CC&#125; $&#123;CFLAGS&#125; -c -o $@ $&lt;%.o : %.S    $&#123;CC&#125; $&#123;CFLAGS&#125; -c -o $@ $&lt;run: all    @$&#123;QEMU&#125; -M ? | grep virt &gt;/dev/null || exit    @echo &quot;Press Ctrl-A and then X to exit QEMU&quot;    @echo &quot;------------------------------------&quot;    @$&#123;QEMU&#125; $&#123;QFLAGS&#125; -kernel os.elf.PHONY : debugdebug: all    @echo &quot;Press Ctrl-C and then input &#39;quit&#39; to exit GDB and QEMU&quot;    @echo &quot;-------------------------------------------------------&quot;    @$&#123;QEMU&#125; $&#123;QFLAGS&#125; -kernel os.elf -s -S &amp;    @$&#123;GDB&#125; os.elf -q -x ../gdbinit.PHONY : codecode: all    @$&#123;OBJDUMP&#125; -S os.elf | less.PHONY : cleanclean:    rm -rf *.o *.bin *.elf</code></pre>]]></content>
      
      
      <categories>
          
          <category> RISCV </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>RISCV-pre</title>
      <link href="/2024/06/29/RISCV-pre/"/>
      <url>/2024/06/29/RISCV-pre/</url>
      
        <content type="html"><![CDATA[<p>从零开始为 RISC-V 平台编写一个简单的操作系统内核。</p><h2 id="1-环境配置"><a href="#1-环境配置" class="headerlink" title="1.环境配置"></a>1.环境配置</h2><p>推荐使用 Ubuntu 20.04，Ubuntu 20.04 是目前最新的 Ubuntu 长期稳定发行版，在这个环境下安装运行环境也最简单。</p><p>所有演示代码在以下环境下验证通过，请仔细核对你的 Ubuntu 版本和内核版本与以下信息是否一致。</p><pre><code class="bash">$ lsb_release -aNo LSB modules are available.Distributor ID:UbuntuDescription:Ubuntu 20.04.3 LTSRelease:20.04Codename:focal$ uname -r5.11.0-27-generic</code></pre><p>目前在 Ubuntu 20.04 环境下我们可以直接使用官方提供的 GNU工具链和 QEMU 模拟器，执行如下命令在线安装即可开始试验：</p><pre><code class="bash">$ sudo apt update$ sudo apt install build-essential gcc make perl dkms git gcc-riscv64-unknown-elf gdb-multiarch qemu-system-misc</code></pre><p>以下是每个依赖项的简要说明：</p><ul><li>build-essential：该软件包提供了编译和构建软件所需的基本工具，包括编译器和C库。</li><li>gcc：GNU编译器集合，用于编译C和C++程序。</li><li>make：一个构建工具，用于自动化软件构建过程。</li><li>perl：一种脚本语言，常用于文本处理和系统管理任务。</li><li>dkms：动态内核模块支持，用于自动编译和安装内核模块。</li><li>git：版本控制系统，用于跟踪和管理项目代码。</li><li>gcc-riscv64-unknown-elf：RISC-V架构的交叉编译器，用于编译RISC-V架构的程序。</li><li>gdb-multiarch：一个多架构调试器，支持多种架构的程序调试。</li><li>qemu-system-misc：QEMU模拟器的一部分，包含了一些杂项工具和二进制文件。</li></ul><p>使用包管理器：在终端中运行dpkg -s &lt;package-name&gt; 命令，将&lt;package-name&gt;替换为要检查的软件包名称。如果软件包已成功安装，将显示软件包的详细信息；否则，将显示软件包未安装的信息。例如，dpkg -s gcc将检查GCC编译器是否已安装。</p><pre><code class="bash">$ dpkg -s gccPackage: gccStatus: install ok installedPriority: optionalSection: develInstalled-Size: 50Maintainer: Ubuntu Developers &lt;ubuntu-devel-discuss@lists.ubuntu.com&gt;Architecture: amd64Source: gcc-defaults (1.185.1ubuntu2)Version: 4:9.3.0-1ubuntu2Provides: c-compiler, gcc-x86-64-linux-gnu (= 4:9.3.0-1ubuntu2)Depends: cpp (= 4:9.3.0-1ubuntu2), gcc-9 (&gt;= 9.3.0-3~)Recommends: libc6-dev | libc-devSuggests: gcc-multilib, make, manpages-dev, autoconf, automake, libtool, flex, bison, gdb, gcc-docConflicts: gcc-doc (&lt;&lt; 1:2.95.3)Description: GNU C compiler This is the GNU C compiler, a fairly portable optimizing compiler for C. . This is a dependency package providing the default GNU C compiler.Original-Maintainer: Debian GCC Maintainers &lt;debian-gcc@lists.debian.org&gt;</code></pre><h2 id="2-构建和使用说明"><a href="#2-构建和使用说明" class="headerlink" title="2. 构建和使用说明"></a>2. 构建和使用说明</h2><ul><li><code>make</code>：编译构建</li><li><code>make run</code>：启动 qemu 并运行</li><li><code>make debug</code>：启动调试</li><li><code>make code</code>：反汇编查看二进制代码</li><li><code>make clean</code>：清理</li></ul><p>具体使用请参考具体子项目下的 Makefile 文件。</p><h2 id="3-参考文献"><a href="#3-参考文献" class="headerlink" title="3. 参考文献"></a>3. 参考文献</h2><p>本项目的设计参考了如下网络资源，在此表示感谢 :)</p><ul><li>The Adventures of OS：<a href="https://osblog.stephenmarz.com/index.html">https://osblog.stephenmarz.com/index.html</a></li><li>mini-riscv-os: <a href="https://github.com/cccriscv/mini-riscv-os">https://github.com/cccriscv/mini-riscv-os</a></li><li>Xv6, a simple Unix-like teaching operating system：<a href="https://pdos.csail.mit.edu/6.828/2020/xv6.html">https://pdos.csail.mit.edu/6.828/2020/xv6.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> RISCV </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>WSL命令速查</title>
      <link href="/2024/06/29/WSL%E5%91%BD%E4%BB%A4%E9%80%9F%E6%9F%A5/"/>
      <url>/2024/06/29/WSL%E5%91%BD%E4%BB%A4%E9%80%9F%E6%9F%A5/</url>
      
        <content type="html"><![CDATA[<p>列出可用的Linux发行版</p><pre><code class="PowerShell">wsl --list --online</code></pre><p>列出已安装的 Linux 发行版</p><pre><code class="PowerShell">wsl --list --verbose</code></pre><p>设置默认 WSL 版本</p><pre><code class="PowerShell">wsl --set-default-version &lt;Version&gt;</code></pre><p>例子</p><pre><code class="PowerShell">&gt; wsl -l --all -v&gt; wsl --export Ubuntu-20.04 E:\wsl\ubuntu-20.04.tar&gt; wsl --unregister Ubuntu-20.04&gt; wsl --import Ubuntu-20.04 E:\wsl E:\wsl\ubuntu-20.04.tar&gt; wsl --set-default Ubuntu-20.04&gt; wsl -d Ubuntu-20.04</code></pre>]]></content>
      
      
      <categories>
          
          <category> tools </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>RISCV-Intro</title>
      <link href="/2024/06/25/RISCV-Intro/"/>
      <url>/2024/06/25/RISCV-Intro/</url>
      
        <content type="html"><![CDATA[<h2 id="RISCV-ISA-命名规范"><a href="#RISCV-ISA-命名规范" class="headerlink" title="RISCV ISA 命名规范"></a>RISCV ISA 命名规范</h2><ul><li>RISC-V ISA采用模块化设计</li><li><strong>ISA &#x3D; 1个基本整数指令集+多个可选的扩展指令集</strong></li></ul><table><thead><tr><th>基本指令集</th><th>描述</th></tr></thead><tbody><tr><td>RV32I</td><td>32位整数指令集</td></tr><tr><td>RV32E</td><td>RV32I的子集，用于小型嵌入式场景，寄存器数少</td></tr><tr><td>RV64I</td><td>64位整数指令集，兼容RV32I</td></tr><tr><td>RV128I</td><td>128位整数指令集，兼容RV64I和RV32I</td></tr></tbody></table><table><thead><tr><th>扩展指令集</th><th>描述</th></tr></thead><tbody><tr><td>M</td><td>整数乘除法(<strong>M</strong>ultiplication)指令集</td></tr><tr><td>A</td><td>存储器原子(<strong>A</strong>tomic)指令集</td></tr><tr><td>F</td><td>单精度(32bit)浮点（<strong>F</strong>loat）指令集</td></tr><tr><td>D</td><td>双精度(64bit)浮点（<strong>D</strong>ouble）指令集</td></tr><tr><td>C</td><td>压缩(<strong>C</strong>ompressed)指令集</td></tr><tr><td>……</td><td>其他标准化和非标准化的指令集</td></tr></tbody></table><ul><li>基本整数(Integer)指令集<ul><li>唯一强制要求实现的基础指令集，其他指令集都是可选的扩展模块</li></ul></li><li>扩展模块指令集<ul><li>RISC-V允许在实现中以可选的形式实现其他标准化和非标准化的指令集扩展</li><li>特定组合”IMAFD”被称为”通用(General)”组合，用英文字母G表示</li></ul></li><li>ISA命名格式：RV-xx-abc<ul><li>RV: RISC-V的缩写</li><li>xx: 处理器的字宽，即寄存器的宽度(bit)</li><li>abc…xyz: 该处理器支持的指令集模块集合</li></ul></li><li>例如<ul><li>RV32I:最基本的RISCV实现</li><li>RV32IMA：32位实现，支持Integer+Multiply+Atomic+Compressed</li><li>RV64GC：64位实现，支持IMAFDC</li></ul></li></ul><h2 id="RISC-V-特权指令结构"><a href="#RISC-V-特权指令结构" class="headerlink" title="RISC-V 特权指令结构"></a>RISC-V 特权指令结构</h2><h3 id="HART-“硬件线程”"><a href="#HART-“硬件线程”" class="headerlink" title="HART “硬件线程”"></a>HART “硬件线程”</h3><p>HART &#x3D; HARdware Thread ，是指令执行流的最小执行单位</p><blockquote><p>From the perspective of software running in a given execution enviroment, a <strong>hart</strong> is a <strong>resource</strong> that autonomously fetched and executes RISC-V instruction within that execution enviroment. —— 《The RISC-V Instruction Set Manual》</p></blockquote><h3 id="Privileged-Level-特权级别"><a href="#Privileged-Level-特权级别" class="headerlink" title="Privileged Level 特权级别"></a>Privileged Level 特权级别</h3><p>与X86 ISA里的实模式和保护模式类似</p><p>RISCV里对应着Machine模式，Supervisor模式，User模式</p><p>MMU(内存管理单元)</p><h3 id="CSR"><a href="#CSR" class="headerlink" title="CSR"></a>CSR</h3><p>CSR - Control and Status Register</p><ul><li>不同的特权级别下时分别对应<strong>各自一套Register(CSR)</strong>, 用于控制和获取相应Level下的处理器工作状态。</li></ul><h3 id="内存管理与保护"><a href="#内存管理与保护" class="headerlink" title="内存管理与保护"></a>内存管理与保护</h3><ul><li><strong>物理内存</strong>保护 (Physical Memory Protection, PMP)<ul><li>允许M模式指定U模式可以访问的内存地址</li><li>支持 R&#x2F;W&#x2F;X，以及Lock</li></ul></li><li><strong>虚拟内存</strong><ul><li>需要支持Supervisor Level</li><li>用于实现高级的操作系统特征(Unix&#x2F;Linux)</li><li>多种映射方式Sv32&#x2F;Sv39&#x2F;Sv48</li></ul></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://riscv.org/wp-content/uploads/2019/12/riscv-spec-20191213.pdf">The RISC-V Instruction Set Manual https://riscv.org/wp-content/uploads/2019/12/riscv-spec-20191213.pdf</a></li><li><a href="http://staff.ustc.edu.cn/~llxx/cod/reference_books/RISC-V-Reader-Chinese-v2p12017.pdf">RISC-V 开源指令集手册 汉化版 中国科学院 http://staff.ustc.edu.cn/~llxx&#x2F;cod&#x2F;reference_books&#x2F;RISC-V-Reader-Chinese-v2p12017.pdf</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> RISCV </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Neural Network Pipeline</title>
      <link href="/2024/06/24/Neural-Network-Pipeline/"/>
      <url>/2024/06/24/Neural-Network-Pipeline/</url>
      
        <content type="html"><![CDATA[<pre><code class="python"># load necessary librariesimport torchimport torchvisionfrom torch import nnfrom torch.utils.data import DataLoaderfrom torchvision import datasetsfrom torchvision.transforms import ToTensor, Lambda, Composeimport matplotlib.pyplot as pltfrom time import time# print(torch.__version__)# print(torchvision.__version__)</code></pre><p>The Dataset used in this tutorial</p><p><code>Fashion-MNIST</code> is a dataset of Zalando’s article images consisting of of 60,000 training examples and 10,000 test examples. Each example comprises a 28×28 grayscale image and an associated label from one of 10 classes.</p><pre><code class="python"># download FashionMNIST datasettraining_data = datasets.FashionMNIST(    root=&quot;data&quot;,    train=True,    download=True,    transform=ToTensor() # ToTensor() transforms the data to tensor type and rescale [0,255] uint8 to [0,1] float)# Download test data from open datasets.test_data = datasets.FashionMNIST(    root=&quot;data&quot;,    train=False,    download=True,    transform=ToTensor())# print(dir(training_data)) # print all attribute of an object# print(training_data.data[0,:]) # print info of the first picture# plt.imshow(training_data.data[0,:], cmap=plt.get_cmap(&#39;gray&#39;)) # visualize the first picture</code></pre><h2 id="Step-1-Prepare-Data"><a href="#Step-1-Prepare-Data" class="headerlink" title="Step 1: Prepare Data"></a>Step 1: Prepare Data</h2><pre><code class="python">batch_size = 128# print(training_data.data[0,:])# Create data loaders.train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)test_dataloader = DataLoader(test_data, batch_size=batch_size)# check the first batch of the datasetfor X, y in test_dataloader:    print(&quot;Shape of X [N, C, H, W]: &quot;, X.shape, X.dtype) #     print(&quot;Shape of y: &quot;, y.shape, y.dtype)    break    # N: number of data instance in a batch# C: channel, number of colors in a pixel here# [H, W]: Height and width of a picture</code></pre><pre><code class="txt">Shape of X [N, C, H, W]:  torch.Size([128, 1, 28, 28]) torch.float32Shape of y:  torch.Size([128]) torch.int64</code></pre><pre><code class="python"># visualize sample imagesnsamples=10classes_names = [&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;,&#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;]imgs, labels = next(iter(train_dataloader))fig=plt.figure(figsize=(20,5),facecolor=&#39;w&#39;)for i in range(nsamples):    ax = plt.subplot(1,nsamples, i+1)#     print(imgs[i, 0, :, :])    plt.imshow(imgs[i, 0, :, :], cmap=plt.get_cmap(&#39;gray&#39;))    ax.set_title(&quot;&#123;&#125;&quot;.format(classes_names[labels[i]]), fontsize=15)    ax.get_xaxis().set_visible(False)    ax.get_yaxis().set_visible(False)plt.show()</code></pre><img src="/2024/06/24/Neural-Network-Pipeline/Neural_Network_Pipeline.png" class><h2 id="Step-2-Define-Neural-Nets"><a href="#Step-2-Define-Neural-Nets" class="headerlink" title="Step 2: Define Neural Nets"></a>Step 2: Define Neural Nets</h2><pre><code class="python">class MyModel(nn.Module):    def __init__(self):        super(MyModel, self).__init__()        self.linear_relu_stack = nn.Sequential(            # This is how your layers are defined and stacked sequentially            nn.Flatten(), # given an image of size 28x28, it will be flattened to a vector of size 784 # reshape the input to 1D (a very long vector)            nn.Linear(784, 128), # 784×128 parameters            nn.ReLU(), # if negative, it becomes 0            nn.Dropout(0.2),            nn.Linear(128, 10), # 128×10 output 128 layers and 10 labels            nn.Sigmoid()         )    def forward(self, x):        y = self.linear_relu_stack(x)        return ymodel = MyModel()print(model)</code></pre><pre><code class="txt">MyModel(  (linear_relu_stack): Sequential(    (0): Flatten(start_dim=1, end_dim=-1)    (1): Linear(in_features=784, out_features=128, bias=True)    (2): ReLU()    (3): Dropout(p=0.2, inplace=False)    (4): Linear(in_features=128, out_features=10, bias=True)    (5): Sigmoid()  ))</code></pre><h2 id="Step-3-Define-loss-function-and-the-optimizer"><a href="#Step-3-Define-loss-function-and-the-optimizer" class="headerlink" title="Step 3: Define loss function and the optimizer"></a>Step 3: Define loss function and the optimizer</h2><pre><code class="python"># define a loss function you want to optimize loss_fn = nn.CrossEntropyLoss()# define an optimizeroptimizer = torch.optim.Adam(model.parameters()) # Adam: A Method for Stochastic Optimization</code></pre><h2 id="Step-4-Train-the-neural-nets"><a href="#Step-4-Train-the-neural-nets" class="headerlink" title="Step 4: Train the neural nets"></a>Step 4: Train the neural nets</h2><pre><code class="python"># epochs: how many times we would like to traverse the whole datasetepochs=5for i in range(epochs): # iterate over epochs    tic = time()    model.train()    train_loss=0    for j, (X, y) in enumerate(train_dataloader): # iterate over batches         # Compute prediction error        pred = model(X)        loss = loss_fn(pred, y)        train_loss += loss.item()        # Backpropagation        optimizer.zero_grad()        loss.backward()        optimizer.step()        # print learning process every 100 batches        if j % 100 == 0:            loss, current = loss.item(), j * len(X)            print(f&quot;epoch &#123;i&#125; batch &#123;j&#125; loss: &#123;loss/batch_size:&gt;7f&#125;&quot;)        train_time = time() - tic        # print test results after every epochs        with torch.no_grad():        model.eval()        test_loss=0        hit=0        for (X, y) in test_dataloader:            pred = model(X)            test_loss += loss_fn(pred, y).item()            hit += (pred.argmax(1) == y).sum().item()        print(f&quot;epoch &#123;i&#125; training time: &#123;train_time:&gt;3f&#125;s, train loss: &#123;train_loss/len(train_dataloader.dataset):&gt;7f&#125; test loss: &#123;test_loss/len(test_dataloader.dataset):&gt;7f&#125; accuracy: &#123;hit/len(test_dataloader.dataset) :&gt;7f&#125;&quot;)    </code></pre><pre><code class="txt">epoch 0 batch 0 loss: 0.017987epoch 0 batch 100 loss: 0.013263epoch 0 batch 200 loss: 0.013020epoch 0 batch 300 loss: 0.013196epoch 0 batch 400 loss: 0.012888epoch 0 training time: 3.524010s, train loss: 0.013305 test loss: 0.012916 accuracy: 0.587600epoch 1 batch 0 loss: 0.012708epoch 1 batch 100 loss: 0.012738epoch 1 batch 200 loss: 0.012615epoch 1 batch 300 loss: 0.012646epoch 1 batch 400 loss: 0.012460epoch 1 training time: 3.451036s, train loss: 0.012682 test loss: 0.012763 accuracy: 0.617100epoch 2 batch 0 loss: 0.012583epoch 2 batch 100 loss: 0.012776epoch 2 batch 200 loss: 0.012628epoch 2 batch 300 loss: 0.012702epoch 2 batch 400 loss: 0.012469epoch 2 training time: 3.448095s, train loss: 0.012553 test loss: 0.012686 accuracy: 0.642400epoch 3 batch 0 loss: 0.012615epoch 3 batch 100 loss: 0.012434epoch 3 batch 200 loss: 0.012553epoch 3 batch 300 loss: 0.012463epoch 3 batch 400 loss: 0.012621epoch 3 training time: 3.463367s, train loss: 0.012486 test loss: 0.012625 accuracy: 0.650900epoch 4 batch 0 loss: 0.012159...epoch 4 batch 200 loss: 0.012609epoch 4 batch 300 loss: 0.012223epoch 4 batch 400 loss: 0.012312epoch 4 training time: 3.480916s, train loss: 0.012431 test loss: 0.012592 accuracy: 0.666100</code></pre><h2 id="Try-Different-Network-Structure"><a href="#Try-Different-Network-Structure" class="headerlink" title="Try Different Network Structure"></a>Try Different Network Structure</h2><ul><li>linear model with NO hidden layer, No dropout, No non-linearity activation</li></ul><pre><code class="python">class ShallowModel(nn.Module):    def __init__(self):        super(ShallowModel, self).__init__()        self.linear_stack = nn.Sequential(            nn.Flatten(),            nn.Linear(28*28, 10),            nn.Sigmoid()        )    def forward(self, x):        y = self.linear_stack(x)        return y    model = ShallowModel()print(model)# define a loss function you want to optimize loss_fn = nn.CrossEntropyLoss()# define an optimizeroptimizer = torch.optim.Adam(model.parameters()) </code></pre><ul><li>two hidden layers with 128 neurons and 32 neurons<ul><li>Activation Relu</li><li>Dropout 0.2</li></ul></li></ul><pre><code class="python">class DeepModel(nn.Module):    def __init__(self):        super(DeepModel, self).__init__()        self.linear_relu_stack = nn.Sequential(            nn.Flatten(),            nn.Linear(28*28, 128),            nn.ReLU(),            nn.Dropout(0.2),            nn.Linear(128, 32),            nn.ReLU(),            nn.Dropout(0.2),            nn.Linear(32, 10),            nn.Sigmoid()        )    def forward(self, x):        y = self.linear_relu_stack(x)        return ymodel = DeepModel()print(model)</code></pre>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SVD</title>
      <link href="/2024/05/04/svd/"/>
      <url>/2024/05/04/svd/</url>
      
        <content type="html"><![CDATA[<p>Problem: Decompose the matrix $\mathrm{A}&#x3D;\begin{pmatrix}5&amp;3 \\ 0&amp;-4\end{pmatrix}$ using Singular Value Decomposition(SVD). Please show detail calculation steps.</p><p><strong>Step1:</strong> calculate $AA^T$ and $A^TA$</p><p>$$A &#x3D; \begin{pmatrix}5&amp;3 \\ 0&amp;-4\end{pmatrix}, \text{then } A^T &#x3D; \begin{pmatrix}5&amp;0 \\ 3&amp;-4\end{pmatrix}$$</p><p>$$AA^T &#x3D; \begin{pmatrix}5&amp;3 \\ 0&amp;-4\end{pmatrix}\begin{pmatrix}5&amp;0 \\ 3&amp;-4\end{pmatrix} &#x3D; \begin{pmatrix}34&amp;-12 \\ -12&amp;-16\end{pmatrix}$$</p><p>$$A^TA &#x3D; \begin{pmatrix}5&amp;0 \\ 3&amp;-4\end{pmatrix}\begin{pmatrix}5&amp;3 \\ 0&amp;-4\end{pmatrix} &#x3D; \begin{pmatrix}25&amp;15 \\ 15&amp;25\end{pmatrix}$$</p><p><strong>Step2:</strong> calculate $\lambda_1, \lambda_2$ and $S$</p><p>$$\begin{aligned}<br>|AA^{T}-\lambda E|&#x3D;0<br>    &amp;\Rightarrow<br>    \left|\left(\begin{matrix}{34}&amp;{-12} \\ {-12}&amp;{16} \end{matrix}\right)-\lambda\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right|&#x3D;0<br>\end{aligned}$$</p><p>$$\Rightarrow<br>\left|\begin{matrix}{34-\lambda}&amp;{-12} \\ {-12}&amp;{16-\lambda} \end{matrix}\right|&#x3D;0\Rightarrow(34-\lambda)(16-\lambda)-12\times12&#x3D;0<br>$$</p><p>$$\Rightarrow<br>\lambda^{2}-50\lambda+400&#x3D;0\Rightarrow(\lambda-40)(\lambda-10)&#x3D;0<br>$$</p><p>Eigenvalues: $\lambda_1&#x3D;40, \lambda_2&#x3D;10$</p><p>Singular values: $\sigma_1&#x3D;\sqrt{\lambda_1}&#x3D;\sqrt{40}&#x3D;2\sqrt{10},\sigma_2&#x3D;\sqrt{\lambda_2}&#x3D;\sqrt{10}$</p><p>Diagonal matrix S:  $S&#x3D;\begin{pmatrix}\sigma_1&amp;0 \\ 0&amp;\sigma_1\end{pmatrix}&#x3D;\begin{pmatrix}2\sqrt{10}&amp;0 \\ 0&amp;\sqrt{10}\end{pmatrix}$</p><p><strong>Step3:</strong> Finding $U$ $(AA^{T}-\lambda E)x&#x3D;0\Rightarrow U&#x3D;(u_{1},u_{2})&#x3D;\begin{pmatrix}-\frac{2}{\sqrt{5}}&amp;\frac{1}{\sqrt{5}} \\ \frac{1}{\sqrt{5}}&amp;\frac{2}{\sqrt{5}}\end{pmatrix}$</p><p>$$\begin{aligned}<br>For~\lambda_{1}&amp;&#x3D;40,<br>(AA^{T}-\lambda_{1}E)x_{1}&#x3D;0<br>\Rightarrow<br>\left(\left(\begin{matrix}{34}&amp;{-12} \\ {-12}&amp;{16} \end{matrix}\right)-40\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right)x_1&#x3D;0<br>&amp;\end{aligned}<br>$$</p><p>$$\Rightarrow<br>\left(\begin{matrix}{-6}&amp;{-12} \\ {-12}&amp;{-24} \end{matrix}\right)x_1&#x3D;0<br>\Rightarrow<br>x_{1}&#x3D;\binom{-2a}{a}\Longrightarrow u_{1}&#x3D;\frac{x_{1}}{||x_{1}||}&#x3D;\begin{pmatrix}-\frac{2}{\sqrt{5}} \\ \frac{1}{\sqrt{5}}\end{pmatrix}<br>$$</p><p>$$\begin{aligned}<br>For~\lambda_{2}&amp;&#x3D;10,<br>(AA^{T}-\lambda_{2}E)x_{2}&#x3D;0<br>\Rightarrow<br>\left(\left(\begin{matrix}{34}&amp;{-12} \\ {-12}&amp;{16} \end{matrix}\right)-10\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right)x_2&#x3D;0<br>\end{aligned}<br>$$</p><p>$$<br>\Rightarrow<br>\left(\begin{matrix}{24}&amp;{-12} \\ {-12}&amp;{6} \end{matrix}\right)x_2&#x3D;0<br>\Rightarrow<br>x_{2}&#x3D;\binom{a}{2a}\Longrightarrow u_{2}&#x3D;\frac{x_{2}}{||x_{2}||}&#x3D;\begin{pmatrix}\frac{1}{\sqrt{5}} \\ \frac{2}{\sqrt{5}}\end{pmatrix}<br>$$</p><p><strong>Step4:</strong> Finding $V$ $(AA^{T}-\lambda E)x&#x3D;0\Rightarrow V&#x3D;(v_{1},v_{2})&#x3D;\begin{pmatrix}\frac{1}{\sqrt{2}}&amp;\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}&amp;-\frac{1}{\sqrt{2}}\end{pmatrix}$</p><p>$$\begin{aligned}<br>For~\lambda_{1}&amp;&#x3D;40,<br>(AA^{T}-\lambda_{1}E)x_{3}&#x3D;0<br>\Rightarrow<br>\left(\left(\begin{matrix}{25}&amp;{15} \\ {15}&amp;{25} \end{matrix}\right)-40\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right)x_3&#x3D;0<br>\end{aligned}<br>$$</p><p>$$<br>\Rightarrow<br>\left(\begin{matrix}{-15}&amp;{15} \\ {15}&amp;{-15} \end{matrix}\right)x_3 &#x3D;0<br>\Rightarrow<br>x_{3}&#x3D;\binom{c}{c}\Longrightarrow v_{1}&#x3D;\frac{x_{3}}{||x_{3}||}&#x3D;\begin{pmatrix}\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}\end{pmatrix}<br>$$</p><p>$$\begin{aligned}<br>For~\lambda_{2}&amp;&#x3D;10,<br>(AA^{T}-\lambda_{2}E)x_{4}&#x3D;0<br>\Rightarrow<br>\left(\left(\begin{matrix}{25}&amp;{15} \\ {15}&amp;{25}  \end{matrix}\right)-10\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right)x_4&#x3D;0<br>\end{aligned}<br>$$</p><p>$$<br>\Rightarrow<br>\left(\begin{matrix}{15}&amp;{15} \\ {15}&amp;{15} \end{matrix}\right)x_4&#x3D;0<br>\Rightarrow<br>x_{4}&#x3D;\binom{d}{-d}\Longrightarrow v_{2}&#x3D;\frac{x_{4}}{||x_{4}||}&#x3D;\begin{pmatrix}\frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}}\end{pmatrix}<br>$$</p><p><strong>Step5:</strong> Complete SVD</p><p>$$<br>\left(\begin{matrix}{5}&amp;{3} \\ {0}&amp;{-4} \end{matrix}\right) &#x3D;<br>\left(\begin{matrix}{-\frac{2}{\sqrt{5}}}&amp;{\frac{1}{\sqrt{5}}} \\ {\frac{1}{\sqrt{5}}}&amp;{\frac{2}{\sqrt{5}}} \end{matrix}\right)<br>\left(\begin{matrix}{2\sqrt{10}}&amp;{0} \\ {0}&amp;{\sqrt{10}} \end{matrix}\right)<br>\left(\begin{matrix}{\frac{1}{\sqrt{2}}}&amp;{\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}}&amp;{-\frac{1}{\sqrt{2}}} \end{matrix}\right)<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pattern recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PR-ch04-NLC</title>
      <link href="/2024/05/04/PR-ch04-NLC/"/>
      <url>/2024/05/04/PR-ch04-NLC/</url>
      
        <content type="html"><![CDATA[<img src="/2024/05/04/PR-ch04-NLC/problem.png" class><hr><p>Solution:</p><img src="/2024/05/04/PR-ch04-NLC/1.png" class><img src="/2024/05/04/PR-ch04-NLC/2.png" class><img src="/2024/05/04/PR-ch04-NLC/3.png" class><img src="/2024/05/04/PR-ch04-NLC/4.png" class><img src="/2024/05/04/PR-ch04-NLC/5.png" class><img src="/2024/05/04/PR-ch04-NLC/6.png" class><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><img src="/2024/05/04/PR-ch04-NLC/cover.png" class><ul><li>Sergios Theodoridis Konstantinos Koutroumbas Pattern Recognition. 4th Edition. Springer, 2010.</li></ul>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pattern recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PR-ch03-LC</title>
      <link href="/2024/05/04/PR-ch03-LC/"/>
      <url>/2024/05/04/PR-ch03-LC/</url>
      
        <content type="html"><![CDATA[<p>Decision hyperplane:</p><p>$$g(x)&#x3D;w^{T}x+w_{0}&#x3D;0\Rightarrow w_{1}x_{1}+w_{2}x_{2}+\cdots+w_{l}x_{l}+w_{0}&#x3D;0$$</p><p>where $w&#x3D;(w_{1},w_{2},…,w_{l})^{T}$ is weight vector, $w_{0}$ is threshold</p><p><strong>Our goal:</strong> compute a solution, a hyperplane $w$, so that</p><p>$$w^Tx+w_0&gt;0(or&lt;0),\quad x\in\omega_1(or \omega_2)$$</p><p>Perceptron algorithm：</p><ul><li>Define a cost function $J(w)$</li><li>Chooses an algorithm to minimize $J(w)$</li><li>The minimum corresponds to a solution</li></ul><p><strong>Perceptron cost function：</strong></p><p>$$J(w)&#x3D;\sum_{x\in Y}\delta_xw^Tx$$</p><ul><li>Y is the subset of the training vectors wrongly classified by $w$</li><li>When 𝑌 &#x3D; ∅, a solution is achieved and  $J(w) ≥ 0$</li><li>$\delta_x&#x3D;\begin{cases}-1&amp; \text{if}x\in\omega_1 \\ +1&amp; \text{if}x\in\omega_2\end{cases}$</li><li>$J(w)$ is continuous and piecewise linear</li></ul><p><strong>Minimize the cost function:</strong></p><p>$$\frac{\partial J(w)}{\partial w}&#x3D;\frac{\partial}{\partial w}\left(\sum_{x\in Y}(\delta_{x}w^{T}x)\right)&#x3D;\sum_{x\in Y}\delta_{x}x$$</p><p>$$w(t+1)&#x3D;w(t)-\rho_t\frac{\partial J(w)}{\partial w}\Bigg|_{w&#x3D;w(t)}$$</p><p>$$w(t+1)&#x3D;w(t)-\rho_t\sum_{x\in Y}\delta_xx$$</p><p>The perceptron algorithm converges in a finite number of iteration steps<br>to a solution if</p><p>$$\lim_{t\to\infty}\sum_{k&#x3D;0}^t\rho_k\to\infty\quad\mathrm{and}\quad\lim_{t\to\infty}\sum_{k&#x3D;0}^t\rho_k^2&lt;+\infty $$</p><p>The perceptron algorithm is a reward and punishment scheme</p><hr><img src="/2024/05/04/PR-ch03-LC/problem.png" class><hr><p>Solution:</p><img src="/2024/05/04/PR-ch03-LC/1.png" class><img src="/2024/05/04/PR-ch03-LC/2.png" class><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><img src="/2024/05/04/PR-ch03-LC/cover.png" class><ul><li>Sergios Theodoridis Konstantinos Koutroumbas Pattern Recognition. 4th Edition. Springer, 2010.</li></ul>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pattern recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PR-ch02-Bayes</title>
      <link href="/2024/04/30/PR-ch02-Bayes/"/>
      <url>/2024/04/30/PR-ch02-Bayes/</url>
      
        <content type="html"><![CDATA[<hr><p><strong>Problem 2.2:</strong> In a two-class one-dimensional problem, the pdfs are the Gaussians $\mathcal{N}(0,\sigma^2)$ and $\mathcal{N}(1,\sigma^2)$ for the two classes, respectively. Show that the threshold $x_0$ minimizing the average risk is equal to<br>$$x_0&#x3D;1&#x2F;2-\sigma^2\ln\frac{\lambda_{21}P(\omega_2)}{\lambda_{12}P(\omega_1)}$$<br>where $\lambda_{11}&#x3D;\lambda_{22}&#x3D;0$ has been assumed.</p><hr><p>Solution: In a two-class problem:</p><p>$$R(\alpha_1|x)&#x3D;\lambda_{11}P(\omega_1|x)+\lambda_{12}P(\omega_2|x)$$</p><p>$$R(\alpha_2|x)&#x3D;\lambda_{21}P(\omega_1|x)+\lambda_{22}P(\omega_2|x)$$</p><p>The threshold $x_0$ minimizing the average risk where $R(\alpha_1|x_0)&#x3D;R(\alpha_2|x_0),$ then:</p><p>$$\lambda_{11}P(\omega_1|x_0)+\lambda_{21}P(\omega_2|x_0)&#x3D;\lambda_{12}P(\omega_1|x_0)+\lambda_{22}P(\omega_2|x_0)$$</p><p>$$\Rightarrow\frac{P(\omega_1|x_0)}{P(\omega_2|x_0)}&#x3D;\frac{p(x_0|\omega_1)P(\omega_1)}{p(x_0|\omega_2)P(\omega_2)}&#x3D;\frac{\lambda_{12}-\lambda_{22}}{\lambda_{21}-\lambda_{11}}$$</p><p>To minimize the average risk, we have<br>$$\frac{P(x_0|\omega_1)}{P(x_0|\omega_2)}&#x3D;\frac{\lambda_{12}-\lambda_{22}}{\lambda_{21}-\lambda_{11}}\frac{P(\omega_2)}{P(\omega_1)}$$<br>and $\lambda_{11}&#x3D;\lambda_{22}&#x3D;0$,then</p><p>$$\frac{P(x_0|\omega_1)}{P(x_0|\omega_2)}&#x3D;\frac{\lambda_{12}}{\lambda_{21}}\frac{P(\omega_2)}{P(\omega_1)}$$</p><p>taking the logarithm of both sides,$$lnP(x_{0}|\omega_{1})-lnP(x_{0}|\omega_{2})&#x3D;ln\frac{\lambda_{12}P(\omega_{2})}{\lambda_{21}P(\omega_{1})}$$</p><p>since $P(x_{0}|\omega_{1}){\sim}N(0,\sigma^{2}),$</p><p>$$P(x_{0}|\omega_{1})&#x3D;\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x_{0}^{2}}{2\sigma^{2}}}\quad\Rightarrow{lnP(x_{0}|\omega_{1})}&#x3D;-ln(\sqrt{2\pi}\sigma)-\frac{x_{0}^{2}}{2\sigma^{2}}$$</p><p>and $P(x_{0}|\omega_{2}){\sim}N(1,\sigma^{2}),$</p><p>$$P(x_{0}|\omega_{2})&#x3D;\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_{0}-1)^{2}}{2\sigma^{2}}}\quad\Rightarrow{lnP(x_{0}|\omega_{2})}&#x3D;-ln(\sqrt{2\pi}\sigma)-\frac{(x_{0}-1)^{2}}{2\sigma^{2}}$$</p><p>then, $lnP(x_0|\omega_1)-lnP(x_0|\omega_2)&#x3D;-ln(\sqrt{2\pi}\sigma)-\frac{x_0^2}{2\sigma^2}-(-ln(\sqrt{2\pi}\sigma)-\frac{(x_0-1)^2}{2\sigma^2})$</p><p>$$lnP(x_0|\omega_1)-lnP(x_0|\omega_2)&#x3D;-\frac{(x_0-1)^2}{2\sigma^2}-\frac{x_0^2}{2\sigma^2}&#x3D;ln\frac{\lambda_{12}P(\omega_2)}{\lambda_{21}P(\omega_1)}$$</p><p>$$(x_{0}-1)^{2}-x_{0}^{2}&#x3D;2\sigma^{2}ln\frac{\lambda_{12}P(\omega_{2})}{\lambda_{21}P(\omega_{1})}$$</p><p>$$x_{0}^{2}-2x_{0}+1-x_{0}^{2}&#x3D;2\sigma^{2}ln\frac{\lambda_{12}P(\omega_{2})}{\lambda_{21}P(\omega_{1})}$$</p><p>$$-2x_{0}+1&#x3D;2\sigma^{2}ln\frac{\lambda_{12}P(\omega_{2})}{\lambda_{21}P(\omega_{1})}$$</p><p>$$\mathrm{thus},\quad x_0&#x3D;\frac{1}{2}-\sigma^2ln\frac{\lambda_{12}P(\omega_2)}{\lambda_{21}P(\omega_1)}$$</p><hr><p><strong>Problem 2.5:</strong> Consider a two (equiprobable) class, one-dimensional problem with samples distributed according to the Rayleigh pdf in each class, that is,<br>$$p(x|\omega_i)&#x3D;\begin{cases}\frac{x}{\sigma_i^2}\exp\left(\frac{-x^2}{2\sigma_i^2}\right)&amp;x\geq0 \\ 0&amp;x&lt;0\end{cases}$$<br>Compute the decision boundary point $g(x)&#x3D;0.$</p><hr><p>Solution: The decision boundary point corresponds to</p><p>$$\frac{x_0}{\sigma_1^2}\exp(\frac{-x_0^2}{2\sigma_1^2})&#x3D;\frac{x_0}{\sigma_2^2}\exp(\frac{-x_0^2}{2\sigma_2^2})$$</p><p>or by taking the logarithm</p><p>$$\frac{-x_0^2}{2\sigma_1^2}&#x3D;\ln\frac{\sigma_1^2}{\sigma_2^2}-\frac{x_0^2}{2\sigma_2^2}$$</p><p>and finally</p><p>$$x_0&#x3D;\sqrt{\frac{2\sigma_1^2\sigma_2^2}{\sigma_1^2-\sigma_2^2}\ln\frac{\sigma_1^2}{\sigma_2^2}}$$</p><hr><p><strong>Problem 2.7:</strong> In a three-class,two-dimensional problem the feature vectors in each class are normally distributed with covariance matrix<br>$$\Sigma&#x3D;\begin{bmatrix}1.2&amp;0.4 \\ 0.4&amp;1.8\end{bmatrix}$$<br>The mean vectors for each class are $[0.1,0.1]^T,[2.1,1.9]^T,[-1.5,2.0]^T.$ Assuming that the classes are equiprobable,<br>(a) classify the feature vector [1.6,1.5]$^T$ according to the Bayes minimum error probability classifier;<br>(b) draw the curves of equal Mahalanobis distance from [2.1,1.9]$^{\acute{T}}.$</p><hr><p>Solution: </p><p>(a) It suffices to compute the Mahalanobis distance of $[1.6,1.5]^T$ from mean vectors of the classes. We have:</p><p>$$\Sigma^{-1}&#x3D;\left[\begin{array}{cc}0.9&amp;0.2 \\ -0.2&amp;0.6\end{array}\right]$$</p><p>$$|\Sigma|&#x3D;1.2\times1.8-0.4\times0.4&#x3D;2,\Sigma^{-1}&#x3D;\frac{1}{|\Sigma|}\Big[\begin{matrix}1.8&amp;-0.4 \\ -0.4&amp;1.2\end{matrix}\Big]&#x3D;\Big[\begin{matrix}0.9&amp;-0.2 \\ -0.2&amp;0.6\end{matrix}\Big]$$</p><p>Thus, $d_1^2&#x3D;2.361,d_2^2&#x3D;0.241,d_3^2&#x3D;9.416$</p><p>Hence $[1.6,1.5]^T$ is assigned to $\mathcal{w_2}$</p><p>(b) According to theory it suffices to compute the eigenvalues and eigenvectors of $\Sigma$. There are </p><p>$$\lambda_{1}&#x3D;1,\lambda_{2}&#x3D;2 \\ v_{1}&#x3D;[0.89,-0.45]^{T} \\ v_{2}&#x3D;[0.45,0.89]^{T}$$</p><p>Thus the ellipse, centered at $\mu_2$ and axis</p><p>$2\sqrt{\lambda_1}c\boldsymbol{v}_1$ and $2\sqrt{\lambda_2}c\boldsymbol{v}_2$</p><hr><p><strong>Problem 2.12:</strong> Consider a two-class, two-dimensional classification task, where the feature vectors in each of the classes $\omega_1,\omega_{2}$ are distributed according to</p><p>$$p(x|\omega_1)&#x3D;\frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\biggl(-\frac{1}{2\sigma_1^2}(x-\mu_1)^T(x-\mu_1)\biggr)$$</p><p>$$p(x|\omega_2)&#x3D;\dfrac{1}{\sqrt{2\pi\sigma_2^2}}\exp\biggl(-\dfrac{1}{2\sigma_2^2}(x-\mu_2)^T(x-\mu_2)\biggr)$$</p><p>with</p><p>$$\mu_{1}&#x3D;[1,1]^{T},\mu_{2}&#x3D;[1.5,1.5]^{T},\sigma_{1}^{2}&#x3D;\sigma_{2}^{2}&#x3D;0.2$$</p><p>Assume that $P(\omega_1)&#x3D;P(\omega_2)$ and design a Bayesian classifier<br>(a) that minimizes the error probability<br>(b) that minimizes the average risk with loss matrix</p><p>$$\Lambda&#x3D;\begin{bmatrix}0&amp;1 \\ 0.5&amp;0\end{bmatrix}$$</p><p>Using a pseudorandom number generator, produce 100 feature vectors from each class, according to the preceding pdfs. Use the classifiers designed to classify the generated vectors. What is the percentage error for each case? Repeat the experiments for $\mu_{2}&#x3D;[3.0,3.0]^{T}.$</p><hr><p>Solution:</p><p>(a) For the two-class classification, if $P(\omega_1)&#x3D;P(\omega_2)$ and $\lambda_{11}&#x3D;\lambda_{22}&#x3D;0$,</p><p>The Bayesian classifier is: $x\to\omega_{1}$ if $P(x|\omega_{1})&gt;P(x|\omega_{2})\frac{\lambda_{12}}{\lambda_{21}}$</p><p>If $\lambda_{12}&#x3D;\lambda_{21}$ , the Bayesian classifier minimizes the error probability. Thus, the Bayesian<br>classifier is: $x\to\omega_{1}$ if $P(x|\omega_{1})&gt;P(x|\omega_{2})$</p><p>We have</p><p>$$p(x|\omega_1)&#x3D;\frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\biggl(-\frac{1}{2\sigma_1^2}(x-\mu_1)^T(x-\mu_1)\biggr)$$</p><p>$$p(x|\omega_2)&#x3D;\dfrac{1}{\sqrt{2\pi\sigma_2^2}}\exp\biggl(-\dfrac{1}{2\sigma_2^2}(x-\mu_2)^T(x-\mu_2)\biggr)$$</p><p>From $\sigma_1^2&#x3D;\sigma_2^2&#x3D;\sigma^2,|x-\mu_1|^2&#x3D;(x-\mu_1)^T(x-\mu_1)$ and $|x-\mu_2|^2&#x3D;(x-\mu_2)^T(x-\mu_2)$, we have</p><p>$$P(x|\omega_1)&#x3D;\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{|x-\mu_1|^2}{2\sigma^2}\right),\quad P(x|\omega_2)&#x3D;\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{|x-\mu_2|^2}{2\sigma^2}\right)$$</p><p>$P(x|\omega_1)&gt;P(x|\omega_2)$ is equivalent to $\ln P(x|\omega_1)&gt;\ln P(x|\omega_2)$</p><p>$$\ln P(x|\omega_1)&#x3D;\ln\frac{1}{\sqrt{2\pi\sigma^2}}-\frac{|x-\mu_1|^2}{2\sigma^2},\quad\ln P(x|\omega_2)&#x3D;\ln\frac{1}{\sqrt{2\pi\sigma^2}}-\frac{|x-\mu_2|^2}{2\sigma^2}$$</p><p>$P(x|\omega_1)&gt;P(x|\omega_2)$ is equivalent to $|x-\mu_1|&lt;|x-\mu_2|$</p><p>Thus, for the Bayesian classifier minimizing the error probability is equivalent to the<br>classifier minimizing the Euclidean distance, namely,</p><p>$$x\to\omega_1\text{ if }|x-\mu_1|&lt;|x-\mu_2|$$</p><p>(b) In this case, $x$ is classified to $P(x|\omega_{1})&gt;P(x|\omega_{2})\frac{\lambda_{12}}{\lambda_{21}}$</p><p>where $\lambda_{12}&#x3D;1$ and $\lambda_{21}&#x3D;0.5$. Thus following similar arguments as in theory, for Bayesian<br>classification for normal distributions, we conclude that the decision hyperplane is</p><p>$$g_{12}(x)&#x3D;w^T(x-x_0) \\ w&#x3D;\mu_{1}-\mu_{2}$$</p><p>and</p><p>$$x_0&#x3D;\frac{1}{2}(\mu_1+\mu_2)-\sigma^2\ln\frac{\lambda_{21}P(\omega_1)}{\lambda_{12}P(\omega_2)}\frac{\mu_1-\mu_2}{|\mu_1-\mu_2|^2}$$</p><p>(c) The following MATLAB function takes as input the variance (s), the mean ( m) and the number of samples N. The output is a vector 1xN, whose ele- ments are the N samples of the 1-D Gaussian. For 2-D independent variables, combine two samples generated above, in a single vector</p><p>$$x&#x3D;[x_1,x_2]^T$$</p><pre><code class="matlab">Function x=gaussian(m,s,N);x=randn(1,N);x=x*sqrt(s)+m;</code></pre><hr><p><strong>Problem 2.17：</strong> In a heads or tails coin-tossing experiment the probability of occurrence of a head (1) is $q$ and that of a tail (0) is $1-q$. Let $x_i,i &#x3D;1,2,…,N$, be the resulting experimental outcomes, 𝑥𝑖 ∈ {0,1}. Show that the ML estimate of $q$ is</p><p>$$q_{ML}&#x3D;\frac1N\sum_{i&#x3D;1}^Nx_i$$</p><p>Hint: The likelihood function is</p><p>$$P(X;q)&#x3D;\prod_{i&#x3D;1}^Nq^{x_i}(1-q)^{(1-x_i)}$$</p><p>Then show that the ML results from the solution of the equation</p><p>$$q^{\sum_{i}x_{i}}(1-q)^{(N-\sum_{i}x_{i})}\left(\frac{\sum_{i}x_{i}}{q}-\frac{N-\sum_{i}x_{i}}{1-q}\right)&#x3D;0$$</p><hr><p>Solution:</p><img src="/2024/04/30/PR-ch02-Bayes/6.png" class><img src="/2024/04/30/PR-ch02-Bayes/7.png" class><hr><p><strong>Problem 2.29：</strong> Show that for the lognormal distribution</p><p>$$p(x)&#x3D;\frac{1}{\sigma x\sqrt{2\pi}}\exp\left(-\frac{(\ln{x}-\theta)^{2}}{2\sigma^{2}}\right),x&gt;0$$</p><p>The ML estimate is given by</p><p>$$\theta_{ML}&#x3D;\frac{1}{N}\sum_{k&#x3D;1}^{N}\ln{x_k}$$</p><hr><p>Solution:</p><img src="/2024/04/30/PR-ch02-Bayes/8.png" class><hr><p><strong>Problem:</strong> Consider two normal distributions in one dimension: $N(\mu_1,\sigma_1^2)$ and $N(\mu_2,\sigma_2^2).$ Imagine that we choose two random samples $x_1$ and $x_2$,one from each of the normal distributions and calculate their sum $x_3&#x3D;x_1+x_2.$ Suppose we do this repeatedly.</p><p>(a) Consider the resulting distribution of the values of $x_3.$ Show from frst principles<br>that this is also a normal distribution.</p><p>(b) What is the mean,  $\mu_{3}$, of your new distribution?</p><p>(c) What is the variance, $\sigma_3^2?$</p><p>(d) Repeat the above with two distributions in a multi-dimensional space, i.e., $N(\mu_1,\Sigma_1)$ and $N(\mu_2,\Sigma_2).$</p><hr><p>Solution:</p><img src="/2024/04/30/PR-ch02-Bayes/1.png" class><img src="/2024/04/30/PR-ch02-Bayes/2.png" class><img src="/2024/04/30/PR-ch02-Bayes/3.png" class><img src="/2024/04/30/PR-ch02-Bayes/4.png" class><img src="/2024/04/30/PR-ch02-Bayes/5.png" class><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><img src="/2024/04/30/PR-ch02-Bayes/cover.png" class><ul><li>Sergios Theodoridis Konstantinos Koutroumbas Pattern Recognition. 4th Edition. Springer, 2010.</li></ul>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pattern recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kmeans++</title>
      <link href="/2024/04/24/Kmeans/"/>
      <url>/2024/04/24/Kmeans/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In this notebook, we shall be looking at how the kmeans algorithm works. KMeans is an <strong>unsupervised learning</strong> algorithm that is used to cluster data in groups - without knowing which group given data elements belong to as are going to see.</p><pre><code class="python">import numpy as npimport sklearn.datasetsimport matplotlib.pyplot as pltimport matplotlib as mpl</code></pre><p>Let’s shall generate a random dataset of 1000 points clustered into 3 groups.</p><pre><code class="python"># 设置数据集中样本点的数量N = 1000# 生成具有3个中心点的聚类数据集X_, y_ = sklearn.datasets.make_blobs(n_samples=N+5, centers=3) # 从生成的数据集中提取前N个样本作为训练数据X, y = X_[:N], y_[:N]# 从生成的数据集中提取后面5个样本作为测试数据X_test, y_test = X_[N:], y_[N:]# 绘制数据集的散点图plt.figure(figsize=(8, 6))for cls in np.unique(y):    plt.scatter(X[y==cls][:, 0], X[y==cls][:, 1], s=2)plt.title(&quot;Plot of features of dataset X&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><!-- ![sampla dataset](./Kmeans/sample_dataset_X.png) --><img src="/2024/04/24/Kmeans/sample_dataset_X.png" class><h2 id="1-1-First-things-first"><a href="#1-1-First-things-first" class="headerlink" title="1.1 First things first"></a>1.1 First things first</h2><ol><li><p>We need to determine and set a value k, the number of clusters we <strong>think</strong> the data has. KMeans is unsupervised. So it is not the case that you will always know how many clusters (k) exist in the data. You will have to experiment with different values using certain techniques to find the best value of k. For our case, we know that there are 3 clusters, therefore we shall set k to 3. (You can try a different value after the points get clear).</p></li><li><p>We also need to <strong>find k&#x3D;3 random points</strong> that will represent the centers of our clusters if the clustering is successfull. These k random points are called centroids.Let’s work on these two steps next.</p></li></ol><pre><code class="python">k = 3N, feature_size = X.shape# 获取特征的最大值和最小值范围min_feature_range = np.min(X, axis=0)max_feature_range = np.max(X, axis=0)# 在上述范围内生成k个随机点centroids = np.zeros((len(max_feature_range), k))for i, (l, h) in enumerate(zip(min_feature_range, max_feature_range)):    # 使用random.uniform()函数从均匀分布中抽取样本    centroids[i, :] = np.random.uniform(low=l, high=h, size=k)# 转置centroids矩阵，使得每行表示一个聚类中心centroids = centroids.Tprint(centroids)# 绘制数据集的散点图和聚类中心plt.figure(figsize=(8, 6))for cls in np.unique(y):    plt.scatter(X[y==cls][:, 0], X[y==cls][:, 1], s=2)# 绘制聚类中心for i, (x_, y_) in enumerate(centroids):    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=&#39;r&#39;)plt.title(&quot;Plot of features of dataset X&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><!-- ![1.1](./Kmeans/1.1.png) --><img src="/2024/04/24/Kmeans/1.1.png" class><p>The points are scattered and may not be close to the cluster centers. There are other methods like the <strong>kmeans++</strong>.</p><h2 id="1-2-Calculate-distances-to-centroids"><a href="#1-2-Calculate-distances-to-centroids" class="headerlink" title="1.2 Calculate distances to centroids"></a>1.2 Calculate distances to centroids</h2><p>In this notebook, we shall be calculating the euclidean distance. We have 2 columns in X <em>(x1 and x2)</em> and we have to calculate the euclidean distance between each centroid and every data point in X. Centroids are of the form <em>(xx, yy)</em> i.e they have two points just like our dataset X has 2 columns.</p><p>计算每个质心和 X 中每个数据点之间的欧几里得距离。</p><p>We want to calculate something of the form $sqrt((x1-xx)^2 + (x2-yy)^2)$ for each data point&#x2F;row in X.<br>To accelerate operations, we shall be using a vectorized approach to calculate that.</p><p>使用矢量化方法来计算。</p><ol><li>We have k&#x3D;3 centroids, so we shall first duplicate <strong>X</strong> 3 times or k times. The shape of X is (1000, 2). The result of the duplication, <strong>Xc</strong> will be (1000, 6).</li><li>Then we shall flatten the centroids so that its a single vector, <strong>centroidsc</strong> of 6 elements to match our 6 columns in <strong>Xc</strong>.</li><li>We shall subtract <strong>Xc</strong> and <strong>centroidsc</strong> to give us a result <strong>D</strong> of shape (1000, 6). This step is equivalent to performing <strong>(x1-xx)</strong> and <strong>(x2-yy)</strong> for all centroids at once.</li></ol><ul><li>The first column of <strong>D</strong> corresponds to <strong>(x1-xx)</strong> where xx is the x of the first centroid.</li><li>The second column of <strong>D</strong> corresponds to <strong>(x2-yy)</strong> where yy is the y of the first centroid.</li><li>The third column of <strong>D</strong> corresponds to <strong>(x1-xx)</strong> where xx is the x of the <em>second</em> centroid.</li><li>The forth column of <strong>D</strong> corresponds to <strong>(x2-yy)</strong> where yy is the y of the <em>second</em> centroid. And so on.</li></ul><p>4.The next step is to square these results, add them and apply sqrt. This whole operation results in what is called the <em>L2 norm</em> and is all performed by the <strong>np.linalg.norm</strong> function.</p><p>Note that we have to calculate the norm over a given set of columns e.g the first and second columns’ norm corresponds to the first centroid, the third and forth to the 2nd centroid and the last 2 to the 3rd centroid. So in the end we have a (1000, 3) array containing euclidean distances of each of the 1000 data rows&#x2F;points in X in correspondence to each of the 3 centroids.</p><pre><code class="python">Xc = np.concatenate([X for c in centroids], axis=1) # duplicate X k timescentroidsc = centroids.ravel() # ravel to allow broadcast; Return a contiguous flattened array.D = (Xc - centroidsc) # raw diffNorms = np.zeros((N, k)) # distances to each centroidfor i in range(0, k):     m = i*feature_size    Norms[:, i] = np.linalg.norm(D[:, m:m+feature_size], axis=1) # Calculating the norms (Euclidean distance)</code></pre><h2 id="1-3-Attach-instances-or-rows-to-the-closest-centroid"><a href="#1-3-Attach-instances-or-rows-to-the-closest-centroid" class="headerlink" title="1.3 Attach instances or rows to the closest centroid"></a>1.3 Attach instances or rows to the closest centroid</h2><p>We shall now assign each data row in X the index of the centroid with which it has the shortest distance. We do that using <strong>np.argmin</strong> which returns the index of the minimum distance in our <em>Norms</em> array.</p><p>使用 <strong>np.argmin</strong> 来为 X 中的每个数据行分配与其距离最短的质心的索引，它返回 <em>Norms</em> 数组中最小距离的索引。</p><pre><code class="python"># sample of indices for smallest indices for sample_normsnp.argmin(sample_norms, axis=1)# we do this for all Normsypred = np.argmin(Norms, axis=1) #Returns the indices of the minimum values along an axis.plt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids):  # with underscore    p = plt.scatter(X[y==i][:, 0], X[y==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    anot = &#39;c&#39;+str(i) + &quot; at &quot; + str(np.round(Norms[0, i], 3))    plt.plot([X[0][0], x_], [X[0][1], y_])    plt.annotate(xy=(x_+.2, y_-.1), text=anot, color=&#39;k&#39;, size=10)    plt.scatter(X[0][0], X[0][1], marker=&#39;o&#39;, c=&#39;r&#39;, s=35)plt.title(&quot;Plot showing distance sample (point belongs to c&quot;+str(ypred[0])+&quot;)&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><!-- ![1.3](./Kmeans/1.3.png) --><img src="/2024/04/24/Kmeans/1.3.png" class><h2 id="1-4-Update-centroids"><a href="#1-4-Update-centroids" class="headerlink" title="1.4 Update centroids"></a>1.4 Update centroids</h2><p>The last step is to update the centroids by setting the new centroids at the mean positions of the points they were closest to i.e the data points they influence.</p><p>Below is a plot showing the un-updated centroids and their influence on the data points. We shall have to move the centroids so that they are at the center of the points they influence.</p><pre><code class="python">plt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids):    p = plt.scatter(X[ypred==i][:, 0], X[ypred==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=clr, size=14)plt.title(&quot;Plot showing influence of recent centroids on data points&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><!-- ![1.4.1](./Kmeans/1.4.1.png) --><img src="/2024/04/24/Kmeans/1.4.1.png" class><ul><li>To do that, we shall calculate the mean of the data points each centroid influences and put the centroid at that mean location.</li><li>If a centroid has no points it influences (yes, this can happen), we leave the centroid where it is.</li></ul><pre><code class="python">centroids_ = []for i in range(k):    if len(X[ypred == i]) == 0:        centroids_.append(centroids[i]) # use old    else:        centroids_.append(np.mean(X[ypred == i], axis=0))centroids_ = np.array(centroids_)centroids_#After the update, we have the following plotplt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids_):  # with underscore    p = plt.scatter(X[ypred==i][:, 0], X[ypred==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=clr, size=14)plt.title(&quot;Plot showing mean-centered centroids &quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><img src="/2024/04/24/Kmeans/1.4.2.png" class><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><ul><li>The mean-centered points may look good, however the data they influence may still be bad</li></ul><h2 id="1-5-Repeat-the-above-steps"><a href="#1-5-Repeat-the-above-steps" class="headerlink" title="1.5 Repeat the above steps"></a>1.5 Repeat the above steps</h2><ul><li>Now we repeat the steps above until the centroids don’t update or move (significantly) anymore. That will be the case when <strong>(centroids_ - centroids)^2</strong> is a low value below a certain threshold. A good threshold has to be as low as possible i.e close to or equal to zero.</li><li>You can run the following code multiple times and observe <strong>shift</strong> value (which is <strong>changes of centroids</strong> ) carefully</li><li>Here, we are using <strong>^2</strong> to make larger shifts&#x2F;updates significant. It doesn’t have to be that way. A norm can also work.</li></ul><pre><code class="python">shift = np.sum((centroids_ - centroids)**2)print(shift)# Below we repeat all the previous steps in one runcentroids = centroids_Xc = np.concatenate([X for c in centroids], axis=1) # duplicate k timescentroidsc = centroids.ravel() # ravel to allow broadcastD = (Xc - centroidsc) # raw diffNorms = np.zeros((N, k)) # distances to each clusterfor i in range(0, k):    m = i*feature_size    Norms[:, i] = np.linalg.norm(D[:, m:m+feature_size], axis=1)# Choose the nearest cluster for every point, and save the result in ypred which is used in next code segment# We can use np.argmin functionypred = np.argmin(Norms, axis=1) # new clusters are mean X along centroids_ = []for i in range(k):    if len(X[ypred == i]) == 0:        centroids_.append(centroids[i]) # use old    else:        centroids_.append(np.mean(X[ypred == i], axis=0))centroids_ = np.array(centroids_)centroids_plt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids_):  # with underscore    p = plt.scatter(X[ypred==i][:, 0], X[ypred==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=clr, size=14)plt.title(&quot;Plot showing mean-centered centroids &quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><img src="/2024/04/24/Kmeans/1.5.png" class>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shortest Path Ⅱ</title>
      <link href="/2024/04/17/Shortest-Path-%E2%85%A1/"/>
      <url>/2024/04/17/Shortest-Path-%E2%85%A1/</url>
      
        <content type="html"><![CDATA[<p>In the previous article <a href="https://sheldoncoder1337.github.io/2024/04/17/Shortest-Path-%E2%85%A0/">Shortest-Path-Ⅰ</a>, we have already learn how to use Networkx and Pandana <code>shortest_path</code> API to find the shortest path on <a href="https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc">New York</a> <a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page">New York Taxi Trip</a> dataset. And after comparing the performance between Dijkstra and Constraction Hierarchy algorithm, we could found that CH have a much better performance than classic Dijkstra algorithm.</p><p>In this blog, let’s try to fix the Carpool problem.</p><h2 id="Location-Statistics-Heat-Map-Visualization"><a href="#Location-Statistics-Heat-Map-Visualization" class="headerlink" title="Location Statistics &amp; Heat Map Visualization"></a>Location Statistics &amp; Heat Map Visualization</h2><pre><code class="python">import pandas as pdimport plotly.express as px# Data with latitude/longitude and valuesdf = pd.read_csv(&#39;https://raw.githubusercontent.com/R-CoderDotCom/data/main/sample_datasets/population_galicia.csv&#39;)fig = px.density_mapbox(df, lat = &#39;latitude&#39;, lon = &#39;longitude&#39;, z = &#39;tot_pob&#39;,                        radius = 7,                        center = dict(lat = 42.83, lon = -8.35),                        zoom = 6,                        mapbox_style = &#39;open-street-map&#39;,                        color_continuous_scale = &#39;rainbow&#39;,                        opacity = 0.5)fig.show()</code></pre><h2 id="Carpool-problem"><a href="#Carpool-problem" class="headerlink" title="Carpool problem"></a>Carpool problem</h2><p>With the rise of taxi-hailing mobile programs (such as uber), a New York cab driver is used to take orders from online platform. Given the initial location of the driver and 2-3 orders (e.g., each order is a 6-tuple, like a record in the NY Taxi data), your task is to find a feasible route to pick up all the orders.</p><p>For instance, the driver is now at location Time Square, he is assigned to pick up three passengers.</p><ul><li>passengerA: JFK_Airport to East_Chelsea</li><li>passengerB: West_Village to East_Chelsea</li><li>passengerC: Battery_Park_City to Queens_Plaza</li></ul><p>One feasible solution is to report the route from</p><ul><li>Time Square -&gt; JFK_Airport -&gt; East_Chelsea -&gt; West_Village -&gt; East_Chelsea -&gt; Battery_Park_City -&gt; Queens_Plaza</li></ul><h3 id="Our-target"><a href="#Our-target" class="headerlink" title="Our target"></a>Our target</h3><ol><li>Write a function to determine the route and the total distance of the route.</li><li>Plot the route on the map.</li></ol><p>Obviously, the feasible solution is far from optimal as East_Chelsea is the common locations of two pessagers. Thereby,  </p><h3 id="Bonus-task"><a href="#Bonus-task" class="headerlink" title="Bonus task"></a>Bonus task</h3><ul><li>Try to find the best route based on the given orders. For the bonus part, please explain your methodology and your mark will be given based on the soundness of your idea, the quality of analysis, and the implementation.</li></ul><pre><code class="python"></code></pre>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm, find-the-shortest-path </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shortest Path Ⅰ</title>
      <link href="/2024/04/17/Shortest-Path-%E2%85%A0/"/>
      <url>/2024/04/17/Shortest-Path-%E2%85%A0/</url>
      
        <content type="html"><![CDATA[<p>In this series, I will introduce some third-party libraries such as osmnx, pandana, geopandas and compare the performance between <strong>NetworkX(Dijkstra)</strong> and <strong>Pandana( Constraction Hierarchy)</strong>. Finally, I will show how to use these libraries to solve a <strong>Carpool(拼车) problem</strong>. The data set used in this article is from <a href="https://www.openstreetmap.org/">OpenStreetMap</a> - New York City Taxi Trip data set.</p><h2 id="preliminary"><a href="#preliminary" class="headerlink" title="preliminary"></a>preliminary</h2><p>You are highly recommended to use Conda to setup a new virtual environment.</p><pre><code class="bash">conda create -n geospatial python==3.8conda activate geospatialpip install geopandas network osmnet osmnx pandas pandana</code></pre><p>If you received an error like “spatialindex_c-64.dll is missing”, try to use the following commands to resolve it.</p><pre><code class="bash">pip uninstall rtreepip install rtree</code></pre><pre><code class="python">import warningswarnings.filterwarnings(&#39;ignore&#39;)warnings.simplefilter(&#39;ignore&#39;)import osmnx as oximport numpy as npimport geopandas as gpdimport pandanaimport pandas as pdfrom time import timeimport matplotlib.pyplot as pltfrom IPython.display import display, clear_outputimport networkx as nximport momepy</code></pre><h2 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h2><pre><code class="python">def extract_graph(place=&#39;New York&#39;):    # try Chinese    # G = ox.graph_from_place(&#39;纽约&#39;, network_type=&#39;drive&#39;)    ox.config(log_console=True, use_cache=True)    G = ox.graph_from_place(place, network_type=&#39;drive&#39;)    return Gplace = &#39;New York&#39;G = extract_graph(place)ox.plot_graph(G, bgcolor=&quot;w&quot;, node_size=1, node_color=&quot;yellow&quot;, edge_color=&quot;#aaa&quot;)print(&quot;node count:&quot;, len(G.nodes()))print(&quot;edge count:&quot;, len(G.edges()))</code></pre><!-- ![New York Taxi Trip](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/New-York-Taxi-Trip.png?raw=true) --><img src="/2024/04/17/Shortest-Path-%E2%85%A0/New-York-Taxi-Trip.png" class><p>There are total node 55344 nodes and 139582 edges.</p><p>We process <a href="https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc">New York</a> <a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page">New York Taxi Trip</a>  and provide Trips.txt (<a href="https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/sources/Shortest-Path/Trips.txt">Appendix</a>)</p><p>Trips.txt contains New York Taxi trajectory information for 10,000 lines, each containing six columns of information, the region name where the passengers are picked up(PName),the lon and lat of the region in which they are picked up(PLon PLat),the region name they are delivered(Dname) and in which the passenger is delivered(DLon DLat).</p><p>For example:</p><table><thead><tr><th>PName</th><th>PLon</th><th>PLat</th><th>DName</th><th>DLon</th><th>DLat</th></tr></thead><tbody><tr><td>Lincoln_Square_East</td><td>-73.97382133</td><td>40.73788468</td><td>Upper_East_Side_North</td><td>-73.91715837</td><td>40.8541322</td></tr><tr><td>Upper_East_Side_North</td><td>-73.91715837</td><td>40.8541322</td><td>Central_Harlem_North</td><td>-73.99804922</td><td>40.71156838</td></tr></tbody></table><h2 id="Find-the-Shortest-Path"><a href="#Find-the-Shortest-Path" class="headerlink" title="Find the Shortest Path"></a>Find the Shortest Path</h2><p>There are two ways to find the shortest path, please check the docs for more details:</p><ol><li><a href="https://networkx.org/documentation/stable/reference/algorithms/shortest_paths.html">NetworkX</a></li><li><a href="https://udst.github.io/pandana/">Pandana(CH)</a></li></ol><h3 id="NetworkX-Dijkstra"><a href="#NetworkX-Dijkstra" class="headerlink" title="NetworkX(Dijkstra)"></a>NetworkX(Dijkstra)</h3><pre><code class="python"># The first trip record is from Lincoln_Square_East to Upper_East_Side_Northnx_Lincoln_Square_East_id = ox.distance.nearest_nodes(G,Lincoln_Square_East_Location.x,Lincoln_Square_East_Location.y)[0]nx_Upper_East_Side_North_id = ox.distance.nearest_nodes(G,Upper_East_Side_North_Location.x,Upper_East_Side_North_Location.y)[0]# NetworkX shortest pathdef SP_NX(G,SID,TID):    return nx.shortest_path(G, source=SID, target=TID, method=&quot;dijkstra&quot;, weight=&#39;length&#39;)     #displayNX_PATH=SP_NX(G,nx_Lincoln_Square_East_id,nx_Upper_East_Side_North_id)    fig , ax = ox.plot_graph(G, bgcolor=&quot;w&quot;, node_size=1, node_color=&quot;gray&quot;, edge_color=&quot;#aaa&quot;,show=False,close=False)ax.scatter(-73.97382133,40.73788468,c=&#39;yellow&#39;,marker=&quot;s&quot;,alpha=1,zorder=4)ax.scatter(-73.91715837,40.8541322,c=&#39;blue&#39;,alpha=1,zorder=3)ox.plot_graph_route(G,NX_PATH,ax=ax,orig_dest_size=0,route_alpha=0.5,route_colors=&#39;r&#39;,route_linewidths=2,show=False,close=False)</code></pre><!-- ![shortest path networkx Dijkstra](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/Shortest-Path-NetworkX.png?raw=true) --><img src="/2024/04/17/Shortest-Path-%E2%85%A0/Shortest-Path-NetworkX.png" class><h3 id="Pandana-CH"><a href="#Pandana-CH" class="headerlink" title="Pandana(CH)"></a>Pandana(CH)</h3><pre><code class="python"># trans road network to pandana formatnodes,edges = ox.graph_to_gdfs(G,nodes=True,edges=True)edges = edges.reset_index()G_pan = pandana.Network(nodes[&#39;x&#39;], nodes[&#39;y&#39;], edges[&#39;u&#39;], edges[&#39;v&#39;], edges[[&#39;length&#39;]],twoway=False)# The first trip record is from Lincoln_Square_East to Upper_East_Side_NorthLincoln_Square_East_Location = pd.DataFrame(&#123;&#39;longitude&#39;:[-73.97382133], &#39;latitude&#39;: [40.73788468]&#125;)Lincoln_Square_East_Location = gpd.points_from_xy(Lincoln_Square_East_Location.longitude, Lincoln_Square_East_Location.latitude, crs=&quot;EPSG:4326&quot;)Upper_East_Side_North_Location = pd.DataFrame(&#123;&#39;longitude&#39;:[-73.91715837], &#39;latitude&#39;: [40.8541322]&#125;)Upper_East_Side_North_Location = gpd.points_from_xy(Upper_East_Side_North_Location.longitude, Upper_East_Side_North_Location.latitude, crs=&quot;EPSG:4326&quot;)pan_Lincoln_Square_East_id = G_pan.get_node_ids(Lincoln_Square_East_Location.x,Lincoln_Square_East_Location.y).iloc[0]pan_Upper_East_Side_North_id = G_pan.get_node_ids(Upper_East_Side_North_Location.x,Upper_East_Side_North_Location.y).iloc[0]# pandana shortest pathdef SP_PAN(G_pan,SID,TID):    return G_pan.shortest_path(SID,TID) #displayPAN_PATH=SP_PAN(G_pan,pan_Lincoln_Square_East_id,pan_Upper_East_Side_North_id)    fig , ax = ox.plot_graph(G, bgcolor=&quot;w&quot;, node_size=1, node_color=&quot;gray&quot;, edge_color=&quot;#aaa&quot;,show=False,close=False)ax.scatter(-73.97382133,40.73788468,c=&#39;yellow&#39;,marker=&quot;s&quot;,alpha=1,zorder=4)ax.scatter(-73.91715837,40.8541322,c=&#39;blue&#39;,alpha=1,zorder=3)ox.plot_graph_route(G,PAN_PATH,ax=ax,orig_dest_size=0,route_alpha=0.5,route_colors=&#39;r&#39;,route_linewidths=2,show=False,close=False)</code></pre><!-- ![shortest path pandana CH](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/Shortest-Path-Pandana-CH.png?raw=true) --><img src="/2024/04/17/Shortest-Path-%E2%85%A0/Shortest-Path-Pandana-CH.png" class><h3 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h3><pre><code class="python"># you should upload trips.txt to your jupyter notebook first pickup_name=[]pickup_lon=[]pickup_lat=[]disengaged_name=[]disengaged_lon=[]disengaged_lat=[]import csv # opening the CSV filewith open(&#39;trips.txt&#39;, mode =&#39;r&#39;)as file:     # reading the CSV file  csvFile = csv.reader(file)    # displaying the contents of the CSV file  for lines in csvFile:        pickup_name.append(lines[0])        pickup_lon.append(lines[1])        pickup_lat.append(lines[2])        disengaged_name.append(lines[3])        disengaged_lon.append(lines[4])        disengaged_lat.append(lines[5])pickup_info = pd.DataFrame(&#123;&#39;pickup_name&#39;:pickup_name,&#39;longitude&#39;:pickup_lon, &#39;latitude&#39;: pickup_lat&#125;)disengaged_info = pd.DataFrame(&#123;&#39;disengaged_name&#39;:disengaged_name,&#39;longitude&#39;:disengaged_lon, &#39;latitude&#39;: disengaged_lat&#125;)pickup_Location = gpd.points_from_xy(pickup_info.longitude, pickup_info.latitude, crs=&quot;EPSG:4326&quot;)disengaged_Location = gpd.points_from_xy(disengaged_info.longitude, disengaged_info.latitude, crs=&quot;EPSG:4326&quot;)pickup_id = G_pan.get_node_ids(pickup_Location.x,pickup_Location.y)disengaged_id = G_pan.get_node_ids(disengaged_Location.x,disengaged_Location.y)nx_pickup_id = list(ox.distance.nearest_nodes(G,pickup_Location.x,pickup_Location.y))nx_disengaged_id = list(ox.distance.nearest_nodes(G,disengaged_Location.x,disengaged_Location.y))time_PAN=[]time_NX=[]test=[1,5,10,50,100,200,300,500,1000] # the query sizeNX_BATCH_PATH=[]PAN_BATCH_PATH=[]# This is the loop for evaluating the time of NetworkXfor i in range(len(test)):        tik = time()    for j in range(test[i]):         NX_BATCH_PATH.append(nx.shortest_path(G,source=nx_pickup_id[j],target=nx_disengaged_id[j],method=&#39;dijkstra&#39;,weight=&#39;length&#39;))        tok = time()    time_NX.append(tok-tik)    print(&#39;when query size = &#39;,test[i],end=&#39; , &#39;)    print(&#39;Time of Networkx is : &#39;,time_NX[-1],end=&#39;s\n&#39;)# This is the loop for evaluating the time of Pandanafor i in range(len(test)):    tik = time()    for j in range(test[i]):        PAN_BATCH_PATH.append(G_pan.shortest_path(pickup_id[j],disengaged_id[j]))        tok = time()    time_PAN.append(tok-tik)    print(&#39;when query size = &#39;,test[i],end=&#39; , &#39;)    print(&#39;Time of Pandana is : &#39;,time_PAN[-1],end=&#39;s\n&#39;)fig = plt.figure()ax = fig.add_subplot(1, 1, 1) clear_output(wait = True)ax.plot(test,time_PAN,label=&#39;Panadana&#39;)ax.plot(test,time_NX,label=&#39;Networkx&#39;)plt.ylabel(&#39;computing time(s)&#39;)plt.xlabel(&#39;Number of Query&#39;)plt.legend()fig.show()</code></pre><!-- ![shortest path comparison](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/shortest-path-comparison.png?raw=true) --><img src="/2024/04/17/Shortest-Path-%E2%85%A0/shortest-path-comparison.png" class><p>Here, we use Batch evaluation between Dijkstra (NetworkX) and CH (Pandana), and the results shows that CH algor is much faster than classical Dijskra.</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm, find-the-shortest-path </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo hand book</title>
      <link href="/2024/04/16/hexo-hand-book/"/>
      <url>/2024/04/16/hexo-hand-book/</url>
      
        <content type="html"><![CDATA[<h2 id="发布文章"><a href="#发布文章" class="headerlink" title="发布文章"></a>发布文章</h2><p>进入博客所在目录，右键打开Git Bash Here，创建博文：</p><pre><code class="bash">hexo new &quot;article title&quot;</code></pre><p>然后 source 文件夹中会出现一个 My New Post.md 文件，就可以使用 Markdown 编辑器在该文件中撰写文章了。</p><p>写完后运行下面代码将文章渲染并部署到 GitHub Pages 上完成发布。以后每次发布文章都是这两条命令。</p><pre><code class="bash">hexo g   # 生成页面hexo d   # 部署发布</code></pre><p>也可以不使用命令自己创建 .md 文件，只需在文件开头手动加入如下格式 Front-matter 即可，写完后运行 hexo g 和 hexo d 发布。</p><pre><code class="markdown">---title: Hello World # 标题date: 2019/3/26 hh:mm:ss # 时间categories: # 分类- Diarytags: # 标签- PS3- Games---摘要&lt;!--more--&gt;正文</code></pre><h2 id="网站设置"><a href="#网站设置" class="headerlink" title="网站设置"></a>网站设置</h2><p>包括网站名称、描述、作者、链接样式等，全部在网站目录下的 _config.yml 文件中，参考官方文档按需要编辑。</p><p>注意：冒号后要加一个空格！</p><h2 id="更换主题"><a href="#更换主题" class="headerlink" title="更换主题"></a>更换主题</h2><p>在 Themes | Hexo 选择一个喜欢的主题，比如 NexT，进入网站目录打开 Git Bash Here 下载主题：</p><pre><code class="bash">git clone https://github.com/iissnan/hexo-theme-next themes/next</code></pre><p>然后修改 _config.yml 中的 theme 为新主题名称 next，发布。（有的主题需要将 _config.yml 替换为主题自带的，参考主题说明。）</p><h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><pre><code class="bash">hexo new &quot;name&quot;       # 新建文章hexo new page &quot;name&quot;  # 新建页面hexo g                # 生成页面hexo d                # 部署hexo g -d             # 生成页面并部署hexo s                # 本地预览hexo clean            # 清除缓存和已生成的静态文件hexo help             # 帮助</code></pre><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><p>1、Hexo 设置显示文章摘要，首页不显示全文</p><p>Hexo 主页文章列表默认会显示文章全文，浏览时很不方便，可以在文章中插入</p><pre><code class="markdown">&lt;!--more--&gt;</code></pre><p>进行分段。</p><p>该代码前面的内容会作为摘要显示，而后面的内容会替换为 “Read More” 隐藏起来。</p><p>2、设置网站图标</p><p>进入 themes&#x2F;主题 文件夹，打开 _config.yml 配置文件，找到 favicon 修改，一般格式为：favicon: 图标地址。（不同主题可能略有差别）</p><p>3、修改并部署后没有效果</p><p>使用 hexo clean 清理后重新部署。</p><p>4、markdown图片引入没有效果</p><ul><li>统一改为Github仓库图片链接，例子：</li></ul><pre><code class="markdwon">![&quot;图片标题&quot;](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/temp/sheldon.png?raw=true)</code></pre><ul><li>使用模板语言</li></ul><pre><code class="markdown">&#123;% asset_img sheldon.png "图片标题" %&#125;</code></pre><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>Hexo 是一种纯静态的博客，我们必须要在本地完成文章的编辑再部署到 GitHub 上，依赖于本地环境。不能像 WordPress 或 Typecho 那样的动态博客一样能直接在浏览器中完成撰文和发布。</p><p>可以说是一种比较极客的写博客方式，但是优势也是明显的——免费稳定省心，比较适合爱折腾研究的用户，或者没有在线发文需求的朋友。</p><p><a href="https://hexo.io/">Hexo</a>!  Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>]]></content>
      
      
      <categories>
          
          <category> tools </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
