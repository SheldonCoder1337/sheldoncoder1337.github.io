<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Kmeans</title>
      <link href="/2024/04/24/Kmeans/"/>
      <url>/2024/04/24/Kmeans/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In this notebook, we shall be looking at how the kmeans algorithm works. KMeans is an <strong>unsupervised learning</strong> algorithm that is used to cluster data in groups - without knowing which group given data elements belong to as are going to see.</p><p>在本笔记本中，我们将研究 kmeans 算法的工作原理。KMeans 是一种<strong>无监督学习</strong>算法，用于将数据分组聚类 - 不知道给定的数据元素属于哪个组，正如将要看到的那样。</p><pre><code class="python">import numpy as npimport sklearn.datasetsimport matplotlib.pyplot as pltimport matplotlib as mpl</code></pre><p>Let’s shall generate a random dataset of 1000 points clustered into 3 groups.</p><p>让我们生成一个包含 1000 个点的随机数据集，这些点被聚类到 3 组。</p><pre><code class="python"># 设置数据集中样本点的数量N = 1000# 生成具有3个中心点的聚类数据集X_, y_ = sklearn.datasets.make_blobs(n_samples=N+5, centers=3) # 从生成的数据集中提取前N个样本作为训练数据X, y = X_[:N], y_[:N]# 从生成的数据集中提取后面5个样本作为测试数据X_test, y_test = X_[N:], y_[N:]# 绘制数据集的散点图plt.figure(figsize=(8, 6))for cls in np.unique(y):    plt.scatter(X[y==cls][:, 0], X[y==cls][:, 1], s=2)plt.title(&quot;Plot of features of dataset X&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><p><img src="/./Kmeans/sample_dataset_X.png" alt="sampla dataset"></p><h2 id="1-1-First-things-first"><a href="#1-1-First-things-first" class="headerlink" title="1.1 First things first"></a>1.1 First things first</h2><ol><li><p>We need to determine and set a value k, the number of clusters we <strong>think</strong> the data has. KMeans is unsupervised. So it is not the case that you will always know how many clusters (k) exist in the data. You will have to experiment with different values using certain techniques to find the best value of k. For our case, we know that there are 3 clusters, therefore we shall set k to 3. (You can try a different value after the points get clear).</p><ul><li>我们需要确定并设置一个值k，即我们<strong>认为</strong>数据具有的聚类数。KMeans 是无人监督的。因此，您并非总是知道数据中存在多少个聚类 （k）。您必须使用某些技术尝试不同的值，以找到 k 的最佳值。对于我们的情况，我们知道有 3 个聚类，因此我们将 k 设置为 3。（您可以在点明确后尝试其他值）。</li></ul></li><li><p>We also need to find k&#x3D;3 random points that will represent the centers of our clusters if the clustering is successfull. These k random points are called centroids.Let’s work on these two steps next.</p><ul><li>如果聚类成功，我们还需要找到 k&#x3D;3 个随机点，这些点将代表我们聚类的中心。这 k 个随机点称为质心。接下来，让我们继续执行这两个步骤。</li></ul></li></ol><pre><code class="python">k = 3N, feature_size = X.shape# 获取特征的最大值和最小值范围min_feature_range = np.min(X, axis=0)max_feature_range = np.max(X, axis=0)# 在上述范围内生成k个随机点centroids = np.zeros((len(max_feature_range), k))for i, (l, h) in enumerate(zip(min_feature_range, max_feature_range)):    # 使用random.uniform()函数从均匀分布中抽取样本    centroids[:, i] = np.random.uniform(low=l, high=h, size=feature_size)# 转置centroids矩阵，使得每行表示一个聚类中心centroids = centroids.Tprint(centroids)# 绘制数据集的散点图和聚类中心plt.figure(figsize=(8, 6))for cls in np.unique(y):    plt.scatter(X[y==cls][:, 0], X[y==cls][:, 1], s=2)# 绘制聚类中心for i, (x_, y_) in enumerate(centroids):    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=&#39;r&#39;)plt.title(&quot;Plot of features of dataset X&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><p><img src="/./Kmeans/1.1.png" alt="1.1"></p><p>The points are scattered and may not be close to the cluster centers. There are other methods like the <strong>kmeans++</strong>.</p><p>这些点是分散的并且可能不靠近聚类中心。还有其他方法，例如 <strong>kmeans++</strong></p><h2 id="1-2-Calculate-distances-to-centroids"><a href="#1-2-Calculate-distances-to-centroids" class="headerlink" title="1.2 Calculate distances to centroids"></a>1.2 Calculate distances to centroids</h2><p>In this notebook, we shall be calculating the euclidean distance. We have 2 columns in X <em>(x1 and x2)</em> and we have to calculate the euclidean distance between each centroid and every data point in X. Centroids are of the form <em>(xx, yy)</em> i.e they have two points just like our dataset X has 2 columns.</p><p>在这个笔记本中，我们将计算欧几里得距离。我们在 X <em>（x1 和 x2）</em> 中有 2 列，我们必须计算每个质心和 X 中每个数据点之间的欧几里得距离。 质心的形式为 <em>（xx， yy）</em>，即它们有两个点，就像我们的数据集 X 有 2 列一样。</p><p>We want to calculate something of the form $sqrt((x1-xx)^2 + (x2-yy)^2)$ for each data point&#x2F;row in X.</p><p>我们想要为 X 中的每个数据点&#x2F;行计算 $sqrt((x1-xx)^2 + (x2-yy)^2)$ 形式的内容。</p><p>To accelerate operations, we shall be using a vectorized approach to calculate that.</p><p>为了加速操作，我们将使用矢量化方法来计算。</p><ol><li>We have k&#x3D;3 centroids, so we shall first duplicate <strong>X</strong> 3 times or k times. The shape of X is (1000, 2). The result of the duplication, <strong>Xc</strong> will be (1000, 6).<ul><li>我们有 k&#x3D;3 个质心，所以我们首先要复制 <strong>X</strong> 3 次或 k 次。 X 的形状为 (1000, 2)。复制的结果 <strong>Xc</strong> 将是 (1000, 6)</li></ul></li><li>Then we shall flatten the centroids so that its a single vector, <strong>centroidsc</strong> of 6 elements to match our 6 columns in <strong>Xc</strong>.<ul><li>然后我们将展平质心，使其成为 6 个元素的单个矢量 <strong>centroidsc</strong>，以匹配 <strong>Xc</strong> 中的 6 列。</li></ul></li><li>We shall subtract <strong>Xc</strong> and <strong>centroidsc</strong> to give us a result <strong>D</strong> of shape (1000, 6). This step is equivalent to performing <strong>(x1-xx)</strong> and <strong>(x2-yy)</strong> for all centroids at once.<ul><li>我们将减去 <strong>Xc</strong> 和 <strong>centroidsc</strong> 得到形状为 (1000, 6) 的结果 <strong>D</strong>。此步骤相当于同时对所有质心执行 <strong>(x1-xx)</strong> 和 **(x2-yy)**。</li></ul></li></ol><ul><li>The first column of <strong>D</strong> corresponds to <strong>(x1-xx)</strong> where xx is the x of the first centroid.<ul><li><strong>D</strong> 的第一列对应于 **(x1-xx)**，其中 xx 是第一个质心的 x。</li></ul></li><li>The second column of <strong>D</strong> corresponds to <strong>(x2-yy)</strong> where yy is the y of the first centroid.<ul><li><strong>D</strong> 的第二列对应于 **(x2-yy)**，其中 yy 是第一个质心的 y。</li></ul></li><li>The third column of <strong>D</strong> corresponds to <strong>(x1-xx)</strong> where xx is the x of the <em>second</em> centroid.<ul><li><strong>D</strong> 的第三列对应于 **(x1-xx)**，其中 xx 是 <em>第二个</em> 质心的 x。</li></ul></li><li>The forth column of <strong>D</strong> corresponds to <strong>(x2-yy)</strong> where yy is the y of the <em>second</em> centroid. And so on.<ul><li><strong>D</strong> 的第四列对应于 **(x2-yy)*<em>，其中 yy 是</em>第二*质心的 y。等等。</li></ul></li></ul><p>4.The next step is to square these results, add them and apply sqrt. This whole operation results in what is called the <em>L2 norm</em> and is all performed by the <strong>np.linalg.norm</strong> function.</p><ul><li>下一步是将这些结果平方、相加并应用 sqrt。整个操作产生所谓的 <em>L2 范数</em>，并且全部由 <strong>np.linalg.norm</strong> 函数执行。</li></ul><p>Note that we have to calculate the norm over a given set of columns e.g the first and second columns’ norm corresponds to the first centroid, the third and forth to the 2nd centroid and the last 2 to the 3rd centroid. So in the end we have a (1000, 3) array containing euclidean distances of each of the 1000 data rows&#x2F;points in X in correspondence to each of the 3 centroids.</p><p>请注意，我们必须计算给定列集的范数，例如第一列和第二列的范数对应于第一个质心，第三列和第四列对应于第二个质心，最后 2 个对应于第三个质心。所以最后我们有一个 (1000, 3) 数组，其中包含 X 中 1000 个数据行&#x2F;点中每一个与 3 个质心相对应的欧氏距离。</p><p>Let’s do that</p><pre><code class="python">Xc = np.concatenate([X for c in centroids], axis=1) # duplicate X k timescentroidsc = centroids.ravel() # ravel to allow broadcast; Return a contiguous flattened array.D = (Xc - centroidsc) # raw diffprint(np.round(X[:5], 3))  # X (up to 5) without duplicationprint(np.round(Xc[:5], 3)) # Xc (up to 5) with duplicationprint(centroidsc) # raveled centroidsprint(D[:5])  # the difference between Xc and centroids#Calculating the normsNorms = np.zeros((N, k)) # distances to each centroidfor i in range(0, k):    m = i*feature_size    Norms[:, i] = np.linalg.norm(D[:, m:m+feature_size], axis=1)    # sample norm / distancesample_norms = Norms[:4]sample_norms</code></pre><h2 id="1-3-Attach-instances-or-rows-to-the-closest-centroid"><a href="#1-3-Attach-instances-or-rows-to-the-closest-centroid" class="headerlink" title="1.3 Attach instances or rows to the closest centroid."></a>1.3 Attach instances or rows to the closest centroid.</h2><p>We shall now assign each data row in X the index of the centroid with which it has the shortest distance. We do that using <strong>np.argmin</strong> which returns the index of the minimum distance in our <em>Norms</em> array.</p><p>现在，我们将为 X 中的每个数据行分配与其距离最短的质心的索引。我们使用 <strong>np.argmin</strong> 来执行此操作，它返回 <em>Norms</em> 数组中最小距离的索引。</p><pre><code class="python"># sample of indices for smallest indices for sample_normsnp.argmin(sample_norms, axis=1)# we do this for all Normsypred = np.argmin(Norms, axis=1) #Returns the indices of the minimum values along an axis.plt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids):  # with underscore    p = plt.scatter(X[y==i][:, 0], X[y==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    anot = &#39;c&#39;+str(i) + &quot; at &quot; + str(np.round(Norms[0, i], 3))    plt.plot([X[0][0], x_], [X[0][1], y_])    plt.annotate(xy=(x_+.2, y_-.1), text=anot, color=&#39;k&#39;, size=10)    plt.scatter(X[0][0], X[0][1], marker=&#39;o&#39;, c=&#39;r&#39;, s=35)plt.title(&quot;Plot showing distance sample (point belongs to c&quot;+str(ypred[0])+&quot;)&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><p><img src="/./Kmeans/1.3.png" alt="1.3"></p><h2 id="1-4-Update-centroids"><a href="#1-4-Update-centroids" class="headerlink" title="1.4 Update centroids"></a>1.4 Update centroids</h2><p>The last step is to update the centroids by setting the new centroids at the mean positions of the points they were closest to i.e the data points they influence.</p><p>最后一步是通过将新质心设置在它们最接近的点的平均位置（即它们影响的数据点）来更新质心。</p><p>Below is a plot showing the un-updated centroids and their influence on the data points. We shall have to move the centroids so that they are at the center of the points they influence.</p><p>下面的图显示了未更新的质心及其对数据点的影响。我们必须移动质心，使它们位于它们影响的点的中心。</p><pre><code class="python">plt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids):    p = plt.scatter(X[ypred==i][:, 0], X[ypred==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=clr, size=14)plt.title(&quot;Plot showing influence of recent centroids on data points&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><p><img src="/./Kmeans/1.4.1.png" alt="1.4.1"></p><ul><li>To do that, we shall calculate the mean of the data points each centroid influences and put the centroid at that mean location.</li><li>If a centroid has no points it influences (yes, this can happen), we leave the centroid where it is.</li><li>为此，我们将计算每个质心影响的数据点的平均值，并将质心放在该平均位置。</li><li>如果质心没有受到影响的点（是的，这可能发生），我们将质心保留在原来的位置。</li></ul><pre><code class="python">centroids_ = []for i in range(k):    if len(X[ypred == i]) == 0:        centroids_.append(centroids[i]) # use old    else:        centroids_.append(np.mean(X[ypred == i], axis=0))centroids_ = np.array(centroids_)centroids_#After the update, we have the following plotplt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids_):  # with underscore    p = plt.scatter(X[ypred==i][:, 0], X[ypred==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=clr, size=14)plt.title(&quot;Plot showing mean-centered centroids &quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><p><img src="/./Kmeans/1.4.2.png" alt="1.4.2"></p><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><ul><li>The mean-centered points may look good, however the data they influence may still be bad</li><li>以均值为中心的点可能看起来不错，但它们影响的数据可能仍然很糟糕</li></ul><h2 id="1-5-Repeat-the-above-steps"><a href="#1-5-Repeat-the-above-steps" class="headerlink" title="1.5 Repeat the above steps"></a>1.5 Repeat the above steps</h2><ul><li><p>Now we repeat the steps above until the centroids don’t update or move (significantly) anymore. That will be the case when <strong>(centroids_ - centroids)^2</strong> is a low value below a certain threshold. A good threshold has to be as low as possible i.e close to or equal to zero.</p></li><li><p>You can run the following code multiple times and observe <strong>shift</strong> value (which is <strong>changes of centroids</strong> ) carefully</p></li><li><p>Here, we are using <strong>^2</strong> to make larger shifts&#x2F;updates significant. It doesn’t have to be that way. A norm can also work.</p></li><li><p>现在我们重复上述步骤，直到质心不再更新或移动（显着）。当 <strong>(centroids_ - centroids)^2</strong> 是低于某个阈值的低值时，就会出现这种情况。一个好的阈值必须尽可能低，即接近或等于零。</p></li><li><p>您可以多次运行以下代码并仔细观察<strong>shift</strong>值（即<strong>质心的变化</strong>）</p></li><li><p>在这里，我们使用 <strong>^2</strong> 来使更大的转变&#x2F;更新变得重要。事情并不一定是这样的。规范也可以发挥作用。</p></li></ul><pre><code class="python">shift = np.sum((centroids_ - centroids)**2)print(shift)#Below we repeat all the previous steps in one runcentroids = centroids_Xc = np.concatenate([X for c in centroids], axis=1) # duplicate k timescentroidsc = centroids.ravel() # ravel to allow broadcastD = (Xc - centroidsc) # raw diffNorms = np.zeros((N, k)) # distances to each clusterfor i in range(0, k):    m = i*feature_size    Norms[:, i] = np.linalg.norm(D[:, m:m+feature_size], axis=1)###### You should Choose the nearest cluster for every point, and save the result in ypred which is used in next code segment###### You can use np.argmin function###### Write your code here, just one lineypred = np.argmin(Norms, axis=1)####################### new clusters are mean X along centroids_ = []for i in range(k):    if len(X[ypred == i]) == 0:        centroids_.append(centroids[i]) # use old    else:        centroids_.append(np.mean(X[ypred == i], axis=0))centroids_ = np.array(centroids_)centroids_plt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids_):  # with underscore    p = plt.scatter(X[ypred==i][:, 0], X[ypred==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=clr, size=14)plt.title(&quot;Plot showing mean-centered centroids &quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Network</title>
      <link href="/2024/04/17/Neural-Network/"/>
      <url>/2024/04/17/Neural-Network/</url>
      
        <content type="html"><![CDATA[<ol><li>Neural Architecture</li><li>Activate function and  Loss function</li><li>Optimizer<ul><li>Adadelta</li></ul></li><li>Overfitting</li><li>the size of the filter</li></ol><h2 id="Convolution-Neural-Networks-CNN"><a href="#Convolution-Neural-Networks-CNN" class="headerlink" title="Convolution Neural Networks (CNN)"></a>Convolution Neural Networks (CNN)</h2><h2 id="Take-away-today"><a href="#Take-away-today" class="headerlink" title="Take-away today"></a>Take-away today</h2><ul><li>Perceptron<ul><li>Neuron</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep learning </tag>
            
            <tag> Math </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shortest Path Ⅱ</title>
      <link href="/2024/04/17/Shortest-Path-%E2%85%A1/"/>
      <url>/2024/04/17/Shortest-Path-%E2%85%A1/</url>
      
        <content type="html"><![CDATA[<p>In the previous article <a href="https://sheldoncoder1337.github.io/2024/04/17/Shortest-Path-%E2%85%A0/">Shortest-Path-Ⅰ</a>, we have already learn how to use Networkx and Pandana <code>shortest_path</code> API to find the shortest path on <a href="https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc">New York</a> <a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page">New York Taxi Trip</a> dataset. And after comparing the performance between Dijkstra and Constraction Hierarchy algorithm, we could found that CH have a much better performance than classic Dijkstra algorithm.</p><p>In this blog, let’s try to fix the Carpool problem.</p><h2 id="Location-Statistics-Heat-Map-Visualization"><a href="#Location-Statistics-Heat-Map-Visualization" class="headerlink" title="Location Statistics &amp; Heat Map Visualization"></a>Location Statistics &amp; Heat Map Visualization</h2><pre><code class="python"></code></pre><pre><code class="python">import pandas as pdimport plotly.express as px# Data with latitude/longitude and valuesdf = pd.read_csv(&#39;https://raw.githubusercontent.com/R-CoderDotCom/data/main/sample_datasets/population_galicia.csv&#39;)fig = px.density_mapbox(df, lat = &#39;latitude&#39;, lon = &#39;longitude&#39;, z = &#39;tot_pob&#39;,                        radius = 7,                        center = dict(lat = 42.83, lon = -8.35),                        zoom = 6,                        mapbox_style = &#39;open-street-map&#39;,                        color_continuous_scale = &#39;rainbow&#39;,                        opacity = 0.5)fig.show()</code></pre><h2 id="Carpool-problem"><a href="#Carpool-problem" class="headerlink" title="Carpool problem"></a>Carpool problem</h2><p>With the rise of taxi-hailing mobile programs (such as uber), a New York cab driver is used to take orders from online platform. Given the initial location of the driver and 2-3 orders (e.g., each order is a 6-tuple, like a record in the NY Taxi data), your task is to find a feasible route to pick up all the orders.</p><p>For instance, the driver is now at location Time Square, he is assigned to pick up three passengers.</p><ul><li>passengerA: JFK_Airport to East_Chelsea</li><li>passengerB: West_Village to East_Chelsea</li><li>passengerC: Battery_Park_City to Queens_Plaza</li></ul><p>One feasible solution is to report the route from</p><ul><li>Time Square -&gt; JFK_Airport -&gt; East_Chelsea -&gt; West_Village -&gt; East_Chelsea -&gt; Battery_Park_City -&gt; Queens_Plaza</li></ul><h3 id="Our-target"><a href="#Our-target" class="headerlink" title="Our target"></a>Our target</h3><ol><li>Write a function to determine the route and the total distance of the route.</li><li>Plot the route on the map.</li></ol><p>Obviously, the feasible solution is far from optimal as East_Chelsea is the common locations of two pessagers. Thereby,  </p><h3 id="Bonus-task"><a href="#Bonus-task" class="headerlink" title="Bonus task"></a>Bonus task</h3><ul><li>Try to find the best route based on the given orders. For the bonus part, please explain your methodology and your mark will be given based on the soundness of your idea, the quality of analysis, and the implementation.</li></ul><pre><code class="python"></code></pre>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm, find-the-shortest-path </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shortest Path Ⅰ</title>
      <link href="/2024/04/17/Shortest-Path-%E2%85%A0/"/>
      <url>/2024/04/17/Shortest-Path-%E2%85%A0/</url>
      
        <content type="html"><![CDATA[<p>In this series, I will introduce some third-party libraries such as osmnx, pandana, geopandas and compare the performance between <strong>NetworkX(Dijkstra)</strong> and <strong>Pandana( Constraction Hierarchy)</strong>. Finally, I will show how to use these libraries to solve a <strong>Carpool(拼车) problem</strong>. The data set used in this article is from <a href="https://www.openstreetmap.org/">OpenStreetMap</a> - New York City Taxi Trip data set.</p><h2 id="preliminary"><a href="#preliminary" class="headerlink" title="preliminary"></a>preliminary</h2><p>You are highly recommended to use Conda to setup a new virtual environment.</p><pre><code class="bash">conda create -n geospatial python==3.8conda activate geospatialpip install geopandas network osmnet osmnx pandas pandana</code></pre><p>If you received an error like “spatialindex_c-64.dll is missing”, try to use the following commands to resolve it.</p><pre><code class="bash">pip uninstall rtreepip install rtree</code></pre><pre><code class="python">import warningswarnings.filterwarnings(&#39;ignore&#39;)warnings.simplefilter(&#39;ignore&#39;)import osmnx as oximport numpy as npimport geopandas as gpdimport pandanaimport pandas as pdfrom time import timeimport matplotlib.pyplot as pltfrom IPython.display import display, clear_outputimport networkx as nximport momepy</code></pre><h2 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h2><pre><code class="python">def extract_graph(place=&#39;New York&#39;):    # try Chinese    # G = ox.graph_from_place(&#39;纽约&#39;, network_type=&#39;drive&#39;)    ox.config(log_console=True, use_cache=True)    G = ox.graph_from_place(place, network_type=&#39;drive&#39;)    return Gplace = &#39;New York&#39;G = extract_graph(place)ox.plot_graph(G, bgcolor=&quot;w&quot;, node_size=1, node_color=&quot;yellow&quot;, edge_color=&quot;#aaa&quot;)print(&quot;node count:&quot;, len(G.nodes()))print(&quot;edge count:&quot;, len(G.edges()))</code></pre><p><img src="https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/New-York-Taxi-Trip.png?raw=true" alt="New York Taxi Trip"></p><p>There are total node 55344 nodes and 139582 edges.</p><p>We process <a href="https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc">New York</a> <a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page">New York Taxi Trip</a>  and provide Trips.txt (<a href="https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/sources/Shortest-Path/Trips.txt">Appendix</a>)</p><p>Trips.txt contains New York Taxi trajectory information for 10,000 lines, each containing six columns of information, the region name where the passengers are picked up(PName),the lon and lat of the region in which they are picked up(PLon PLat),the region name they are delivered(Dname) and in which the passenger is delivered(DLon DLat).</p><p>For example:</p><table><thead><tr><th>PName</th><th>PLon</th><th>PLat</th><th>DName</th><th>DLon</th><th>DLat</th></tr></thead><tbody><tr><td>Lincoln_Square_East</td><td>-73.97382133</td><td>40.73788468</td><td>Upper_East_Side_North</td><td>-73.91715837</td><td>40.8541322</td></tr><tr><td>Upper_East_Side_North</td><td>-73.91715837</td><td>40.8541322</td><td>Central_Harlem_North</td><td>-73.99804922</td><td>40.71156838</td></tr></tbody></table><h2 id="Find-the-Shortest-Path"><a href="#Find-the-Shortest-Path" class="headerlink" title="Find the Shortest Path"></a>Find the Shortest Path</h2><p>There are two ways to find the shortest path, please check the docs for more details:</p><ol><li><a href="https://networkx.org/documentation/stable/reference/algorithms/shortest_paths.html">NetworkX</a></li><li><a href="https://udst.github.io/pandana/">Pandana(CH)</a></li></ol><h3 id="NetworkX-Dijkstra"><a href="#NetworkX-Dijkstra" class="headerlink" title="NetworkX(Dijkstra)"></a>NetworkX(Dijkstra)</h3><pre><code class="python"># The first trip record is from Lincoln_Square_East to Upper_East_Side_Northnx_Lincoln_Square_East_id = ox.distance.nearest_nodes(G,Lincoln_Square_East_Location.x,Lincoln_Square_East_Location.y)[0]nx_Upper_East_Side_North_id = ox.distance.nearest_nodes(G,Upper_East_Side_North_Location.x,Upper_East_Side_North_Location.y)[0]# NetworkX shortest pathdef SP_NX(G,SID,TID):    return nx.shortest_path(G, source=SID, target=TID, method=&quot;dijkstra&quot;, weight=&#39;length&#39;)     #displayNX_PATH=SP_NX(G,nx_Lincoln_Square_East_id,nx_Upper_East_Side_North_id)    fig , ax = ox.plot_graph(G, bgcolor=&quot;w&quot;, node_size=1, node_color=&quot;gray&quot;, edge_color=&quot;#aaa&quot;,show=False,close=False)ax.scatter(-73.97382133,40.73788468,c=&#39;yellow&#39;,marker=&quot;s&quot;,alpha=1,zorder=4)ax.scatter(-73.91715837,40.8541322,c=&#39;blue&#39;,alpha=1,zorder=3)ox.plot_graph_route(G,NX_PATH,ax=ax,orig_dest_size=0,route_alpha=0.5,route_colors=&#39;r&#39;,route_linewidths=2,show=False,close=False)</code></pre><p><img src="https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/Shortest-Path-NetworkX.png?raw=true" alt="shortest path networkx Dijkstra"></p><h3 id="Pandana-CH"><a href="#Pandana-CH" class="headerlink" title="Pandana(CH)"></a>Pandana(CH)</h3><pre><code class="python"># trans road network to pandana formatnodes,edges = ox.graph_to_gdfs(G,nodes=True,edges=True)edges = edges.reset_index()G_pan = pandana.Network(nodes[&#39;x&#39;], nodes[&#39;y&#39;], edges[&#39;u&#39;], edges[&#39;v&#39;], edges[[&#39;length&#39;]],twoway=False)# The first trip record is from Lincoln_Square_East to Upper_East_Side_NorthLincoln_Square_East_Location = pd.DataFrame(&#123;&#39;longitude&#39;:[-73.97382133], &#39;latitude&#39;: [40.73788468]&#125;)Lincoln_Square_East_Location = gpd.points_from_xy(Lincoln_Square_East_Location.longitude, Lincoln_Square_East_Location.latitude, crs=&quot;EPSG:4326&quot;)Upper_East_Side_North_Location = pd.DataFrame(&#123;&#39;longitude&#39;:[-73.91715837], &#39;latitude&#39;: [40.8541322]&#125;)Upper_East_Side_North_Location = gpd.points_from_xy(Upper_East_Side_North_Location.longitude, Upper_East_Side_North_Location.latitude, crs=&quot;EPSG:4326&quot;)pan_Lincoln_Square_East_id = G_pan.get_node_ids(Lincoln_Square_East_Location.x,Lincoln_Square_East_Location.y).iloc[0]pan_Upper_East_Side_North_id = G_pan.get_node_ids(Upper_East_Side_North_Location.x,Upper_East_Side_North_Location.y).iloc[0]# pandana shortest pathdef SP_PAN(G_pan,SID,TID):    return G_pan.shortest_path(SID,TID) #displayPAN_PATH=SP_PAN(G_pan,pan_Lincoln_Square_East_id,pan_Upper_East_Side_North_id)    fig , ax = ox.plot_graph(G, bgcolor=&quot;w&quot;, node_size=1, node_color=&quot;gray&quot;, edge_color=&quot;#aaa&quot;,show=False,close=False)ax.scatter(-73.97382133,40.73788468,c=&#39;yellow&#39;,marker=&quot;s&quot;,alpha=1,zorder=4)ax.scatter(-73.91715837,40.8541322,c=&#39;blue&#39;,alpha=1,zorder=3)ox.plot_graph_route(G,PAN_PATH,ax=ax,orig_dest_size=0,route_alpha=0.5,route_colors=&#39;r&#39;,route_linewidths=2,show=False,close=False)</code></pre><p><img src="https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/Shortest-Path-Pandana-CH.png?raw=true" alt="shortest path pandana CH"></p><h3 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h3><pre><code class="python"># you should upload trips.txt to your jupyter notebook first pickup_name=[]pickup_lon=[]pickup_lat=[]disengaged_name=[]disengaged_lon=[]disengaged_lat=[]import csv # opening the CSV filewith open(&#39;trips.txt&#39;, mode =&#39;r&#39;)as file:     # reading the CSV file  csvFile = csv.reader(file)    # displaying the contents of the CSV file  for lines in csvFile:        pickup_name.append(lines[0])        pickup_lon.append(lines[1])        pickup_lat.append(lines[2])        disengaged_name.append(lines[3])        disengaged_lon.append(lines[4])        disengaged_lat.append(lines[5])pickup_info = pd.DataFrame(&#123;&#39;pickup_name&#39;:pickup_name,&#39;longitude&#39;:pickup_lon, &#39;latitude&#39;: pickup_lat&#125;)disengaged_info = pd.DataFrame(&#123;&#39;disengaged_name&#39;:disengaged_name,&#39;longitude&#39;:disengaged_lon, &#39;latitude&#39;: disengaged_lat&#125;)pickup_Location = gpd.points_from_xy(pickup_info.longitude, pickup_info.latitude, crs=&quot;EPSG:4326&quot;)disengaged_Location = gpd.points_from_xy(disengaged_info.longitude, disengaged_info.latitude, crs=&quot;EPSG:4326&quot;)pickup_id = G_pan.get_node_ids(pickup_Location.x,pickup_Location.y)disengaged_id = G_pan.get_node_ids(disengaged_Location.x,disengaged_Location.y)nx_pickup_id = list(ox.distance.nearest_nodes(G,pickup_Location.x,pickup_Location.y))nx_disengaged_id = list(ox.distance.nearest_nodes(G,disengaged_Location.x,disengaged_Location.y))time_PAN=[]time_NX=[]test=[1,5,10,50,100,200,300,500,1000] # the query sizeNX_BATCH_PATH=[]PAN_BATCH_PATH=[]# This is the loop for evaluating the time of NetworkXfor i in range(len(test)):        tik = time()    for j in range(test[i]):         NX_BATCH_PATH.append(nx.shortest_path(G,source=nx_pickup_id[j],target=nx_disengaged_id[j],method=&#39;dijkstra&#39;,weight=&#39;length&#39;))        tok = time()    time_NX.append(tok-tik)    print(&#39;when query size = &#39;,test[i],end=&#39; , &#39;)    print(&#39;Time of Networkx is : &#39;,time_NX[-1],end=&#39;s\n&#39;)# This is the loop for evaluating the time of Pandanafor i in range(len(test)):    tik = time()    for j in range(test[i]):        PAN_BATCH_PATH.append(G_pan.shortest_path(pickup_id[j],disengaged_id[j]))        tok = time()    time_PAN.append(tok-tik)    print(&#39;when query size = &#39;,test[i],end=&#39; , &#39;)    print(&#39;Time of Pandana is : &#39;,time_PAN[-1],end=&#39;s\n&#39;)fig = plt.figure()ax = fig.add_subplot(1, 1, 1) clear_output(wait = True)ax.plot(test,time_PAN,label=&#39;Panadana&#39;)ax.plot(test,time_NX,label=&#39;Networkx&#39;)plt.ylabel(&#39;computing time(s)&#39;)plt.xlabel(&#39;Number of Query&#39;)plt.legend()fig.show()</code></pre><p><img src="https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/shortest-path-comparison.png?raw=true" alt="shortest path comparison"></p><p>Here, we use Batch evaluation between Dijkstra (NetworkX) and CH (Pandana), and the results shows that CH algor is much faster than classical Dijskra.</p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm, find-the-shortest-path </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo hand book</title>
      <link href="/2024/04/16/hexo-hand-book/"/>
      <url>/2024/04/16/hexo-hand-book/</url>
      
        <content type="html"><![CDATA[<h2 id="发布文章"><a href="#发布文章" class="headerlink" title="发布文章"></a>发布文章</h2><p>进入博客所在目录，右键打开Git Bash Here，创建博文：</p><pre><code class="bash">hexo new &quot;article title&quot;</code></pre><p>然后 source 文件夹中会出现一个 My New Post.md 文件，就可以使用 Markdown 编辑器在该文件中撰写文章了。</p><p>写完后运行下面代码将文章渲染并部署到 GitHub Pages 上完成发布。以后每次发布文章都是这两条命令。</p><pre><code class="bash">hexo g   # 生成页面hexo d   # 部署发布</code></pre><p>也可以不使用命令自己创建 .md 文件，只需在文件开头手动加入如下格式 Front-matter 即可，写完后运行 hexo g 和 hexo d 发布。</p><pre><code class="markdown">---title: Hello World # 标题date: 2019/3/26 hh:mm:ss # 时间categories: # 分类- Diarytags: # 标签- PS3- Games---摘要&lt;!--more--&gt;正文</code></pre><h2 id="网站设置"><a href="#网站设置" class="headerlink" title="网站设置"></a>网站设置</h2><p>包括网站名称、描述、作者、链接样式等，全部在网站目录下的 _config.yml 文件中，参考官方文档按需要编辑。</p><p>注意：冒号后要加一个空格！</p><h2 id="更换主题"><a href="#更换主题" class="headerlink" title="更换主题"></a>更换主题</h2><p>在 Themes | Hexo 选择一个喜欢的主题，比如 NexT，进入网站目录打开 Git Bash Here 下载主题：</p><pre><code class="bash">git clone https://github.com/iissnan/hexo-theme-next themes/next</code></pre><p>然后修改 _config.yml 中的 theme 为新主题名称 next，发布。（有的主题需要将 _config.yml 替换为主题自带的，参考主题说明。）</p><h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><pre><code class="bash">hexo new &quot;name&quot;       # 新建文章hexo new page &quot;name&quot;  # 新建页面hexo g                # 生成页面hexo d                # 部署hexo g -d             # 生成页面并部署hexo s                # 本地预览hexo clean            # 清除缓存和已生成的静态文件hexo help             # 帮助</code></pre><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><p>1、Hexo 设置显示文章摘要，首页不显示全文</p><p>Hexo 主页文章列表默认会显示文章全文，浏览时很不方便，可以在文章中插入</p><pre><code class="markdown">&lt;!--more--&gt;</code></pre><p>进行分段。</p><p>该代码前面的内容会作为摘要显示，而后面的内容会替换为 “Read More” 隐藏起来。</p><p>2、设置网站图标</p><p>进入 themes&#x2F;主题 文件夹，打开 _config.yml 配置文件，找到 favicon 修改，一般格式为：favicon: 图标地址。（不同主题可能略有差别）</p><p>3、修改并部署后没有效果</p><p>使用 hexo clean 清理后重新部署。</p><p>4、markdown图片引入没有效果</p><p>统一改为Github仓库图片链接，例子：</p><pre><code class="markdwon">![&quot;图片标题&quot;](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/temp/sheldon.png?raw=true)</code></pre><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>Hexo 是一种纯静态的博客，我们必须要在本地完成文章的编辑再部署到 GitHub 上，依赖于本地环境。不能像 WordPress 或 Typecho 那样的动态博客一样能直接在浏览器中完成撰文和发布。</p><p>可以说是一种比较极客的写博客方式，但是优势也是明显的——免费稳定省心，比较适合爱折腾研究的用户，或者没有在线发文需求的朋友。</p><p><a href="https://hexo.io/">Hexo</a>!  Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>]]></content>
      
      
      <categories>
          
          <category> tools </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
