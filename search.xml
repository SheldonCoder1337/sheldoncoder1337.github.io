<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>svd</title>
      <link href="/2024/05/04/svd/"/>
      <url>/2024/05/04/svd/</url>
      
        <content type="html"><![CDATA[<p>Problem: Decompose the matrix $\mathrm{A}&#x3D;\begin{pmatrix}5&amp;3 \\ 0&amp;-4\end{pmatrix}$ using Singular Value Decomposition(SVD). Please show detail calculation steps.</p><p><strong>Step1:</strong> calculate $AA^T$ and $A^TA$</p><p>$$A &#x3D; \begin{pmatrix}5&amp;3 \\ 0&amp;-4\end{pmatrix}, \text{then } A^T &#x3D; \begin{pmatrix}5&amp;0 \\ 3&amp;-4\end{pmatrix}$$</p><p>$$AA^T &#x3D; \begin{pmatrix}5&amp;3 \\ 0&amp;-4\end{pmatrix}\begin{pmatrix}5&amp;0 \\ 3&amp;-4\end{pmatrix} &#x3D; \begin{pmatrix}34&amp;-12 \\ -12&amp;-16\end{pmatrix}$$</p><p>$$A^TA &#x3D; \begin{pmatrix}5&amp;0 \\ 3&amp;-4\end{pmatrix}\begin{pmatrix}5&amp;3 \\ 0&amp;-4\end{pmatrix} &#x3D; \begin{pmatrix}25&amp;15 \\ 15&amp;25\end{pmatrix}$$</p><p><strong>Step2:</strong> calculate $\lambda_1, \lambda_2$ and $S$</p><p>$$\begin{aligned}<br>|AA^{T}-\lambda E|&#x3D;0<br>    &amp;\Rightarrow<br>    \left|\left(\begin{matrix}{34}&amp;{-12} \\ {-12}&amp;{16} \end{matrix}\right)-\lambda\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right|&#x3D;0<br>\end{aligned}$$</p><p>$$\Rightarrow<br>\left|\begin{matrix}{34-\lambda}&amp;{-12} \\ {-12}&amp;{16-\lambda} \end{matrix}\right|&#x3D;0\Rightarrow(34-\lambda)(16-\lambda)-12\times12&#x3D;0<br>$$</p><p>$$\Rightarrow<br>\lambda^{2}-50\lambda+400&#x3D;0\Rightarrow(\lambda-40)(\lambda-10)&#x3D;0<br>$$</p><p>Eigenvalues: $\lambda_1&#x3D;40, \lambda_2&#x3D;10$</p><p>Singular values: $\sigma_1&#x3D;\sqrt{\lambda_1}&#x3D;\sqrt{40}&#x3D;2\sqrt{10},\sigma_2&#x3D;\sqrt{\lambda_2}&#x3D;\sqrt{10}$</p><p>Diagonal matrix S:  $S&#x3D;\begin{pmatrix}\sigma_1&amp;0 \\ 0&amp;\sigma_1\end{pmatrix}&#x3D;\begin{pmatrix}2\sqrt{10}&amp;0 \\ 0&amp;\sqrt{10}\end{pmatrix}$</p><p><strong>Step3:</strong> Finding $U$ $(AA^{T}-\lambda E)x&#x3D;0\Rightarrow U&#x3D;(u_{1},u_{2})&#x3D;\begin{pmatrix}-\frac{2}{\sqrt{5}}&amp;\frac{1}{\sqrt{5}} \\ \frac{1}{\sqrt{5}}&amp;\frac{2}{\sqrt{5}}\end{pmatrix}$</p><p>$$\begin{aligned}<br>For~\lambda_{1}&amp;&#x3D;40,<br>(AA^{T}-\lambda_{1}E)x_{1}&#x3D;0<br>\Rightarrow<br>\left(\left(\begin{matrix}{34}&amp;{-12} \\ {-12}&amp;{16} \end{matrix}\right)-40\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right)x_1&#x3D;0<br>&amp;\end{aligned}<br>$$</p><p>$$\Rightarrow<br>\left(\begin{matrix}{-6}&amp;{-12} \\ {-12}&amp;{-24} \end{matrix}\right)x_1&#x3D;0<br>\Rightarrow<br>x_{1}&#x3D;\binom{-2a}{a}\Longrightarrow u_{1}&#x3D;\frac{x_{1}}{||x_{1}||}&#x3D;\begin{pmatrix}-\frac{2}{\sqrt{5}} \\ \frac{1}{\sqrt{5}}\end{pmatrix}<br>$$</p><p>$$\begin{aligned}<br>For~\lambda_{2}&amp;&#x3D;10,<br>(AA^{T}-\lambda_{2}E)x_{2}&#x3D;0<br>\Rightarrow<br>\left(\left(\begin{matrix}{34}&amp;{-12} \\ {-12}&amp;{16} \end{matrix}\right)-10\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right)x_2&#x3D;0<br>\end{aligned}<br>$$</p><p>$$<br>\Rightarrow<br>\left(\begin{matrix}{24}&amp;{-12} \\ {-12}&amp;{6} \end{matrix}\right)x_2&#x3D;0<br>\Rightarrow<br>x_{2}&#x3D;\binom{a}{2a}\Longrightarrow u_{2}&#x3D;\frac{x_{2}}{||x_{2}||}&#x3D;\begin{pmatrix}\frac{1}{\sqrt{5}} \\ \frac{2}{\sqrt{5}}\end{pmatrix}<br>$$</p><p><strong>Step4:</strong> Finding $V$ $(AA^{T}-\lambda E)x&#x3D;0\Rightarrow V&#x3D;(v_{1},v_{2})&#x3D;\begin{pmatrix}\frac{1}{\sqrt{2}}&amp;\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}&amp;-\frac{1}{\sqrt{2}}\end{pmatrix}$</p><p>$$\begin{aligned}<br>For~\lambda_{1}&amp;&#x3D;40,<br>(AA^{T}-\lambda_{1}E)x_{3}&#x3D;0<br>\Rightarrow<br>\left(\left(\begin{matrix}{25}&amp;{15} \\ {15}&amp;{25} \end{matrix}\right)-40\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right)x_3&#x3D;0<br>\end{aligned}<br>$$</p><p>$$<br>\Rightarrow<br>\left(\begin{matrix}{-15}&amp;{15} \\ {15}&amp;{-15} \end{matrix}\right)x_3 &#x3D;0<br>\Rightarrow<br>x_{3}&#x3D;\binom{c}{c}\Longrightarrow v_{1}&#x3D;\frac{x_{3}}{||x_{3}||}&#x3D;\begin{pmatrix}\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}\end{pmatrix}<br>$$</p><p>$$\begin{aligned}<br>For~\lambda_{2}&amp;&#x3D;10,<br>(AA^{T}-\lambda_{2}E)x_{4}&#x3D;0<br>\Rightarrow<br>\left(\left(\begin{matrix}{25}&amp;{15} \\ {15}&amp;{25}  \end{matrix}\right)-10\left(\begin{matrix}{1}&amp;{0} \\ {0}&amp;{1} \end{matrix}\right)\right)x_4&#x3D;0<br>\end{aligned}<br>$$</p><p>$$<br>\Rightarrow<br>\left(\begin{matrix}{15}&amp;{15} \\ {15}&amp;{15} \end{matrix}\right)x_4&#x3D;0<br>\Rightarrow<br>x_{4}&#x3D;\binom{d}{-d}\Longrightarrow v_{2}&#x3D;\frac{x_{4}}{||x_{4}||}&#x3D;\begin{pmatrix}\frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}}\end{pmatrix}<br>$$</p><p><strong>Step5:</strong> Complete SVD</p><p>$$<br>\left(\begin{matrix}{5}&amp;{3} \\ {0}&amp;{-4} \end{matrix}\right) &#x3D;<br>\left(\begin{matrix}{-\frac{2}{\sqrt{5}}}&amp;{\frac{1}{\sqrt{5}}} \\ {\frac{1}{\sqrt{5}}}&amp;{\frac{2}{\sqrt{5}}} \end{matrix}\right)<br>\left(\begin{matrix}{2\sqrt{10}}&amp;{0} \\ {0}&amp;{\sqrt{10}} \end{matrix}\right)<br>\left(\begin{matrix}{\frac{1}{\sqrt{2}}}&amp;{\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}}&amp;{-\frac{1}{\sqrt{2}}} \end{matrix}\right)<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pattern recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PR-ch04-NLC</title>
      <link href="/2024/05/04/PR-ch04-NLC/"/>
      <url>/2024/05/04/PR-ch04-NLC/</url>
      
        <content type="html"><![CDATA[<img src="/2024/05/04/PR-ch04-NLC/problem.png" class><hr><p>Solution:</p><img src="/2024/05/04/PR-ch04-NLC/1.png" class><img src="/2024/05/04/PR-ch04-NLC/2.png" class><img src="/2024/05/04/PR-ch04-NLC/3.png" class><img src="/2024/05/04/PR-ch04-NLC/4.png" class><img src="/2024/05/04/PR-ch04-NLC/5.png" class><img src="/2024/05/04/PR-ch04-NLC/6.png" class><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><img src="/2024/05/04/PR-ch04-NLC/cover.png" class><ul><li>Sergios Theodoridis Konstantinos Koutroumbas Pattern Recognition. 4th Edition. Springer, 2010.</li></ul>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pattern recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PR-ch03-LC</title>
      <link href="/2024/05/04/PR-ch03-LC/"/>
      <url>/2024/05/04/PR-ch03-LC/</url>
      
        <content type="html"><![CDATA[<img src="/2024/05/04/PR-ch03-LC/problem.png" class><hr><p>Solution:</p><img src="/2024/05/04/PR-ch03-LC/1.png" class><img src="/2024/05/04/PR-ch03-LC/2.png" class><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><img src="/2024/05/04/PR-ch03-LC/cover.png" class><ul><li>Sergios Theodoridis Konstantinos Koutroumbas Pattern Recognition. 4th Edition. Springer, 2010.</li></ul>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pattern recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PR-ch02-Bayes</title>
      <link href="/2024/04/30/PR-ch02-Bayes/"/>
      <url>/2024/04/30/PR-ch02-Bayes/</url>
      
        <content type="html"><![CDATA[<hr><p><strong>Problem 2.2:</strong> In a two-class one-dimensional problem, the pdfs are the Gaussians $\mathcal{N}(0,\sigma^2)$ and $\mathcal{N}(1,\sigma^2)$ for the two classes, respectively. Show that the threshold $x_0$ minimizing the average risk is equal to<br>$$x_0&#x3D;1&#x2F;2-\sigma^2\ln\frac{\lambda_{21}P(\omega_2)}{\lambda_{12}P(\omega_1)}$$<br>where $\lambda_{11}&#x3D;\lambda_{22}&#x3D;0$ has been assumed.</p><hr><p>Solution: In a two-class problem:</p><p>$$R(\alpha_1|x)&#x3D;\lambda_{11}P(\omega_1|x)+\lambda_{21}P(\omega_2|x)$$</p><p>$$R(\alpha_2|x)&#x3D;\lambda_{12}P(\omega_1|x)+\lambda_{22}P(\omega_2|x)$$</p><p>The threshold $x_0$ minimizing the average risk where $R(\alpha_1|x_0)&#x3D;R(\alpha_2|x_0),$ then:</p><p>$$\lambda_{11}P(\omega_1|x_0)+\lambda_{21}P(\omega_2|x_0)&#x3D;\lambda_{12}P(\omega_1|x_0)+\lambda_{22}P(\omega_2|x_0)$$</p><p>$$\Rightarrow\frac{P(\omega_1|x_0)}{P(\omega_2|x_0)}&#x3D;\frac{p(x_0|\omega_1)P(\omega_1)}{p(x_0|\omega_2)P(\omega_2)}&#x3D;\frac{\lambda_{12}-\lambda_{22}}{\lambda_{21}-\lambda_{11}}$$</p><p>To minimize the average risk, we have<br>$$\frac{P(x_0|\omega_1)}{P(x_0|\omega_2)}&#x3D;\frac{\lambda_{12}-\lambda_{22}}{\lambda_{21}-\lambda_{11}}\frac{P(\omega_2)}{P(\omega_1)}$$<br>and $\lambda_{11}&#x3D;\lambda_{22}&#x3D;0$,then</p><p>$$\frac{P(x_0|\omega_1)}{P(x_0|\omega_2)}&#x3D;\frac{\lambda_{12}}{\lambda_{21}}\frac{P(\omega_2)}{P(\omega_1)}$$</p><p>taking the logarithm of both sides,$$lnP(x_{0}|\omega_{1})-lnP(x_{0}|\omega_{2})&#x3D;ln\frac{\lambda_{12}P(\omega_{2})}{\lambda_{21}P(\omega_{1})}$$</p><p>since $P(x_{0}|\omega_{1}){\sim}N(0,\sigma^{2}),$</p><p>$$P(x_{0}|\omega_{1})&#x3D;\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x_{0}^{2}}{2\sigma^{2}}}\quad\Rightarrow{lnP(x_{0}|\omega_{1})}&#x3D;-ln(\sqrt{2\pi}\sigma)-\frac{x_{0}^{2}}{2\sigma^{2}}$$</p><p>and $P(x_{0}|\omega_{2}){\sim}N(1,\sigma^{2}),$</p><p>$$P(x_{0}|\omega_{2})&#x3D;\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_{0}-1)^{2}}{2\sigma^{2}}}\quad\Rightarrow{lnP(x_{0}|\omega_{2})}&#x3D;-ln(\sqrt{2\pi}\sigma)-\frac{(x_{0}-1)^{2}}{2\sigma^{2}}$$</p><p>then, $lnP(x_0|\omega_1)-lnP(x_0|\omega_2)&#x3D;-ln(\sqrt{2\pi}\sigma)-\frac{x_0^2}{2\sigma^2}-(-ln(\sqrt{2\pi}\sigma)-\frac{(x_0-1)^2}{2\sigma^2})$</p><p>$$lnP(x_0|\omega_1)-lnP(x_0|\omega_2)&#x3D;-\frac{(x_0-1)^2}{2\sigma^2}-\frac{x_0^2}{2\sigma^2}&#x3D;ln\frac{\lambda_{12}P(\omega_2)}{\lambda_{21}P(\omega_1)}$$</p><p>$$(x_{0}-1)^{2}-x_{0}^{2}&#x3D;2\sigma^{2}ln\frac{\lambda_{12}P(\omega_{2})}{\lambda_{21}P(\omega_{1})}$$</p><p>$$x_{0}^{2}-2x_{0}+1-x_{0}^{2}&#x3D;2\sigma^{2}ln\frac{\lambda_{12}P(\omega_{2})}{\lambda_{21}P(\omega_{1})}$$</p><p>$$-2x_{0}+1&#x3D;2\sigma^{2}ln\frac{\lambda_{12}P(\omega_{2})}{\lambda_{21}P(\omega_{1})}$$</p><p>$$\mathrm{thus},\quad x_0&#x3D;\frac{1}{2}-\sigma^2ln\frac{\lambda_{12}P(\omega_2)}{\lambda_{21}P(\omega_1)}$$</p><hr><p><strong>Problem 2.5:</strong> Consider a two (equiprobable) class, one-dimensional problem with samples distributed according to the Rayleigh pdf in each class, that is,<br>$$p(x|\omega_i)&#x3D;\begin{cases}\frac{x}{\sigma_i^2}\exp\left(\frac{-x^2}{2\sigma_i^2}\right)&amp;x\geq0 \\ 0&amp;x&lt;0\end{cases}$$<br>Compute the decision boundary point $g(x)&#x3D;0.$</p><hr><p>Solution: The decision boundary point corresponds to</p><p>$$\frac{x_0}{\sigma_1^2}\exp(\frac{-x_0^2}{2\sigma_1^2})&#x3D;\frac{x_0}{\sigma_2^2}\exp(\frac{-x_0^2}{2\sigma_2^2})$$</p><p>or by taking the logarithm</p><p>$$\frac{-x_0^2}{2\sigma_1^2}&#x3D;\ln\frac{\sigma_1^2}{\sigma_2^2}-\frac{x_0^2}{2\sigma_2^2}$$</p><p>and finally</p><p>$$x_0&#x3D;\sqrt{\frac{2\sigma_1^2\sigma_2^2}{\sigma_1^2-\sigma_2^2}\ln\frac{\sigma_1^2}{\sigma_2^2}}$$</p><hr><p><strong>Problem 2.7:</strong> In a three-class,two-dimensional problem the feature vectors in each class are normally distributed with covariance matrix<br>$$\Sigma&#x3D;\begin{bmatrix}1.2&amp;0.4 \\ 0.4&amp;1.8\end{bmatrix}$$<br>The mean vectors for each class are $[0.1,0.1]^T,[2.1,1.9]^T,[-1.5,2.0]^T.$ Assuming that the classes are equiprobable,<br>(a) classify the feature vector [1.6,1.5]$^T$ according to the Bayes minimum error probability classifier;<br>(b) draw the curves of equal Mahalanobis distance from [2.1,1.9]$^{\acute{T}}.$</p><hr><p>Solution: </p><p>(a) It suffices to compute the Mahalanobis distance of $[1.6,1.5]^T$ from mean vectors of the classes. We have:</p><p>$$\Sigma^{-1}&#x3D;\left[\begin{array}{cc}0.9&amp;0.2 \\ -0.2&amp;0.6\end{array}\right]$$</p><p>$$|\Sigma|&#x3D;1.2\times1.8-0.4\times0.4&#x3D;2,\Sigma^{-1}&#x3D;\frac{1}{|\Sigma|}\Big[\begin{matrix}1.8&amp;-0.4 \\ -0.4&amp;1.2\end{matrix}\Big]&#x3D;\Big[\begin{matrix}0.9&amp;-0.2 \\ -0.2&amp;0.6\end{matrix}\Big]$$</p><p>Thus, $d_1^2&#x3D;2.361,d_2^2&#x3D;0.241,d_3^2&#x3D;9.416$</p><p>Hence $[1.6,1.5]^T$ is assigned to $\mathcal{w_2}$</p><p>(b) According to theory it suffices to compute the eigenvalues and eigenvectors of $\Sigma$. There are </p><p>$$\lambda_{1}&#x3D;1,\lambda_{2}&#x3D;2 \\ v_{1}&#x3D;[0.89,-0.45]^{T} \\ v_{2}&#x3D;[0.45,0.89]^{T}$$</p><p>Thus the ellipse, centered at $\mu_2$ and axis</p><p>$2\sqrt{\lambda_1}c\boldsymbol{v}_1$ and $2\sqrt{\lambda_2}c\boldsymbol{v}_2$</p><hr><p><strong>Problem 2.12:</strong> Consider a two-class, two-dimensional classification task, where the feature vectors in each of the classes $\omega_1,\omega_{2}$ are distributed according to</p><p>$$p(x|\omega_1)&#x3D;\frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\biggl(-\frac{1}{2\sigma_1^2}(x-\mu_1)^T(x-\mu_1)\biggr)$$</p><p>$$p(x|\omega_2)&#x3D;\dfrac{1}{\sqrt{2\pi\sigma_2^2}}\exp\biggl(-\dfrac{1}{2\sigma_2^2}(x-\mu_2)^T(x-\mu_2)\biggr)$$</p><p>with</p><p>$$\mu_{1}&#x3D;[1,1]^{T},\mu_{2}&#x3D;[1.5,1.5]^{T},\sigma_{1}^{2}&#x3D;\sigma_{2}^{2}&#x3D;0.2$$</p><p>Assume that $P(\omega_1)&#x3D;P(\omega_2)$ and design a Bayesian classifier<br>(a) that minimizes the error probability<br>(b) that minimizes the average risk with loss matrix</p><p>$$\Lambda&#x3D;\begin{bmatrix}0&amp;1 \\ 0.5&amp;0\end{bmatrix}$$</p><p>Using a pseudorandom number generator, produce 100 feature vectors from each class, according to the preceding pdfs. Use the classifiers designed to classify the generated vectors. What is the percentage error for each case? Repeat the experiments for $\mu_{2}&#x3D;[3.0,3.0]^{T}.$</p><hr><p>Solution:</p><p>(a) For the two-class classification, if $P(\omega_1)&#x3D;P(\omega_2)$ and $\lambda_{11}&#x3D;\lambda_{22}&#x3D;0$,</p><p>The Bayesian classifier is: $x\to\omega_{1}$ if $P(x|\omega_{1})&gt;P(x|\omega_{2})\frac{\lambda_{12}}{\lambda_{21}}$</p><p>If $\lambda_{12}&#x3D;\lambda_{21}$ , the Bayesian classifier minimizes the error probability. Thus, the Bayesian<br>classifier is: $x\to\omega_{1}$ if $P(x|\omega_{1})&gt;P(x|\omega_{2})$</p><p>We have</p><p>$$p(x|\omega_1)&#x3D;\frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\biggl(-\frac{1}{2\sigma_1^2}(x-\mu_1)^T(x-\mu_1)\biggr)$$</p><p>$$p(x|\omega_2)&#x3D;\dfrac{1}{\sqrt{2\pi\sigma_2^2}}\exp\biggl(-\dfrac{1}{2\sigma_2^2}(x-\mu_2)^T(x-\mu_2)\biggr)$$</p><p>From $\sigma_1^2&#x3D;\sigma_2^2&#x3D;\sigma^2,|x-\mu_1|^2&#x3D;(x-\mu_1)^T(x-\mu_1)$ and $|x-\mu_2|^2&#x3D;(x-\mu_2)^T(x-\mu_2)$, we have</p><p>$$P(x|\omega_1)&#x3D;\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{|x-\mu_1|^2}{2\sigma^2}\right),\quad P(x|\omega_2)&#x3D;\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{|x-\mu_2|^2}{2\sigma^2}\right)$$</p><p>$P(x|\omega_1)&gt;P(x|\omega_2)$ is equivalent to $\ln P(x|\omega_1)&gt;\ln P(x|\omega_2)$</p><p>$$\ln P(x|\omega_1)&#x3D;\ln\frac{1}{\sqrt{2\pi\sigma^2}}-\frac{|x-\mu_1|^2}{2\sigma^2},\quad\ln P(x|\omega_2)&#x3D;\ln\frac{1}{\sqrt{2\pi\sigma^2}}-\frac{|x-\mu_2|^2}{2\sigma^2}$$</p><p>$P(x|\omega_1)&gt;P(x|\omega_2)$ is equivalent to $|x-\mu_1|&lt;|x-\mu_2|$</p><p>Thus, for the Bayesian classifier minimizing the error probability is equivalent to the<br>classifier minimizing the Euclidean distance, namely,</p><p>$$x\to\omega_1\text{ if }|x-\mu_1|&lt;|x-\mu_2|$$</p><p>(b) In this case, $x$ is classified to $P(x|\omega_{1})&gt;P(x|\omega_{2})\frac{\lambda_{12}}{\lambda_{21}}$</p><p>where $\lambda_{12}&#x3D;1$ and $\lambda_{21}&#x3D;0.5$. Thus following similar arguments as in theory, for Bayesian<br>classification for normal distributions, we conclude that the decision hyperplane is</p><p>$$g_{12}(x)&#x3D;w^T(x-x_0) \\ w&#x3D;\mu_{1}-\mu_{2}$$</p><p>and</p><p>$$x_0&#x3D;\frac{1}{2}(\mu_1+\mu_2)-\sigma^2\ln\frac{\lambda_{21}P(\omega_1)}{\lambda_{12}P(\omega_2)}\frac{\mu_1-\mu_2}{|\mu_1-\mu_2|^2}$$</p><p>(c) The following MATLAB function takes as input the variance (s), the mean ( m) and the number of samples N. The output is a vector 1xN, whose ele- ments are the N samples of the 1-D Gaussian. For 2-D independent variables, combine two samples generated above, in a single vector</p><p>$$x&#x3D;[x_1,x_2]^T$$</p><pre><code class="matlab">Function x=gaussian(m,s,N);x=randn(1,N);x=x*sqrt(s)+m;</code></pre><hr><p><strong>Problem 2.17ï¼š</strong> In a heads or tails coin-tossing experiment the probability of occurrence of a head (1) is $q$ and that of a tail (0) is $1-q$. Let $x_i,i &#x3D;1,2,â€¦,N$, be the resulting experimental outcomes, ğ‘¥ğ‘– âˆˆ {0,1}. Show that the ML estimate of $q$ is</p><p>$$q_{ML}&#x3D;\frac1N\sum_{i&#x3D;1}^Nx_i$$</p><p>Hint: The likelihood function is</p><p>$$P(X;q)&#x3D;\prod_{i&#x3D;1}^Nq^{x_i}(1-q)^{(1-x_i)}$$</p><p>Then show that the ML results from the solution of the equation</p><p>$$q^{\sum_{i}x_{i}}(1-q)^{(N-\sum_{i}x_{i})}\left(\frac{\sum_{i}x_{i}}{q}-\frac{N-\sum_{i}x_{i}}{1-q}\right)&#x3D;0$$</p><hr><p>Solution:</p><img src="/2024/04/30/PR-ch02-Bayes/6.png" class><img src="/2024/04/30/PR-ch02-Bayes/7.png" class><hr><p><strong>Problem 2.29ï¼š</strong> Show that for the lognormal distribution</p><p>$$p(x)&#x3D;\frac{1}{\sigma x\sqrt{2\pi}}\exp\left(-\frac{(\ln{x}-\theta)^{2}}{2\sigma^{2}}\right),x&gt;0$$</p><p>The ML estimate is given by</p><p>$$\theta_{ML}&#x3D;\frac{1}{N}\sum_{k&#x3D;1}^{N}\ln{x_k}$$</p><hr><p>Solution:</p><img src="/2024/04/30/PR-ch02-Bayes/8.png" class><hr><p><strong>Problem:</strong> Consider two normal distributions in one dimension: $N(\mu_1,\sigma_1^2)$ and $N(\mu_2,\sigma_2^2).$ Imagine that we choose two random samples $x_1$ and $x_2$,one from each of the normal distributions and calculate their sum $x_3&#x3D;x_1+x_2.$ Suppose we do this repeatedly.</p><p>(a) Consider the resulting distribution of the values of $x_3.$ Show from frst principles<br>that this is also a normal distribution.</p><p>(b) What is the mean,  $\mu_{3}$, of your new distribution?</p><p>(c) What is the variance, $\sigma_3^2?$</p><p>(d) Repeat the above with two distributions in a multi-dimensional space, i.e., $N(\mu_1,\Sigma_1)$ and $N(\mu_2,\Sigma_2).$</p><hr><p>Solution:</p><img src="/2024/04/30/PR-ch02-Bayes/1.png" class><img src="/2024/04/30/PR-ch02-Bayes/2.png" class><img src="/2024/04/30/PR-ch02-Bayes/3.png" class><img src="/2024/04/30/PR-ch02-Bayes/4.png" class><img src="/2024/04/30/PR-ch02-Bayes/5.png" class><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><img src="/2024/04/30/PR-ch02-Bayes/cover.png" class><ul><li>Sergios Theodoridis Konstantinos Koutroumbas Pattern Recognition. 4th Edition. Springer, 2010.</li></ul>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pattern recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kmeans++</title>
      <link href="/2024/04/24/Kmeans/"/>
      <url>/2024/04/24/Kmeans/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In this notebook, we shall be looking at how the kmeans algorithm works. KMeans is an <strong>unsupervised learning</strong> algorithm that is used to cluster data in groups - without knowing which group given data elements belong to as are going to see.</p><pre><code class="python">import numpy as npimport sklearn.datasetsimport matplotlib.pyplot as pltimport matplotlib as mpl</code></pre><p>Letâ€™s shall generate a random dataset of 1000 points clustered into 3 groups.</p><pre><code class="python"># è®¾ç½®æ•°æ®é›†ä¸­æ ·æœ¬ç‚¹çš„æ•°é‡N = 1000# ç”Ÿæˆå…·æœ‰3ä¸ªä¸­å¿ƒç‚¹çš„èšç±»æ•°æ®é›†X_, y_ = sklearn.datasets.make_blobs(n_samples=N+5, centers=3) # ä»ç”Ÿæˆçš„æ•°æ®é›†ä¸­æå–å‰Nä¸ªæ ·æœ¬ä½œä¸ºè®­ç»ƒæ•°æ®X, y = X_[:N], y_[:N]# ä»ç”Ÿæˆçš„æ•°æ®é›†ä¸­æå–åé¢5ä¸ªæ ·æœ¬ä½œä¸ºæµ‹è¯•æ•°æ®X_test, y_test = X_[N:], y_[N:]# ç»˜åˆ¶æ•°æ®é›†çš„æ•£ç‚¹å›¾plt.figure(figsize=(8, 6))for cls in np.unique(y):    plt.scatter(X[y==cls][:, 0], X[y==cls][:, 1], s=2)plt.title(&quot;Plot of features of dataset X&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><!-- ![sampla dataset](./Kmeans/sample_dataset_X.png) --><img src="/2024/04/24/Kmeans/sample_dataset_X.png" class><h2 id="1-1-First-things-first"><a href="#1-1-First-things-first" class="headerlink" title="1.1 First things first"></a>1.1 First things first</h2><ol><li><p>We need to determine and set a value k, the number of clusters we <strong>think</strong> the data has. KMeans is unsupervised. So it is not the case that you will always know how many clusters (k) exist in the data. You will have to experiment with different values using certain techniques to find the best value of k. For our case, we know that there are 3 clusters, therefore we shall set k to 3. (You can try a different value after the points get clear).</p></li><li><p>We also need to <strong>find k&#x3D;3 random points</strong> that will represent the centers of our clusters if the clustering is successfull. These k random points are called centroids.Letâ€™s work on these two steps next.</p></li></ol><pre><code class="python">k = 3N, feature_size = X.shape# è·å–ç‰¹å¾çš„æœ€å¤§å€¼å’Œæœ€å°å€¼èŒƒå›´min_feature_range = np.min(X, axis=0)max_feature_range = np.max(X, axis=0)# åœ¨ä¸Šè¿°èŒƒå›´å†…ç”Ÿæˆkä¸ªéšæœºç‚¹centroids = np.zeros((len(max_feature_range), k))for i, (l, h) in enumerate(zip(min_feature_range, max_feature_range)):    # ä½¿ç”¨random.uniform()å‡½æ•°ä»å‡åŒ€åˆ†å¸ƒä¸­æŠ½å–æ ·æœ¬    centroids[i, :] = np.random.uniform(low=l, high=h, size=k)# è½¬ç½®centroidsçŸ©é˜µï¼Œä½¿å¾—æ¯è¡Œè¡¨ç¤ºä¸€ä¸ªèšç±»ä¸­å¿ƒcentroids = centroids.Tprint(centroids)# ç»˜åˆ¶æ•°æ®é›†çš„æ•£ç‚¹å›¾å’Œèšç±»ä¸­å¿ƒplt.figure(figsize=(8, 6))for cls in np.unique(y):    plt.scatter(X[y==cls][:, 0], X[y==cls][:, 1], s=2)# ç»˜åˆ¶èšç±»ä¸­å¿ƒfor i, (x_, y_) in enumerate(centroids):    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=&#39;r&#39;)plt.title(&quot;Plot of features of dataset X&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><!-- ![1.1](./Kmeans/1.1.png) --><img src="/2024/04/24/Kmeans/1.1.png" class><p>The points are scattered and may not be close to the cluster centers. There are other methods like the <strong>kmeans++</strong>.</p><h2 id="1-2-Calculate-distances-to-centroids"><a href="#1-2-Calculate-distances-to-centroids" class="headerlink" title="1.2 Calculate distances to centroids"></a>1.2 Calculate distances to centroids</h2><p>In this notebook, we shall be calculating the euclidean distance. We have 2 columns in X <em>(x1 and x2)</em> and we have to calculate the euclidean distance between each centroid and every data point in X. Centroids are of the form <em>(xx, yy)</em> i.e they have two points just like our dataset X has 2 columns.</p><p>è®¡ç®—æ¯ä¸ªè´¨å¿ƒå’Œ X ä¸­æ¯ä¸ªæ•°æ®ç‚¹ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»ã€‚</p><p>We want to calculate something of the form $sqrt((x1-xx)^2 + (x2-yy)^2)$ for each data point&#x2F;row in X.<br>To accelerate operations, we shall be using a vectorized approach to calculate that.</p><p>ä½¿ç”¨çŸ¢é‡åŒ–æ–¹æ³•æ¥è®¡ç®—ã€‚</p><ol><li>We have k&#x3D;3 centroids, so we shall first duplicate <strong>X</strong> 3 times or k times. The shape of X is (1000, 2). The result of the duplication, <strong>Xc</strong> will be (1000, 6).</li><li>Then we shall flatten the centroids so that its a single vector, <strong>centroidsc</strong> of 6 elements to match our 6 columns in <strong>Xc</strong>.</li><li>We shall subtract <strong>Xc</strong> and <strong>centroidsc</strong> to give us a result <strong>D</strong> of shape (1000, 6). This step is equivalent to performing <strong>(x1-xx)</strong> and <strong>(x2-yy)</strong> for all centroids at once.</li></ol><ul><li>The first column of <strong>D</strong> corresponds to <strong>(x1-xx)</strong> where xx is the x of the first centroid.</li><li>The second column of <strong>D</strong> corresponds to <strong>(x2-yy)</strong> where yy is the y of the first centroid.</li><li>The third column of <strong>D</strong> corresponds to <strong>(x1-xx)</strong> where xx is the x of the <em>second</em> centroid.</li><li>The forth column of <strong>D</strong> corresponds to <strong>(x2-yy)</strong> where yy is the y of the <em>second</em> centroid. And so on.</li></ul><p>4.The next step is to square these results, add them and apply sqrt. This whole operation results in what is called the <em>L2 norm</em> and is all performed by the <strong>np.linalg.norm</strong> function.</p><p>Note that we have to calculate the norm over a given set of columns e.g the first and second columnsâ€™ norm corresponds to the first centroid, the third and forth to the 2nd centroid and the last 2 to the 3rd centroid. So in the end we have a (1000, 3) array containing euclidean distances of each of the 1000 data rows&#x2F;points in X in correspondence to each of the 3 centroids.</p><pre><code class="python">Xc = np.concatenate([X for c in centroids], axis=1) # duplicate X k timescentroidsc = centroids.ravel() # ravel to allow broadcast; Return a contiguous flattened array.D = (Xc - centroidsc) # raw diffNorms = np.zeros((N, k)) # distances to each centroidfor i in range(0, k):     m = i*feature_size    Norms[:, i] = np.linalg.norm(D[:, m:m+feature_size], axis=1) # Calculating the norms (Euclidean distance)</code></pre><h2 id="1-3-Attach-instances-or-rows-to-the-closest-centroid"><a href="#1-3-Attach-instances-or-rows-to-the-closest-centroid" class="headerlink" title="1.3 Attach instances or rows to the closest centroid"></a>1.3 Attach instances or rows to the closest centroid</h2><p>We shall now assign each data row in X the index of the centroid with which it has the shortest distance. We do that using <strong>np.argmin</strong> which returns the index of the minimum distance in our <em>Norms</em> array.</p><p>ä½¿ç”¨ <strong>np.argmin</strong> æ¥ä¸º X ä¸­çš„æ¯ä¸ªæ•°æ®è¡Œåˆ†é…ä¸å…¶è·ç¦»æœ€çŸ­çš„è´¨å¿ƒçš„ç´¢å¼•ï¼Œå®ƒè¿”å› <em>Norms</em> æ•°ç»„ä¸­æœ€å°è·ç¦»çš„ç´¢å¼•ã€‚</p><pre><code class="python"># sample of indices for smallest indices for sample_normsnp.argmin(sample_norms, axis=1)# we do this for all Normsypred = np.argmin(Norms, axis=1) #Returns the indices of the minimum values along an axis.plt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids):  # with underscore    p = plt.scatter(X[y==i][:, 0], X[y==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    anot = &#39;c&#39;+str(i) + &quot; at &quot; + str(np.round(Norms[0, i], 3))    plt.plot([X[0][0], x_], [X[0][1], y_])    plt.annotate(xy=(x_+.2, y_-.1), text=anot, color=&#39;k&#39;, size=10)    plt.scatter(X[0][0], X[0][1], marker=&#39;o&#39;, c=&#39;r&#39;, s=35)plt.title(&quot;Plot showing distance sample (point belongs to c&quot;+str(ypred[0])+&quot;)&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><!-- ![1.3](./Kmeans/1.3.png) --><img src="/2024/04/24/Kmeans/1.3.png" class><h2 id="1-4-Update-centroids"><a href="#1-4-Update-centroids" class="headerlink" title="1.4 Update centroids"></a>1.4 Update centroids</h2><p>The last step is to update the centroids by setting the new centroids at the mean positions of the points they were closest to i.e the data points they influence.</p><p>Below is a plot showing the un-updated centroids and their influence on the data points. We shall have to move the centroids so that they are at the center of the points they influence.</p><pre><code class="python">plt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids):    p = plt.scatter(X[ypred==i][:, 0], X[ypred==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=clr, size=14)plt.title(&quot;Plot showing influence of recent centroids on data points&quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><!-- ![1.4.1](./Kmeans/1.4.1.png) --><img src="/2024/04/24/Kmeans/1.4.1.png" class><ul><li>To do that, we shall calculate the mean of the data points each centroid influences and put the centroid at that mean location.</li><li>If a centroid has no points it influences (yes, this can happen), we leave the centroid where it is.</li></ul><pre><code class="python">centroids_ = []for i in range(k):    if len(X[ypred == i]) == 0:        centroids_.append(centroids[i]) # use old    else:        centroids_.append(np.mean(X[ypred == i], axis=0))centroids_ = np.array(centroids_)centroids_#After the update, we have the following plotplt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids_):  # with underscore    p = plt.scatter(X[ypred==i][:, 0], X[ypred==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=clr, size=14)plt.title(&quot;Plot showing mean-centered centroids &quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><img src="/2024/04/24/Kmeans/1.4.2.png" class><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><ul><li>The mean-centered points may look good, however the data they influence may still be bad</li></ul><h2 id="1-5-Repeat-the-above-steps"><a href="#1-5-Repeat-the-above-steps" class="headerlink" title="1.5 Repeat the above steps"></a>1.5 Repeat the above steps</h2><ul><li>Now we repeat the steps above until the centroids donâ€™t update or move (significantly) anymore. That will be the case when <strong>(centroids_ - centroids)^2</strong> is a low value below a certain threshold. A good threshold has to be as low as possible i.e close to or equal to zero.</li><li>You can run the following code multiple times and observe <strong>shift</strong> value (which is <strong>changes of centroids</strong> ) carefully</li><li>Here, we are using <strong>^2</strong> to make larger shifts&#x2F;updates significant. It doesnâ€™t have to be that way. A norm can also work.</li></ul><pre><code class="python">shift = np.sum((centroids_ - centroids)**2)print(shift)# Below we repeat all the previous steps in one runcentroids = centroids_Xc = np.concatenate([X for c in centroids], axis=1) # duplicate k timescentroidsc = centroids.ravel() # ravel to allow broadcastD = (Xc - centroidsc) # raw diffNorms = np.zeros((N, k)) # distances to each clusterfor i in range(0, k):    m = i*feature_size    Norms[:, i] = np.linalg.norm(D[:, m:m+feature_size], axis=1)# Choose the nearest cluster for every point, and save the result in ypred which is used in next code segment# We can use np.argmin functionypred = np.argmin(Norms, axis=1) # new clusters are mean X along centroids_ = []for i in range(k):    if len(X[ypred == i]) == 0:        centroids_.append(centroids[i]) # use old    else:        centroids_.append(np.mean(X[ypred == i], axis=0))centroids_ = np.array(centroids_)centroids_plt.figure(figsize=(8, 6))for i, (x_, y_) in enumerate(centroids_):  # with underscore    p = plt.scatter(X[ypred==i][:, 0], X[ypred==i][:, 1], s=2)    clr = mpl.colors.to_rgba(p.get_facecolor()) # get color used by matplotlib    plt.scatter(x_, y_, marker=&#39;x&#39;, c=&#39;k&#39;)    plt.annotate(xy=(x_+.1, y_-.1), text=&#39;c&#39;+str(i), color=clr, size=14)plt.title(&quot;Plot showing mean-centered centroids &quot;, fontsize=14)plt.xlabel(&quot;x1&quot;, fontsize=12)plt.ylabel(&quot;x2&quot;, fontsize=12)plt.show()</code></pre><img src="/2024/04/24/Kmeans/1.5.png" class>]]></content>
      
      
      <categories>
          
          <category> ç®—æ³• </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shortest Path â…¡</title>
      <link href="/2024/04/17/Shortest-Path-%E2%85%A1/"/>
      <url>/2024/04/17/Shortest-Path-%E2%85%A1/</url>
      
        <content type="html"><![CDATA[<p>In the previous article <a href="https://sheldoncoder1337.github.io/2024/04/17/Shortest-Path-%E2%85%A0/">Shortest-Path-â… </a>, we have already learn how to use Networkx and Pandana <code>shortest_path</code> API to find the shortest path on <a href="https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc">New York</a> <a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page">New York Taxi Trip</a> dataset. And after comparing the performance between Dijkstra and Constraction Hierarchy algorithm, we could found that CH have a much better performance than classic Dijkstra algorithm.</p><p>In this blog, letâ€™s try to fix the Carpool problem.</p><h2 id="Location-Statistics-Heat-Map-Visualization"><a href="#Location-Statistics-Heat-Map-Visualization" class="headerlink" title="Location Statistics &amp; Heat Map Visualization"></a>Location Statistics &amp; Heat Map Visualization</h2><pre><code class="python">import pandas as pdimport plotly.express as px# Data with latitude/longitude and valuesdf = pd.read_csv(&#39;https://raw.githubusercontent.com/R-CoderDotCom/data/main/sample_datasets/population_galicia.csv&#39;)fig = px.density_mapbox(df, lat = &#39;latitude&#39;, lon = &#39;longitude&#39;, z = &#39;tot_pob&#39;,                        radius = 7,                        center = dict(lat = 42.83, lon = -8.35),                        zoom = 6,                        mapbox_style = &#39;open-street-map&#39;,                        color_continuous_scale = &#39;rainbow&#39;,                        opacity = 0.5)fig.show()</code></pre><h2 id="Carpool-problem"><a href="#Carpool-problem" class="headerlink" title="Carpool problem"></a>Carpool problem</h2><p>With the rise of taxi-hailing mobile programs (such as uber), a New York cab driver is used to take orders from online platform. Given the initial location of the driver and 2-3 orders (e.g., each order is a 6-tuple, like a record in the NY Taxi data), your task is to find a feasible route to pick up all the orders.</p><p>For instance, the driver is now at location Time Square, he is assigned to pick up three passengers.</p><ul><li>passengerA: JFK_Airport to East_Chelsea</li><li>passengerB: West_Village to East_Chelsea</li><li>passengerC: Battery_Park_City to Queens_Plaza</li></ul><p>One feasible solution is to report the route from</p><ul><li>Time Square -&gt; JFK_Airport -&gt; East_Chelsea -&gt; West_Village -&gt; East_Chelsea -&gt; Battery_Park_City -&gt; Queens_Plaza</li></ul><h3 id="Our-target"><a href="#Our-target" class="headerlink" title="Our target"></a>Our target</h3><ol><li>Write a function to determine the route and the total distance of the route.</li><li>Plot the route on the map.</li></ol><p>Obviously, the feasible solution is far from optimal as East_Chelsea is the common locations of two pessagers. Thereby,  </p><h3 id="Bonus-task"><a href="#Bonus-task" class="headerlink" title="Bonus task"></a>Bonus task</h3><ul><li>Try to find the best route based on the given orders. For the bonus part, please explain your methodology and your mark will be given based on the soundness of your idea, the quality of analysis, and the implementation.</li></ul><pre><code class="python"></code></pre>]]></content>
      
      
      <categories>
          
          <category> ç®—æ³• </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm, find-the-shortest-path </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shortest Path â… </title>
      <link href="/2024/04/17/Shortest-Path-%E2%85%A0/"/>
      <url>/2024/04/17/Shortest-Path-%E2%85%A0/</url>
      
        <content type="html"><![CDATA[<p>In this series, I will introduce some third-party libraries such as osmnx, pandana, geopandas and compare the performance between <strong>NetworkX(Dijkstra)</strong> and <strong>Pandana( Constraction Hierarchy)</strong>. Finally, I will show how to use these libraries to solve a <strong>Carpool(æ‹¼è½¦) problem</strong>. The data set used in this article is from <a href="https://www.openstreetmap.org/">OpenStreetMap</a> - New York City Taxi Trip data set.</p><h2 id="preliminary"><a href="#preliminary" class="headerlink" title="preliminary"></a>preliminary</h2><p>You are highly recommended to use Conda to setup a new virtual environment.</p><pre><code class="bash">conda create -n geospatial python==3.8conda activate geospatialpip install geopandas network osmnet osmnx pandas pandana</code></pre><p>If you received an error like â€œspatialindex_c-64.dll is missingâ€, try to use the following commands to resolve it.</p><pre><code class="bash">pip uninstall rtreepip install rtree</code></pre><pre><code class="python">import warningswarnings.filterwarnings(&#39;ignore&#39;)warnings.simplefilter(&#39;ignore&#39;)import osmnx as oximport numpy as npimport geopandas as gpdimport pandanaimport pandas as pdfrom time import timeimport matplotlib.pyplot as pltfrom IPython.display import display, clear_outputimport networkx as nximport momepy</code></pre><h2 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h2><pre><code class="python">def extract_graph(place=&#39;New York&#39;):    # try Chinese    # G = ox.graph_from_place(&#39;çº½çº¦&#39;, network_type=&#39;drive&#39;)    ox.config(log_console=True, use_cache=True)    G = ox.graph_from_place(place, network_type=&#39;drive&#39;)    return Gplace = &#39;New York&#39;G = extract_graph(place)ox.plot_graph(G, bgcolor=&quot;w&quot;, node_size=1, node_color=&quot;yellow&quot;, edge_color=&quot;#aaa&quot;)print(&quot;node count:&quot;, len(G.nodes()))print(&quot;edge count:&quot;, len(G.edges()))</code></pre><!-- ![New York Taxi Trip](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/New-York-Taxi-Trip.png?raw=true) --><img src="/2024/04/17/Shortest-Path-%E2%85%A0/New-York-Taxi-Trip.png" class><p>There are total node 55344 nodes and 139582 edges.</p><p>We process <a href="https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc">New York</a> <a href="https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page">New York Taxi Trip</a>  and provide Trips.txt (<a href="https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/sources/Shortest-Path/Trips.txt">Appendix</a>)</p><p>Trips.txt contains New York Taxi trajectory information for 10,000 lines, each containing six columns of information, the region name where the passengers are picked up(PName),the lon and lat of the region in which they are picked up(PLon PLat),the region name they are delivered(Dname) and in which the passenger is delivered(DLon DLat).</p><p>For example:</p><table><thead><tr><th>PName</th><th>PLon</th><th>PLat</th><th>DName</th><th>DLon</th><th>DLat</th></tr></thead><tbody><tr><td>Lincoln_Square_East</td><td>-73.97382133</td><td>40.73788468</td><td>Upper_East_Side_North</td><td>-73.91715837</td><td>40.8541322</td></tr><tr><td>Upper_East_Side_North</td><td>-73.91715837</td><td>40.8541322</td><td>Central_Harlem_North</td><td>-73.99804922</td><td>40.71156838</td></tr></tbody></table><h2 id="Find-the-Shortest-Path"><a href="#Find-the-Shortest-Path" class="headerlink" title="Find the Shortest Path"></a>Find the Shortest Path</h2><p>There are two ways to find the shortest path, please check the docs for more details:</p><ol><li><a href="https://networkx.org/documentation/stable/reference/algorithms/shortest_paths.html">NetworkX</a></li><li><a href="https://udst.github.io/pandana/">Pandana(CH)</a></li></ol><h3 id="NetworkX-Dijkstra"><a href="#NetworkX-Dijkstra" class="headerlink" title="NetworkX(Dijkstra)"></a>NetworkX(Dijkstra)</h3><pre><code class="python"># The first trip record is from Lincoln_Square_East to Upper_East_Side_Northnx_Lincoln_Square_East_id = ox.distance.nearest_nodes(G,Lincoln_Square_East_Location.x,Lincoln_Square_East_Location.y)[0]nx_Upper_East_Side_North_id = ox.distance.nearest_nodes(G,Upper_East_Side_North_Location.x,Upper_East_Side_North_Location.y)[0]# NetworkX shortest pathdef SP_NX(G,SID,TID):    return nx.shortest_path(G, source=SID, target=TID, method=&quot;dijkstra&quot;, weight=&#39;length&#39;)     #displayNX_PATH=SP_NX(G,nx_Lincoln_Square_East_id,nx_Upper_East_Side_North_id)    fig , ax = ox.plot_graph(G, bgcolor=&quot;w&quot;, node_size=1, node_color=&quot;gray&quot;, edge_color=&quot;#aaa&quot;,show=False,close=False)ax.scatter(-73.97382133,40.73788468,c=&#39;yellow&#39;,marker=&quot;s&quot;,alpha=1,zorder=4)ax.scatter(-73.91715837,40.8541322,c=&#39;blue&#39;,alpha=1,zorder=3)ox.plot_graph_route(G,NX_PATH,ax=ax,orig_dest_size=0,route_alpha=0.5,route_colors=&#39;r&#39;,route_linewidths=2,show=False,close=False)</code></pre><!-- ![shortest path networkx Dijkstra](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/Shortest-Path-NetworkX.png?raw=true) --><img src="/2024/04/17/Shortest-Path-%E2%85%A0/Shortest-Path-NetworkX.png" class><h3 id="Pandana-CH"><a href="#Pandana-CH" class="headerlink" title="Pandana(CH)"></a>Pandana(CH)</h3><pre><code class="python"># trans road network to pandana formatnodes,edges = ox.graph_to_gdfs(G,nodes=True,edges=True)edges = edges.reset_index()G_pan = pandana.Network(nodes[&#39;x&#39;], nodes[&#39;y&#39;], edges[&#39;u&#39;], edges[&#39;v&#39;], edges[[&#39;length&#39;]],twoway=False)# The first trip record is from Lincoln_Square_East to Upper_East_Side_NorthLincoln_Square_East_Location = pd.DataFrame(&#123;&#39;longitude&#39;:[-73.97382133], &#39;latitude&#39;: [40.73788468]&#125;)Lincoln_Square_East_Location = gpd.points_from_xy(Lincoln_Square_East_Location.longitude, Lincoln_Square_East_Location.latitude, crs=&quot;EPSG:4326&quot;)Upper_East_Side_North_Location = pd.DataFrame(&#123;&#39;longitude&#39;:[-73.91715837], &#39;latitude&#39;: [40.8541322]&#125;)Upper_East_Side_North_Location = gpd.points_from_xy(Upper_East_Side_North_Location.longitude, Upper_East_Side_North_Location.latitude, crs=&quot;EPSG:4326&quot;)pan_Lincoln_Square_East_id = G_pan.get_node_ids(Lincoln_Square_East_Location.x,Lincoln_Square_East_Location.y).iloc[0]pan_Upper_East_Side_North_id = G_pan.get_node_ids(Upper_East_Side_North_Location.x,Upper_East_Side_North_Location.y).iloc[0]# pandana shortest pathdef SP_PAN(G_pan,SID,TID):    return G_pan.shortest_path(SID,TID) #displayPAN_PATH=SP_PAN(G_pan,pan_Lincoln_Square_East_id,pan_Upper_East_Side_North_id)    fig , ax = ox.plot_graph(G, bgcolor=&quot;w&quot;, node_size=1, node_color=&quot;gray&quot;, edge_color=&quot;#aaa&quot;,show=False,close=False)ax.scatter(-73.97382133,40.73788468,c=&#39;yellow&#39;,marker=&quot;s&quot;,alpha=1,zorder=4)ax.scatter(-73.91715837,40.8541322,c=&#39;blue&#39;,alpha=1,zorder=3)ox.plot_graph_route(G,PAN_PATH,ax=ax,orig_dest_size=0,route_alpha=0.5,route_colors=&#39;r&#39;,route_linewidths=2,show=False,close=False)</code></pre><!-- ![shortest path pandana CH](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/Shortest-Path-Pandana-CH.png?raw=true) --><img src="/2024/04/17/Shortest-Path-%E2%85%A0/Shortest-Path-Pandana-CH.png" class><h3 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h3><pre><code class="python"># you should upload trips.txt to your jupyter notebook first pickup_name=[]pickup_lon=[]pickup_lat=[]disengaged_name=[]disengaged_lon=[]disengaged_lat=[]import csv # opening the CSV filewith open(&#39;trips.txt&#39;, mode =&#39;r&#39;)as file:     # reading the CSV file  csvFile = csv.reader(file)    # displaying the contents of the CSV file  for lines in csvFile:        pickup_name.append(lines[0])        pickup_lon.append(lines[1])        pickup_lat.append(lines[2])        disengaged_name.append(lines[3])        disengaged_lon.append(lines[4])        disengaged_lat.append(lines[5])pickup_info = pd.DataFrame(&#123;&#39;pickup_name&#39;:pickup_name,&#39;longitude&#39;:pickup_lon, &#39;latitude&#39;: pickup_lat&#125;)disengaged_info = pd.DataFrame(&#123;&#39;disengaged_name&#39;:disengaged_name,&#39;longitude&#39;:disengaged_lon, &#39;latitude&#39;: disengaged_lat&#125;)pickup_Location = gpd.points_from_xy(pickup_info.longitude, pickup_info.latitude, crs=&quot;EPSG:4326&quot;)disengaged_Location = gpd.points_from_xy(disengaged_info.longitude, disengaged_info.latitude, crs=&quot;EPSG:4326&quot;)pickup_id = G_pan.get_node_ids(pickup_Location.x,pickup_Location.y)disengaged_id = G_pan.get_node_ids(disengaged_Location.x,disengaged_Location.y)nx_pickup_id = list(ox.distance.nearest_nodes(G,pickup_Location.x,pickup_Location.y))nx_disengaged_id = list(ox.distance.nearest_nodes(G,disengaged_Location.x,disengaged_Location.y))time_PAN=[]time_NX=[]test=[1,5,10,50,100,200,300,500,1000] # the query sizeNX_BATCH_PATH=[]PAN_BATCH_PATH=[]# This is the loop for evaluating the time of NetworkXfor i in range(len(test)):        tik = time()    for j in range(test[i]):         NX_BATCH_PATH.append(nx.shortest_path(G,source=nx_pickup_id[j],target=nx_disengaged_id[j],method=&#39;dijkstra&#39;,weight=&#39;length&#39;))        tok = time()    time_NX.append(tok-tik)    print(&#39;when query size = &#39;,test[i],end=&#39; , &#39;)    print(&#39;Time of Networkx is : &#39;,time_NX[-1],end=&#39;s\n&#39;)# This is the loop for evaluating the time of Pandanafor i in range(len(test)):    tik = time()    for j in range(test[i]):        PAN_BATCH_PATH.append(G_pan.shortest_path(pickup_id[j],disengaged_id[j]))        tok = time()    time_PAN.append(tok-tik)    print(&#39;when query size = &#39;,test[i],end=&#39; , &#39;)    print(&#39;Time of Pandana is : &#39;,time_PAN[-1],end=&#39;s\n&#39;)fig = plt.figure()ax = fig.add_subplot(1, 1, 1) clear_output(wait = True)ax.plot(test,time_PAN,label=&#39;Panadana&#39;)ax.plot(test,time_NX,label=&#39;Networkx&#39;)plt.ylabel(&#39;computing time(s)&#39;)plt.xlabel(&#39;Number of Query&#39;)plt.legend()fig.show()</code></pre><!-- ![shortest path comparison](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/Shortest-Path/shortest-path-comparison.png?raw=true) --><img src="/2024/04/17/Shortest-Path-%E2%85%A0/shortest-path-comparison.png" class><p>Here, we use Batch evaluation between Dijkstra (NetworkX) and CH (Pandana), and the results shows that CH algor is much faster than classical Dijskra.</p>]]></content>
      
      
      <categories>
          
          <category> ç®—æ³• </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm, find-the-shortest-path </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo hand book</title>
      <link href="/2024/04/16/hexo-hand-book/"/>
      <url>/2024/04/16/hexo-hand-book/</url>
      
        <content type="html"><![CDATA[<h2 id="å‘å¸ƒæ–‡ç« "><a href="#å‘å¸ƒæ–‡ç« " class="headerlink" title="å‘å¸ƒæ–‡ç« "></a>å‘å¸ƒæ–‡ç« </h2><p>è¿›å…¥åšå®¢æ‰€åœ¨ç›®å½•ï¼Œå³é”®æ‰“å¼€Git Bash Hereï¼Œåˆ›å»ºåšæ–‡ï¼š</p><pre><code class="bash">hexo new &quot;article title&quot;</code></pre><p>ç„¶å source æ–‡ä»¶å¤¹ä¸­ä¼šå‡ºç°ä¸€ä¸ª My New Post.md æ–‡ä»¶ï¼Œå°±å¯ä»¥ä½¿ç”¨ Markdown ç¼–è¾‘å™¨åœ¨è¯¥æ–‡ä»¶ä¸­æ’°å†™æ–‡ç« äº†ã€‚</p><p>å†™å®Œåè¿è¡Œä¸‹é¢ä»£ç å°†æ–‡ç« æ¸²æŸ“å¹¶éƒ¨ç½²åˆ° GitHub Pages ä¸Šå®Œæˆå‘å¸ƒã€‚ä»¥åæ¯æ¬¡å‘å¸ƒæ–‡ç« éƒ½æ˜¯è¿™ä¸¤æ¡å‘½ä»¤ã€‚</p><pre><code class="bash">hexo g   # ç”Ÿæˆé¡µé¢hexo d   # éƒ¨ç½²å‘å¸ƒ</code></pre><p>ä¹Ÿå¯ä»¥ä¸ä½¿ç”¨å‘½ä»¤è‡ªå·±åˆ›å»º .md æ–‡ä»¶ï¼Œåªéœ€åœ¨æ–‡ä»¶å¼€å¤´æ‰‹åŠ¨åŠ å…¥å¦‚ä¸‹æ ¼å¼ Front-matter å³å¯ï¼Œå†™å®Œåè¿è¡Œ hexo g å’Œ hexo d å‘å¸ƒã€‚</p><pre><code class="markdown">---title: Hello World # æ ‡é¢˜date: 2019/3/26 hh:mm:ss # æ—¶é—´categories: # åˆ†ç±»- Diarytags: # æ ‡ç­¾- PS3- Games---æ‘˜è¦&lt;!--more--&gt;æ­£æ–‡</code></pre><h2 id="ç½‘ç«™è®¾ç½®"><a href="#ç½‘ç«™è®¾ç½®" class="headerlink" title="ç½‘ç«™è®¾ç½®"></a>ç½‘ç«™è®¾ç½®</h2><p>åŒ…æ‹¬ç½‘ç«™åç§°ã€æè¿°ã€ä½œè€…ã€é“¾æ¥æ ·å¼ç­‰ï¼Œå…¨éƒ¨åœ¨ç½‘ç«™ç›®å½•ä¸‹çš„ _config.yml æ–‡ä»¶ä¸­ï¼Œå‚è€ƒå®˜æ–¹æ–‡æ¡£æŒ‰éœ€è¦ç¼–è¾‘ã€‚</p><p>æ³¨æ„ï¼šå†’å·åè¦åŠ ä¸€ä¸ªç©ºæ ¼ï¼</p><h2 id="æ›´æ¢ä¸»é¢˜"><a href="#æ›´æ¢ä¸»é¢˜" class="headerlink" title="æ›´æ¢ä¸»é¢˜"></a>æ›´æ¢ä¸»é¢˜</h2><p>åœ¨ Themes | Hexo é€‰æ‹©ä¸€ä¸ªå–œæ¬¢çš„ä¸»é¢˜ï¼Œæ¯”å¦‚ NexTï¼Œè¿›å…¥ç½‘ç«™ç›®å½•æ‰“å¼€ Git Bash Here ä¸‹è½½ä¸»é¢˜ï¼š</p><pre><code class="bash">git clone https://github.com/iissnan/hexo-theme-next themes/next</code></pre><p>ç„¶åä¿®æ”¹ _config.yml ä¸­çš„ theme ä¸ºæ–°ä¸»é¢˜åç§° nextï¼Œå‘å¸ƒã€‚ï¼ˆæœ‰çš„ä¸»é¢˜éœ€è¦å°† _config.yml æ›¿æ¢ä¸ºä¸»é¢˜è‡ªå¸¦çš„ï¼Œå‚è€ƒä¸»é¢˜è¯´æ˜ã€‚ï¼‰</p><h2 id="å¸¸ç”¨å‘½ä»¤"><a href="#å¸¸ç”¨å‘½ä»¤" class="headerlink" title="å¸¸ç”¨å‘½ä»¤"></a>å¸¸ç”¨å‘½ä»¤</h2><pre><code class="bash">hexo new &quot;name&quot;       # æ–°å»ºæ–‡ç« hexo new page &quot;name&quot;  # æ–°å»ºé¡µé¢hexo g                # ç”Ÿæˆé¡µé¢hexo d                # éƒ¨ç½²hexo g -d             # ç”Ÿæˆé¡µé¢å¹¶éƒ¨ç½²hexo s                # æœ¬åœ°é¢„è§ˆhexo clean            # æ¸…é™¤ç¼“å­˜å’Œå·²ç”Ÿæˆçš„é™æ€æ–‡ä»¶hexo help             # å¸®åŠ©</code></pre><h2 id="å¸¸è§é—®é¢˜"><a href="#å¸¸è§é—®é¢˜" class="headerlink" title="å¸¸è§é—®é¢˜"></a>å¸¸è§é—®é¢˜</h2><p>1ã€Hexo è®¾ç½®æ˜¾ç¤ºæ–‡ç« æ‘˜è¦ï¼Œé¦–é¡µä¸æ˜¾ç¤ºå…¨æ–‡</p><p>Hexo ä¸»é¡µæ–‡ç« åˆ—è¡¨é»˜è®¤ä¼šæ˜¾ç¤ºæ–‡ç« å…¨æ–‡ï¼Œæµè§ˆæ—¶å¾ˆä¸æ–¹ä¾¿ï¼Œå¯ä»¥åœ¨æ–‡ç« ä¸­æ’å…¥</p><pre><code class="markdown">&lt;!--more--&gt;</code></pre><p>è¿›è¡Œåˆ†æ®µã€‚</p><p>è¯¥ä»£ç å‰é¢çš„å†…å®¹ä¼šä½œä¸ºæ‘˜è¦æ˜¾ç¤ºï¼Œè€Œåé¢çš„å†…å®¹ä¼šæ›¿æ¢ä¸º â€œRead Moreâ€ éšè—èµ·æ¥ã€‚</p><p>2ã€è®¾ç½®ç½‘ç«™å›¾æ ‡</p><p>è¿›å…¥ themes&#x2F;ä¸»é¢˜ æ–‡ä»¶å¤¹ï¼Œæ‰“å¼€ _config.yml é…ç½®æ–‡ä»¶ï¼Œæ‰¾åˆ° favicon ä¿®æ”¹ï¼Œä¸€èˆ¬æ ¼å¼ä¸ºï¼šfavicon: å›¾æ ‡åœ°å€ã€‚ï¼ˆä¸åŒä¸»é¢˜å¯èƒ½ç•¥æœ‰å·®åˆ«ï¼‰</p><p>3ã€ä¿®æ”¹å¹¶éƒ¨ç½²åæ²¡æœ‰æ•ˆæœ</p><p>ä½¿ç”¨ hexo clean æ¸…ç†åé‡æ–°éƒ¨ç½²ã€‚</p><p>4ã€markdownå›¾ç‰‡å¼•å…¥æ²¡æœ‰æ•ˆæœ</p><ul><li>ç»Ÿä¸€æ”¹ä¸ºGithubä»“åº“å›¾ç‰‡é“¾æ¥ï¼Œä¾‹å­ï¼š</li></ul><pre><code class="markdwon">![&quot;å›¾ç‰‡æ ‡é¢˜&quot;](https://github.com/SheldonCoder1337/sheldoncoder1337.github.io/blob/master/2024/04/17/temp/sheldon.png?raw=true)</code></pre><ul><li>ä½¿ç”¨æ¨¡æ¿è¯­è¨€</li></ul><pre><code class="markdown">&#123;% asset_img sheldon.png "å›¾ç‰‡æ ‡é¢˜" %&#125;</code></pre><h2 id="ç»“è¯­"><a href="#ç»“è¯­" class="headerlink" title="ç»“è¯­"></a>ç»“è¯­</h2><p>Hexo æ˜¯ä¸€ç§çº¯é™æ€çš„åšå®¢ï¼Œæˆ‘ä»¬å¿…é¡»è¦åœ¨æœ¬åœ°å®Œæˆæ–‡ç« çš„ç¼–è¾‘å†éƒ¨ç½²åˆ° GitHub ä¸Šï¼Œä¾èµ–äºæœ¬åœ°ç¯å¢ƒã€‚ä¸èƒ½åƒ WordPress æˆ– Typecho é‚£æ ·çš„åŠ¨æ€åšå®¢ä¸€æ ·èƒ½ç›´æ¥åœ¨æµè§ˆå™¨ä¸­å®Œæˆæ’°æ–‡å’Œå‘å¸ƒã€‚</p><p>å¯ä»¥è¯´æ˜¯ä¸€ç§æ¯”è¾ƒæå®¢çš„å†™åšå®¢æ–¹å¼ï¼Œä½†æ˜¯ä¼˜åŠ¿ä¹Ÿæ˜¯æ˜æ˜¾çš„â€”â€”å…è´¹ç¨³å®šçœå¿ƒï¼Œæ¯”è¾ƒé€‚åˆçˆ±æŠ˜è…¾ç ”ç©¶çš„ç”¨æˆ·ï¼Œæˆ–è€…æ²¡æœ‰åœ¨çº¿å‘æ–‡éœ€æ±‚çš„æœ‹å‹ã€‚</p><p><a href="https://hexo.io/">Hexo</a>!  Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>]]></content>
      
      
      <categories>
          
          <category> tools </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
